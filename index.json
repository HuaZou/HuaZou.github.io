[{"authors":null,"categories":null,"content":"I (Hua Zou) am a senior bioinformatics analyst at Xbiome Company since Mar 7 2022. My research interests include host-microbiota intersection, machine learning and multi-omics data integration. I have developed the XMAS 2.0 R package for data analysis on microbiota data.\nDownload my resumé .\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I (Hua Zou) am a senior bioinformatics analyst at Xbiome Company since Mar 7 2022. My research interests include host-microbiota intersection, machine learning and multi-omics data integration. I have developed the XMAS 2.","tags":null,"title":"Hua Zou","type":"authors"},{"authors":null,"categories":["Visualization","ggplot2"],"content":"Introduction Errorbar with jitter gives intuitive sense to observe the differences among groups.\nLoading required packages knitr::opts_chunk$set(message = FALSE, warning = FALSE) library(tidyverse) library(ggpubr) rm(list = ls()) options(stringsAsFactors = F) # group \u0026amp; color group_names \u0026lt;- c(\u0026#34;setosa\u0026#34;, \u0026#34;versicolor\u0026#34;, \u0026#34;virginica\u0026#34;) group_colors \u0026lt;- c(\u0026#34;#0073C2FF\u0026#34;, \u0026#34;#EFC000FF\u0026#34;, \u0026#34;#CD534CFF\u0026#34;) Data preparation Loading iris and ToothGrowth dataset\nFactorizing Species\ndata(\u0026#34;iris\u0026#34;) plotdata \u0026lt;- iris |\u0026gt; dplyr::select(Sepal.Length, Species) |\u0026gt; dplyr::mutate(Species = factor(Species, levels = group_names)) |\u0026gt; dplyr::rename(Group = Species, Index = Sepal.Length) head(plotdata) ## Index Group ## 1 5.1 setosa ## 2 4.9 setosa ## 3 4.7 setosa ## 4 4.6 setosa ## 5 5.0 setosa ## 6 5.4 setosa # other dataset data(\u0026#34;ToothGrowth\u0026#34;) errorbar with points create error bar by stat_summary using errorbar\nadd point by geom_point\npl \u0026lt;- ggplot(data = plotdata, aes(x = Group, y = Index, shape = Group)) + stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = \u0026#34;errorbar\u0026#34;, width = 0.2) + stat_summary(fun = median, fun.min = median, fun.max = median, geom = \u0026#34;crossbar\u0026#34;, width = 0.3) + geom_point(aes(color = Group), position = position_jitter(width = 0.1), size = 1.5, stroke = 1) + stat_compare_means(comparisons = list(c(\u0026#34;setosa\u0026#34;, \u0026#34;versicolor\u0026#34;), c(\u0026#34;setosa\u0026#34;, \u0026#34;virginica\u0026#34;)), method = \u0026#34;t.test\u0026#34;, label = \u0026#34;p.signif\u0026#34;) + labs(x = \u0026#34;\u0026#34;) + scale_y_continuous(expand = expansion(mult = c(0.1, 0.1))) + scale_color_manual(values = group_colors) + scale_shape_manual(values = c(1, 16, 17)) + guides(color = \u0026#34;none\u0026#34;, shape = \u0026#34;none\u0026#34;, fill = \u0026#34;none\u0026#34;) + theme_classic() + theme(axis.title = element_text(size = 12, color = \u0026#34;black\u0026#34;, face = \u0026#34;bold\u0026#34;), axis.text = element_text(size = 10, color = \u0026#34;black\u0026#34;), text = element_text(size = 9, color = \u0026#34;black\u0026#34;)) pl Errorbar with line Using ToothGrowth dataset with groups Dose and supp ToothGrowth$dose \u0026lt;- factor(ToothGrowth$dose) data_summary \u0026lt;- function(data, varname, groupnames){ require(plyr) summary_func \u0026lt;- function(x, col) { mean_value \u0026lt;- mean(x[[col]], na.rm = TRUE) sd_value \u0026lt;- sd(x[[col]], na.rm = TRUE) length_n \u0026lt;- length(x[[col]]) # standard error= standard deviation/squareroot(n) se_value \u0026lt;- sd_value / sqrt(length_n) return(c(mean = mean_value, sd = sd_value, se = se_value)) } data_sum \u0026lt;- ddply(data, groupnames, .fun=summary_func, varname) data_sum \u0026lt;- rename(data_sum, c(\u0026#34;mean\u0026#34; = varname)) return(data_sum) } plotdata2 \u0026lt;- data_summary( ToothGrowth, varname = \u0026#34;len\u0026#34;, groupnames = c(\u0026#34;supp\u0026#34;, \u0026#34;dose\u0026#34;)) pl2 \u0026lt;- ggplot(data = plotdata2, aes(x = dose, y = len, group = supp, color = supp)) + geom_line() + geom_point(size = 1.5, stroke = 1) + # geom_errorbar(aes(ymin = len - sd, ymax = len + sd), width = .2, # position = position_dodge(0.05)) + geom_errorbar(aes(ymin = len - se, ymax = len + se), width = .2, position = position_dodge(0.05)) + stat_compare_means(data = ToothGrowth, aes(x = dose, y = len, group = supp), label = \u0026#34;p.signif\u0026#34;, label.y = c(21, 28, 32)) + labs(x = \u0026#34;\u0026#34;) + scale_y_continuous(expand = expansion(mult = c(0.1, 0.1))) + scale_color_manual(values = c(\u0026#34;#999999\u0026#34;,\u0026#34;#E69F00\u0026#34;)) + guides(color = \u0026#34;none\u0026#34;) + theme_classic() + theme(axis.title = element_text(size = 12, color = \u0026#34;black\u0026#34;, face = \u0026#34;bold\u0026#34;), axis.text = element_text(size = 10, color = \u0026#34;black\u0026#34;), text = element_text(size = 9, color = \u0026#34;black\u0026#34;)) pl2 Conclusion Compared to single boxplot, Errorbar with jitter provides more information.\nSystemic information devtools::session_info() ## ─ Session info ─────────────────────────────────────────────────────────────── ## setting value ## version R version 4.1.3 (2022-03-10) ## os macOS Big Sur/Monterey 10.16 ## system x86_64, darwin17.0 ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz Asia/Shanghai ## date 2023-07-24 ## pandoc 3.1.3 @ /Users/zouhua/opt/anaconda3/bin/ (via rmarkdown) ## ## ─ Packages ─────────────────────────────────────────────────────────────────── ## package * version date (UTC) lib source ## abind 1.4-5 2016-07-21 [2] CRAN (R 4.1.0) ## backports 1.4.1 2021-12-13 [2] CRAN (R 4.1.0) ## base64enc 0.1-3 2015-07-28 [2] CRAN (R 4.1.0) ## blogdown 1.18 2023-06-19 [2] CRAN (R 4.1.3) ## bookdown 0.34 2023-05-09 [2] CRAN (R 4.1.2) ## broom 1.0.5 2023-06-09 [2] CRAN (R 4.1.3) ## bslib 0.5.0 2023-06-09 [2] CRAN (R 4.1.3) ## cachem 1.0.8 2023-05-01 [2] CRAN (R 4.1.2) ## callr 3.7.3 2022-11-02 [2] CRAN (R 4.1.2) ## car 3.1-2 2023-03-30 [2] CRAN (R 4.1.2) ## carData 3.0-5 2022-01-06 [2] CRAN (R 4.1.2) ## checkmate 2.2.0 2023-04-27 [2] CRAN (R 4.1.2) ## cli 3.6.1 2023-03-23 [2] CRAN (R 4.1.2) ## cluster 2.1.4 2022-08-22 [2] CRAN (R 4.1.2) ## colorspace 2.1-0 2023-01-23 [2] CRAN (R 4.1.2) ## crayon 1.5.2 2022-09-29 [2] CRAN (R 4.1.2) ## data.table 1.14.8 2023-02-17 [2] CRAN (R 4.1.2) ## devtools 2.4.5 2022-10-11 [2] CRAN (R 4.1.2) ## digest 0.6.33 2023-07-07 [1] CRAN (R 4.1.3) ## dplyr * 1.1.2 2023-04-20 [2] CRAN (R 4.1.2) ## ellipsis 0.3.2 2021-04-29 [2] CRAN (R 4.1.0) ## evaluate 0.21 2023-05-05 …","date":1690070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690092327,"objectID":"6d592b18ccc087d8c535319852140ddc","permalink":"https://zouhua.top/post/visualization/2023-07-23-errorline/","publishdate":"2023-07-23T00:00:00Z","relpermalink":"/post/visualization/2023-07-23-errorline/","section":"post","summary":"Diffferences among groups by errorbar using ggplot2","tags":["Visualization"],"title":"R visualization: errorbar by ggplot2","type":"post"},{"authors":null,"categories":["Visualization","ggplot2"],"content":"Introduction Barplot with jitter shows the mean and standard deviation or standard error between groups.\nLoading required packages knitr::opts_chunk$set(message = FALSE, warning = FALSE) library(tidyverse) library(ggpubr) rm(list = ls()) options(stringsAsFactors = F) # group \u0026amp; color dose_names \u0026lt;- c(\u0026#34;0.5\u0026#34;, \u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;) dose_colors \u0026lt;- c(\u0026#34;#0073C2FF\u0026#34;, \u0026#34;#EFC000FF\u0026#34;, \u0026#34;#CD534CFF\u0026#34;) supp_names \u0026lt;- c(\u0026#34;OJ\u0026#34;, \u0026#34;VC\u0026#34;) supp_colors \u0026lt;- c(\u0026#39;#999999\u0026#39;,\u0026#39;#E69F00\u0026#39;) Data preparation Loading ToothGrowth dataset\nFactorizing categorical variables\ndata(\u0026#34;ToothGrowth\u0026#34;) plotdata \u0026lt;- ToothGrowth |\u0026gt; dplyr::mutate(dose = factor(as.character(dose), levels = dose_names), supp = factor(supp, levels = supp_names)) |\u0026gt; dplyr::rename(Group = dose, Subgroup = supp, Index = len) |\u0026gt; dplyr::select(Group, Subgroup, Index) head(plotdata) ## Group Subgroup Index ## 1 0.5 VC 4.2 ## 2 0.5 VC 11.5 ## 3 0.5 VC 7.3 ## 4 0.5 VC 5.8 ## 5 0.5 VC 6.4 ## 6 0.5 VC 10.0 Calculating mean \u0026amp; sd \u0026amp; se data_summary \u0026lt;- function(data, varname, groupnames){ require(plyr) summary_func \u0026lt;- function(x, col) { mean_value \u0026lt;- mean(x[[col]], na.rm = TRUE) sd_value \u0026lt;- sd(x[[col]], na.rm = TRUE) length_n \u0026lt;- length(x[[col]]) # standard error= standard deviation/squareroot(n) se_value \u0026lt;- sd_value / sqrt(length_n) return(c(mean = mean_value, sd = sd_value, se = se_value)) } data_sum \u0026lt;- ddply(data, groupnames, .fun=summary_func, varname) data_sum \u0026lt;- rename(data_sum, c(\u0026#34;mean\u0026#34; = varname)) return(data_sum) } dat_cal_one \u0026lt;- data_summary( plotdata, varname = \u0026#34;Index\u0026#34;, groupnames = \u0026#34;Group\u0026#34;) head(dat_cal_one) ## Group Index sd se ## 1 0.5 10.605 4.499763 1.0061776 ## 2 1 19.735 4.415436 0.9873216 ## 3 2 26.100 3.774150 0.8439257 dat_cal_two \u0026lt;- data_summary( plotdata, varname = \u0026#34;Index\u0026#34;, groupnames = c(\u0026#34;Group\u0026#34;, \u0026#34;Subgroup\u0026#34;)) head(dat_cal_two) ## Group Subgroup Index sd se ## 1 0.5 OJ 13.23 4.459709 1.4102837 ## 2 0.5 VC 7.98 2.746634 0.8685620 ## 3 1 OJ 22.70 3.910953 1.2367520 ## 4 1 VC 16.77 2.515309 0.7954104 ## 5 2 OJ 26.06 2.655058 0.8396031 ## 6 2 VC 26.14 4.797731 1.5171757 barplot with single group create bar by geom_bar\nadd point by geom_jitter\npl \u0026lt;- ggplot(data = dat_cal_one, aes(x = Group, y = Index, fill = Group, color = Group)) + geom_bar(stat = \u0026#34;identity\u0026#34;, color = \u0026#34;white\u0026#34;, position = position_dodge()) + geom_errorbar(aes(ymin = Index, ymax = Index + sd), width = 0.2, position = position_dodge(width = 0.9), linewidth = 1) + stat_compare_means(data = plotdata, aes(x = Group, y = Index), comparisons = list(c(\u0026#34;0.5\u0026#34;, \u0026#34;1\u0026#34;), c(\u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;)), label = \u0026#34;p.signif\u0026#34;) + geom_jitter(data = plotdata, aes(x = Group, y = Index), size = 2, color = \u0026#34;gray10\u0026#34;, show.legend = FALSE, pch = 21) + labs(x = \u0026#34;\u0026#34;) + scale_y_continuous(expand = expansion(mult = c(0, 0.1))) + scale_fill_manual(values = dose_colors) + scale_color_manual(values = dose_colors) + guides(color = \u0026#34;none\u0026#34;, fill = \u0026#34;none\u0026#34;) + theme_classic() + theme(axis.title = element_text(size = 12, color = \u0026#34;black\u0026#34;, face = \u0026#34;bold\u0026#34;), axis.text = element_text(size = 10, color = \u0026#34;black\u0026#34;), text = element_text(size = 9, color = \u0026#34;black\u0026#34;)) pl barplot with two groups create bar by geom_bar\nadd point by geom_jitter\npl2 \u0026lt;- ggplot(data = dat_cal_two, aes(x = Group, y = Index, fill = Subgroup, color = Subgroup)) + geom_bar(stat = \u0026#34;identity\u0026#34;, color = \u0026#34;white\u0026#34;, position = position_dodge()) + geom_errorbar(aes(ymin = Index - sd, ymax = Index + sd), width = 0.2, position = position_dodge(width = 0.9), size = 1) + stat_compare_means(data = plotdata, aes(x = Group, y = Index, group = Subgroup), label = \u0026#34;p.signif\u0026#34;, label.y = c(21, 28, 36)) + geom_point(data = plotdata, aes(x = Group, y = Index, shape = Subgroup), position = position_jitterdodge(jitter.width = 0.5, dodge.width = 0.7), size = 2, color = \u0026#34;gray50\u0026#34;, show.legend = FALSE) + labs(x = \u0026#34;\u0026#34;) + scale_y_continuous(expand = expansion(mult = c(0, 0.1))) + scale_fill_manual(values = supp_colors) + scale_color_manual(values = supp_colors) + scale_shape_manual(values = c(1, 21)) + guides(color = \u0026#34;none\u0026#34;, shape = \u0026#34;none\u0026#34;) + theme_classic() + theme(axis.title = element_text(size = 12, color = \u0026#34;black\u0026#34;, face = \u0026#34;bold\u0026#34;), axis.text = element_text(size = 10, color = \u0026#34;black\u0026#34;), text = element_text(size = 9, color = \u0026#34;black\u0026#34;)) pl2 Conclusion Compared to single boxplot, barplot focus on the mean and variation of data per group.\nSystemic information devtools::session_info() ## ─ Session info ─────────────────────────────────────────────────────────────── ## setting value ## version R version 4.1.3 (2022-03-10) ## os macOS Big Sur/Monterey 10.16 ## system x86_64, darwin17.0 ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz Asia/Shanghai ## date 2023-07-25 ## pandoc 3.1.3 @ /Users/zouhua/opt/anaconda3/bin/ (via rmarkdown) ## ## ─ Packages ─────────────────────────────────────────────────────────────────── ## package * version date (UTC) lib source ## abind 1.4-5 2016-07-21 [2] CRAN (R 4.1.0) ## backports 1.4.1 2021-12-13 [2] CRAN (R 4.1.0) ## blogdown 1.18 2023-06-19 [2] CRAN (R 4.1.3) ## bookdown 0.34 2023-05-09 [2] CRAN (R 4.1.2) ## …","date":1689984e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1690005927,"objectID":"a02ac250b6a1b1eb94f1ccab4517305c","permalink":"https://zouhua.top/post/visualization/2023-07-22-barplot/","publishdate":"2023-07-22T00:00:00Z","relpermalink":"/post/visualization/2023-07-22-barplot/","section":"post","summary":"Diffferences among groups by barplot using ggplot2","tags":["Visualization"],"title":"R visualization: barplot by ggplot2","type":"post"},{"authors":null,"categories":["Visualization","ggplot2"],"content":"Introduction STAMP shows the mean abundance/value/proportion of features and the statistical results (t-test or wilcox-test) between groups.\nLoading required packages knitr::opts_chunk$set(message = FALSE, warning = FALSE) library(tidyverse) library(broom) library(patchwork) rm(list = ls()) options(stringsAsFactors = F) group_names \u0026lt;- c(\u0026#34;setosa\u0026#34;, \u0026#34;versicolor\u0026#34;) group_colors \u0026lt;- c(\u0026#34;#E69F00\u0026#34;, \u0026#34;#56B4E9\u0026#34;) Data preparation for hypothesis testing Loading iris dataset\nFactorizing Species\ndata(\u0026#34;iris\u0026#34;) test_data \u0026lt;- iris |\u0026gt; dplyr::select(Species, Sepal.Length, Sepal.Width, Petal.Length, Petal.Width) |\u0026gt; dplyr::filter(Species %in% group_names) |\u0026gt; dplyr::mutate(Species = factor(Species, levels = group_names)) head(test_data) ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1 setosa 5.1 3.5 1.4 0.2 ## 2 setosa 4.9 3.0 1.4 0.2 ## 3 setosa 4.7 3.2 1.3 0.2 ## 4 setosa 4.6 3.1 1.5 0.2 ## 5 setosa 5.0 3.6 1.4 0.2 ## 6 setosa 5.4 3.9 1.7 0.4 T-test for normal distribution, otherwise wilcox’s test test_res \u0026lt;- test_data %\u0026gt;% dplyr::select_if(is.numeric) %\u0026gt;% purrr::map_df(~ broom::tidy(t.test(. ~ Species, data = test_data)), .id = \u0026#39;Vars\u0026#39;) |\u0026gt; dplyr::mutate(adjustP = p.adjust(p.value, method = \u0026#34;BH\u0026#34;)) |\u0026gt; dplyr::filter(adjustP \u0026lt; 0.05) # test_res \u0026lt;- test_data %\u0026gt;% # dplyr::select_if(is.numeric) %\u0026gt;% # purrr::map_df(~ broom::tidy(wilcox.test(. ~ Species, data = test_data)), .id = \u0026#39;Vars\u0026#39;) |\u0026gt; # dplyr::mutate(adjustP = p.adjust(p.value, method = \u0026#34;BH\u0026#34;)) |\u0026gt; # dplyr::filter(adjustP \u0026lt; 0.05) head(test_res) ## # A tibble: 4 × 12 ## Vars estimate estimate1 estimate2 statistic p.value parameter conf.low ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Sepal.Leng… -0.93 5.01 5.94 -10.5 3.75e-17 86.5 -1.11 ## 2 Sepal.Width 0.658 3.43 2.77 9.45 2.48e-15 94.7 0.520 ## 3 Petal.Leng… -2.80 1.46 4.26 -39.5 9.93e-46 62.1 -2.94 ## 4 Petal.Width -1.08 0.246 1.33 -34.1 2.72e-47 74.8 -1.14 ## # ℹ 4 more variables: conf.high \u0026lt;dbl\u0026gt;, method \u0026lt;chr\u0026gt;, alternative \u0026lt;chr\u0026gt;, ## # adjustP \u0026lt;dbl\u0026gt; Data preparation for plotting data for left subfigure, which is barplot data_left \u0026lt;- test_data |\u0026gt; dplyr::select(all_of(c(\u0026#34;Species\u0026#34;, test_res$Vars))) |\u0026gt; tidyr::pivot_longer(cols = -Species, names_to = \u0026#34;Variables\u0026#34;, values_to = \u0026#34;Values\u0026#34;) |\u0026gt; dplyr::group_by(Variables, Species) %\u0026gt;% dplyr::summarise(Mean = mean(Values), .groups = \u0026#34;drop\u0026#34;) head(data_left) ## # A tibble: 6 × 3 ## Variables Species Mean ## \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Petal.Length setosa 1.46 ## 2 Petal.Length versicolor 4.26 ## 3 Petal.Width setosa 0.246 ## 4 Petal.Width versicolor 1.33 ## 5 Sepal.Length setosa 5.01 ## 6 Sepal.Length versicolor 5.94 data for right subfigure, which is error bar plot data_right \u0026lt;- test_res |\u0026gt; dplyr::select(Vars, estimate, conf.low, conf.high, p.value, adjustP) |\u0026gt; dplyr::mutate(Species = ifelse(estimate \u0026gt; 0, group_names[1], group_names[2])) |\u0026gt; dplyr::arrange(desc(estimate)) |\u0026gt; dplyr::mutate(p.value = signif(p.value, 3), adjustP = signif(adjustP, 3)) |\u0026gt; dplyr::mutate(p.value = as.character(p.value), adjustP = as.character(adjustP)) head(data_right) ## # A tibble: 4 × 7 ## Vars estimate conf.low conf.high p.value adjustP Species ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Sepal.Width 0.658 0.520 0.796 2.48e-15 2.48e-15 setosa ## 2 Sepal.Length -0.93 -1.11 -0.754 3.75e-17 5e-17 versicolor ## 3 Petal.Width -1.08 -1.14 -1.02 2.72e-47 1.09e-46 versicolor ## 4 Petal.Length -2.80 -2.94 -2.66 9.93e-46 1.99e-45 versicolor Order variables in two dataset data_left$Variables \u0026lt;- factor(data_left$Variables, levels = rev(data_right$Vars)) data_right$Vars \u0026lt;- factor(data_right$Vars, levels = levels(data_left$Variables)) Plotting left subfigure\napply geom_bar for barplot\nadd segement by annotate\npl_left_bar \u0026lt;- ggplot(data_left, aes(x = Variables, y = Mean, fill = Species)) + scale_x_discrete(limits = levels(data_left$Variables)) + coord_flip() + labs(x = \u0026#34;\u0026#34;, y = \u0026#34;Mean Abundance\u0026#34;) + theme(panel.background = element_rect(fill = \u0026#34;transparent\u0026#34;), panel.grid = element_blank(), axis.ticks.length = unit(0.4, \u0026#34;lines\u0026#34;), axis.ticks = element_line(color = \u0026#34;black\u0026#34;), axis.line = element_line(color = \u0026#34;black\u0026#34;), axis.title.x = element_text(color = \u0026#34;black\u0026#34;, size = 12, face = \u0026#34;bold\u0026#34;), axis.text = element_text(color = \u0026#34;black\u0026#34;, size = 10, face = \u0026#34;bold\u0026#34;), legend.title = element_blank(), legend.text = element_text(size = 12, face = \u0026#34;bold\u0026#34;, color = \u0026#34;black\u0026#34;, margin = margin(r = 20)), legend.position = \u0026#34;bottom\u0026#34;, legend.direction = \u0026#34;horizontal\u0026#34;, legend.key.width = unit(0.8, \u0026#34;cm\u0026#34;), legend.key.height = unit(0.5, \u0026#34;cm\u0026#34;)) for (i in 1:(nrow(data_right) - 1)) { pl_left_bar \u0026lt;- pl_left_bar + annotate(\u0026#34;rect\u0026#34;, xmin = i + 0.5, xmax = i+1.5, ymin = -Inf, ymax = Inf, fill = ifelse(i %% 2 == 0, \u0026#39;white\u0026#39;, \u0026#39;gray95\u0026#39;)) } pl_left_bar \u0026lt;- pl_left_bar + geom_bar(stat = \u0026#34;identity\u0026#34;, position = \u0026#34;dodge\u0026#34;, width = 0.7, color = \u0026#34;black\u0026#34;) + scale_fill_manual(values = group_colors) pl_left_bar right subfigure for error bar\napply geom_errorbar for error bar\nadd segement by annotate\npl_right_scatter \u0026lt;- ggplot(data_right, aes(x = Vars, y = estimate, fill = …","date":1689552e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689573927,"objectID":"8bdc3114d400d32e73fc65a7b0cf5e3e","permalink":"https://zouhua.top/post/visualization/2023-07-17-stamp/","publishdate":"2023-07-17T00:00:00Z","relpermalink":"/post/visualization/2023-07-17-stamp/","section":"post","summary":"STAMP shows the differences between groups and results of hypothesis testing","tags":["Visualization"],"title":"R visualization: STAMP by ggplot2","type":"post"},{"authors":null,"categories":["Visualization","ggplot2"],"content":"Introduction Violin using the density of distribution to show the differences among groups is one of the most important means for data visualization.\nLoading required packages knitr::opts_chunk$set(message = FALSE, warning = FALSE) library(tidyverse) library(ggpubr) # install.packages(\u0026#34;gghalves\u0026#34;) library(gghalves) rm(list = ls()) options(stringsAsFactors = F) # group \u0026amp; color group_names \u0026lt;- c(\u0026#34;setosa\u0026#34;, \u0026#34;versicolor\u0026#34;, \u0026#34;virginica\u0026#34;) group_colors \u0026lt;- c(\u0026#34;#0073C2FF\u0026#34;, \u0026#34;#EFC000FF\u0026#34;, \u0026#34;#CD534CFF\u0026#34;) Data preparation Loading iris and ToothGrowth dataset\nFactorizing Species\ndata(\u0026#34;iris\u0026#34;) plotdata \u0026lt;- iris |\u0026gt; dplyr::select(Sepal.Length, Species, Sepal.Width) |\u0026gt; dplyr::mutate(Species = factor(Species, levels = group_names)) |\u0026gt; dplyr::rename(Group = Species, Index = Sepal.Length, Index2 = Sepal.Width) head(plotdata) ## Index Group Index2 ## 1 5.1 setosa 3.5 ## 2 4.9 setosa 3.0 ## 3 4.7 setosa 3.2 ## 4 4.6 setosa 3.1 ## 5 5.0 setosa 3.6 ## 6 5.4 setosa 3.9 # other dataset data(\u0026#34;ToothGrowth\u0026#34;) Single violin create violin by geom_violin\napply stat_boxplot and geom_boxplot for boxplot\nadd point by geom_point\npl_violin \u0026lt;- ggplot(data = plotdata, aes(x = Group, y = Index, shape = Group)) + geom_violin(aes(fill = Group), alpha = 0.5) + stat_boxplot(geom = \u0026#34;errorbar\u0026#34;, width = 0.2) + geom_boxplot(width = .1, notch = TRUE) + geom_point(aes(color = Group), position = position_jitter(width = 0.1), size = 1) + stat_compare_means(comparisons = list(c(\u0026#34;setosa\u0026#34;, \u0026#34;versicolor\u0026#34;), c(\u0026#34;setosa\u0026#34;, \u0026#34;virginica\u0026#34;)), method = \u0026#34;t.test\u0026#34;, label = \u0026#34;p.signif\u0026#34;) + labs(x = \u0026#34;\u0026#34;) + scale_y_continuous(expand = expansion(mult = c(0.1, 0.1))) + scale_color_manual(values = group_colors) + scale_fill_manual(values = group_colors) + scale_shape_manual(values = c(15, 16, 17)) + guides(color = \u0026#34;none\u0026#34;, shape = \u0026#34;none\u0026#34;, fill = \u0026#34;none\u0026#34;) + theme_bw() + theme(axis.title = element_text(size = 12, color = \u0026#34;black\u0026#34;, face = \u0026#34;bold\u0026#34;), axis.text = element_text(size = 10, color = \u0026#34;black\u0026#34;), text = element_text(size = 9, color = \u0026#34;black\u0026#34;)) pl_violin Violin with two different groups The left part of violin is totally different from the right.\nUsing ToothGrowth dataset with groups Dose and supp pl_violin2 \u0026lt;- ToothGrowth |\u0026gt; dplyr::mutate(supp = factor(supp, levels = c(\u0026#34;VC\u0026#34;, \u0026#34;OJ\u0026#34;))) |\u0026gt; ggplot(aes(x = as.factor(dose), y = len)) + geom_half_violin(aes(fill = supp, color = supp, split = supp), position = \u0026#34;identity\u0026#34;) + labs(x = \u0026#34;Dose\u0026#34;) + scale_y_continuous(expand = expansion(mult = c(0.1, 0.1))) + scale_color_manual(values = c(\u0026#34;#AF0F11\u0026#34;, \u0026#34;#3372A6\u0026#34;)) + scale_fill_manual(values = c(\u0026#34;#AF0F11\u0026#34;, \u0026#34;#3372A6\u0026#34;)) + theme_bw() + theme(axis.title = element_text(size = 12, color = \u0026#34;black\u0026#34;, face = \u0026#34;bold\u0026#34;), axis.text = element_text(size = 10, color = \u0026#34;black\u0026#34;), text = element_text(size = 9, color = \u0026#34;black\u0026#34;), legend.position = c(.9, 0.2)) pl_violin2 add significance between groups by stat_compare_means pl_violin2_1 \u0026lt;- pl_violin2 + stat_compare_means(aes(group = supp), label = \u0026#34;p.signif\u0026#34;, label.y = (max(ToothGrowth$len) + max(ToothGrowth$len) * 0.1)) pl_violin2_1 add significance between groups by ggsignif::geom_signif library(ggsignif) nlength \u0026lt;- length(unique(ToothGrowth$dose)) test_res \u0026lt;- ToothGrowth |\u0026gt; dplyr::mutate(dose = factor(dose)) |\u0026gt; dplyr::group_by(dose) |\u0026gt; rstatix::t_test(len ~ supp) |\u0026gt; rstatix::adjust_pvalue() |\u0026gt; rstatix::add_significance(\u0026#34;p.adj\u0026#34;) |\u0026gt; dplyr::mutate(x = seq(0.875, 0.875+(nlength-1)*1, 1), xend = seq(1.125, 1.125+(nlength-1)*1, 1), y = rep((max(ToothGrowth$len) + max(ToothGrowth$len) * 0.1), nlength)) pl_violin2_2 \u0026lt;- pl_violin2 + geom_signif(stat = \u0026#34;identity\u0026#34;, data = test_res, aes(x = x,xend = xend, y = y, yend = y, annotation = p.adj.signif)) pl_violin2_2 Violin with two different index pl_violin3 \u0026lt;- ggplot(data = plotdata, aes(x = Group, y = Index)) + geom_half_violin(fill = \u0026#34;blue\u0026#34;, side = \u0026#34;l\u0026#34;, adjust = 0.5, alpha = 0.5) + geom_half_violin(aes(y = Index2), fill = \u0026#34;red\u0026#34;, side = \u0026#34;r\u0026#34;, adjust = 0.5, alpha = 0.5) + labs(x = \u0026#34;\u0026#34;) + annotate(x = 0.5, y = 8, geom = \u0026#34;text\u0026#34;, label = \u0026#34;Index\u0026#34;, color = \u0026#34;blue\u0026#34;, size = 4, fontface = \u0026#34;bold\u0026#34;, hjust = 0) + annotate(x = 0.5, y = 7.5, geom = \u0026#34;text\u0026#34;, label = \u0026#34;Index2\u0026#34;, color = \u0026#34;red\u0026#34;, size = 4, fontface = \u0026#34;bold\u0026#34;, hjust = 0) + scale_y_continuous(expand = expansion(mult = c(0.1, 0.1))) + theme_bw() + theme(axis.title = element_text(size = 12, color = \u0026#34;black\u0026#34;, face = \u0026#34;bold\u0026#34;), axis.text = element_text(size = 10, color = \u0026#34;black\u0026#34;), text = element_text(size = 9, color = \u0026#34;black\u0026#34;)) pl_violin3 Conclusion Compared to single boxplot, violin not only shows differences among group, but also displays the distribution of each value, providing more information.\nSystemic information devtools::session_info() ## ─ Session info ─────────────────────────────────────────────────────────────── ## setting value ## version R version 4.1.3 (2022-03-10) ## os macOS Big Sur/Monterey 10.16 ## system x86_64, darwin17.0 ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz Asia/Shanghai ## date 2023-07-24 ## pandoc 3.1.3 @ /Users/zouhua/opt/anaconda3/bin/ (via rmarkdown) ## ## …","date":1689465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689487527,"objectID":"a42460f2e0f94987a7f6a2b5a5bd9cd2","permalink":"https://zouhua.top/post/visualization/2023-07-16-violin/","publishdate":"2023-07-16T00:00:00Z","relpermalink":"/post/visualization/2023-07-16-violin/","section":"post","summary":"Diffferences among groups by Violin using ggplot2","tags":["Visualization"],"title":"R visualization: Violin by ggplot2","type":"post"},{"authors":null,"categories":["Visualization","ggplot2"],"content":"Introduction Beeswarm is useful to show the differences among groups.\nLoading required packages knitr::opts_chunk$set(message = FALSE, warning = FALSE) library(tidyverse) library(ggbeeswarm) library(ggpubr) rm(list = ls()) options(stringsAsFactors = F) # group \u0026amp; color group_names \u0026lt;- c(\u0026#34;setosa\u0026#34;, \u0026#34;versicolor\u0026#34;, \u0026#34;virginica\u0026#34;) group_colors \u0026lt;- c(\u0026#34;#0073C2FF\u0026#34;, \u0026#34;#EFC000FF\u0026#34;, \u0026#34;#CD534CFF\u0026#34;) Data preparation Loading iris dataset\nFactorizing Species\ndata(\u0026#34;iris\u0026#34;) plotdata \u0026lt;- iris |\u0026gt; dplyr::select(Sepal.Length, Species) |\u0026gt; dplyr::mutate(Species = factor(Species, levels = group_names)) |\u0026gt; dplyr::rename(Group = Species, Index = Sepal.Length) head(plotdata) ## Index Group ## 1 5.1 setosa ## 2 4.9 setosa ## 3 4.7 setosa ## 4 4.6 setosa ## 5 5.0 setosa ## 6 5.4 setosa Plotting apply stat_boxplot and geom_boxplot for boxplot\nadd beeswarm by geom_quasirandom\npl \u0026lt;- ggplot(data = plotdata, aes(x = Group, y = Index, color = Group, shape = Group)) + stat_boxplot(geom = \u0026#34;errorbar\u0026#34;, width = 0.4) + geom_boxplot(width = .4, outlier.shape = NA, notch = TRUE) + geom_quasirandom(method = \u0026#34;smiley\u0026#34;, size = 1.5) + stat_compare_means(comparisons = list(c(\u0026#34;setosa\u0026#34;, \u0026#34;versicolor\u0026#34;), c(\u0026#34;setosa\u0026#34;, \u0026#34;virginica\u0026#34;)), method = \u0026#34;t.test\u0026#34;, label = \u0026#34;p.signif\u0026#34;) + labs(x = \u0026#34;\u0026#34;) + scale_y_continuous(expand = expansion(mult = c(0.1, 0.1))) + scale_color_manual(values = group_colors) + scale_shape_manual(values = c(15, 16, 17)) + guides(color = \u0026#34;none\u0026#34;, shape = \u0026#34;none\u0026#34;) + theme_classic() + theme(axis.title = element_text(size = 12, color = \u0026#34;black\u0026#34;, face = \u0026#34;bold\u0026#34;), axis.text = element_text(size = 10, color = \u0026#34;black\u0026#34;), text = element_text(size = 9, color = \u0026#34;black\u0026#34;)) pl Conclusion Compared to single boxplot, beeswarm not only shows differences among group, but also displays the distribution of each value, providing more information.\nSystemic information devtools::session_info() ## ─ Session info ─────────────────────────────────────────────────────────────── ## setting value ## version R version 4.1.3 (2022-03-10) ## os macOS Big Sur/Monterey 10.16 ## system x86_64, darwin17.0 ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz Asia/Shanghai ## date 2023-07-24 ## pandoc 3.1.3 @ /Users/zouhua/opt/anaconda3/bin/ (via rmarkdown) ## ## ─ Packages ─────────────────────────────────────────────────────────────────── ## package * version date (UTC) lib source ## abind 1.4-5 2016-07-21 [2] CRAN (R 4.1.0) ## backports 1.4.1 2021-12-13 [2] CRAN (R 4.1.0) ## beeswarm 0.4.0 2021-06-01 [2] CRAN (R 4.1.0) ## blogdown 1.18 2023-06-19 [2] CRAN (R 4.1.3) ## bookdown 0.34 2023-05-09 [2] CRAN (R 4.1.2) ## broom 1.0.5 2023-06-09 [2] CRAN (R 4.1.3) ## bslib 0.5.0 2023-06-09 [2] CRAN (R 4.1.3) ## cachem 1.0.8 2023-05-01 [2] CRAN (R 4.1.2) ## callr 3.7.3 2022-11-02 [2] CRAN (R 4.1.2) ## car 3.1-2 2023-03-30 [2] CRAN (R 4.1.2) ## carData 3.0-5 2022-01-06 [2] CRAN (R 4.1.2) ## cli 3.6.1 2023-03-23 [2] CRAN (R 4.1.2) ## colorspace 2.1-0 2023-01-23 [2] CRAN (R 4.1.2) ## crayon 1.5.2 2022-09-29 [2] CRAN (R 4.1.2) ## devtools 2.4.5 2022-10-11 [2] CRAN (R 4.1.2) ## digest 0.6.33 2023-07-07 [1] CRAN (R 4.1.3) ## dplyr * 1.1.2 2023-04-20 [2] CRAN (R 4.1.2) ## ellipsis 0.3.2 2021-04-29 [2] CRAN (R 4.1.0) ## evaluate 0.21 2023-05-05 [2] CRAN (R 4.1.2) ## fansi 1.0.4 2023-01-22 [2] CRAN (R 4.1.2) ## farver 2.1.1 2022-07-06 [2] CRAN (R 4.1.2) ## fastmap 1.1.1 2023-02-24 [2] CRAN (R 4.1.2) ## forcats * 1.0.0 2023-01-29 [2] CRAN (R 4.1.2) ## fs 1.6.2 2023-04-25 [2] CRAN (R 4.1.2) ## generics 0.1.3 2022-07-05 [2] CRAN (R 4.1.2) ## ggbeeswarm * 0.7.2 2023-04-29 [1] CRAN (R 4.1.2) ## ggplot2 * 3.4.2 2023-04-03 [2] CRAN (R 4.1.2) ## ggpubr * 0.6.0 2023-02-10 [2] CRAN (R 4.1.2) ## ggsignif 0.6.4 2022-10-13 [2] CRAN (R 4.1.2) ## glue 1.6.2 2022-02-24 [2] CRAN (R 4.1.2) ## gtable 0.3.3 2023-03-21 [2] CRAN (R 4.1.2) ## highr 0.10 2022-12-22 [2] CRAN (R 4.1.2) ## hms 1.1.3 2023-03-21 [2] CRAN (R 4.1.2) ## htmltools 0.5.5 2023-03-23 [2] CRAN (R 4.1.2) ## htmlwidgets 1.6.2 2023-03-17 [2] CRAN (R 4.1.2) ## httpuv 1.6.11 2023-05-11 [2] CRAN (R 4.1.3) ## jquerylib 0.1.4 2021-04-26 [2] CRAN (R 4.1.0) ## jsonlite 1.8.7 2023-06-29 [2] CRAN (R 4.1.3) ## knitr 1.43 2023-05-25 [2] CRAN (R 4.1.3) ## labeling 0.4.2 2020-10-20 [2] CRAN (R 4.1.0) ## later 1.3.1 2023-05-02 [2] CRAN (R 4.1.2) ## lifecycle 1.0.3 2022-10-07 [2] CRAN (R 4.1.2) ## lubridate * 1.9.2 2023-02-10 [2] CRAN (R 4.1.2) ## magrittr 2.0.3 2022-03-30 [2] CRAN (R 4.1.2) ## memoise 2.0.1 2021-11-26 [2] CRAN (R 4.1.0) ## mime 0.12 2021-09-28 [2] CRAN (R 4.1.0) ## miniUI 0.1.1.1 2018-05-18 [2] CRAN (R 4.1.0) ## munsell 0.5.0 2018-06-12 [2] CRAN (R 4.1.0) ## pillar 1.9.0 2023-03-22 [2] CRAN (R 4.1.2) ## pkgbuild 1.4.2 2023-06-26 [2] CRAN (R 4.1.3) ## pkgconfig 2.0.3 2019-09-22 [2] CRAN (R 4.1.0) ## pkgload 1.3.2.1 2023-07-08 [2] CRAN (R 4.1.3) ## prettyunits 1.1.1 2020-01-24 [2] CRAN (R 4.1.0) ## processx 3.8.2 2023-06-30 [2] CRAN (R 4.1.3) ## profvis 0.3.8 2023-05-02 [2] CRAN (R 4.1.2) ## promises 1.2.0.1 2021-02-11 [2] CRAN (R 4.1.0) ## ps 1.7.5 …","date":1689379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689401127,"objectID":"2ab55d053e971557a144c04319d5cdd9","permalink":"https://zouhua.top/post/visualization/2023-07-15-beeswarm/","publishdate":"2023-07-15T00:00:00Z","relpermalink":"/post/visualization/2023-07-15-beeswarm/","section":"post","summary":"Diffferences among groups by Beeswarm using ggplot2","tags":["Visualization"],"title":"R visualization: Beeswarm by ggplot2","type":"post"},{"authors":null,"categories":["Visualization","ggplot2"],"content":"Introduction Compared to pie chart, donut chart has one more hole inside. In addition, multiple circles plot shows different traits of dataset.\nLoading required packages knitr::opts_chunk$set(message = FALSE, warning = FALSE) library(ggplot2) library(dplyr) library(tidyverse) # group \u0026amp; color group_names \u0026lt;- c(\u0026#34;setosa\u0026#34;, \u0026#34;versicolor\u0026#34;, \u0026#34;virginica\u0026#34;) group_colors \u0026lt;- c(\u0026#34;#0073C2FF\u0026#34;, \u0026#34;#EFC000FF\u0026#34;, \u0026#34;#CD534CFF\u0026#34;) Data preparation Loading iris dataset\nSummarizing the total number per species\nCalculating proportion per species\nComputing position of text label\ndata(\u0026#34;iris\u0026#34;) plotdata \u0026lt;- iris |\u0026gt; dplyr::group_by(Species) |\u0026gt; dplyr::summarise(count = length(Species)) |\u0026gt; dplyr::mutate(prop = round(count / sum(count), 2)) |\u0026gt; dplyr::arrange(desc(Species)) |\u0026gt; dplyr::mutate(lab.ypos = cumsum(prop) - 0.5 * prop) |\u0026gt; dplyr::mutate(num_prop = paste0(\u0026#34;n = \u0026#34;, count, \u0026#34;\\n\u0026#34;, paste0(prop, \u0026#34;%\u0026#34;))) |\u0026gt; dplyr::mutate(Species = factor(Species, levels = group_names)) head(plotdata) ## # A tibble: 3 × 5 ## Species count prop lab.ypos num_prop ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 virginica 50 0.33 0.165 \u0026#34;n = 50\\n0.33%\u0026#34; ## 2 versicolor 50 0.33 0.495 \u0026#34;n = 50\\n0.33%\u0026#34; ## 3 setosa 50 0.33 0.825 \u0026#34;n = 50\\n0.33%\u0026#34; Pie pie \u0026lt;- ggplot(plotdata, aes(x = \u0026#34;\u0026#34;, y = prop, fill = Species)) + geom_bar(stat = \u0026#34;identity\u0026#34;, color = \u0026#34;white\u0026#34;, width = 1) + coord_polar(theta = \u0026#34;y\u0026#34;, start = 0) + geom_text(aes(y = lab.ypos, label = num_prop), color = \u0026#34;white\u0026#34;) + scale_fill_manual(breaks = group_names, values = group_colors) + theme_void() pie Donut donut \u0026lt;- ggplot(plotdata, aes(x = 2, y = prop, fill = Species)) + geom_bar(stat = \u0026#34;identity\u0026#34;, color = \u0026#34;white\u0026#34;) + coord_polar(theta = \u0026#34;y\u0026#34;, start = 0) + geom_text(aes(y = lab.ypos, label = num_prop), color = \u0026#34;white\u0026#34;) + scale_fill_manual(breaks = group_names, values = group_colors) + annotate(\u0026#34;text\u0026#34;, x = 1, y = 0, label = \u0026#34;donut chart\u0026#34;) + theme_void() + xlim(0.5, 2.5) + theme(legend.position = c(.5, .45)) donut Mulitple circles chart One circle represents one traits of dataset, should we using multiple circles to characterize the different traits?\nTraits of mtcars: cyl, gear and carb\nData preparation data_select \u0026lt;- mtcars |\u0026gt; dplyr::select(cyl, gear, carb) |\u0026gt; mutate_if(is.numeric, as.character) |\u0026gt; mutate_if(is.character, as.factor) |\u0026gt; mutate(cyl = paste0(\u0026#34;cyl\u0026#34;, cyl), gear = paste0(\u0026#34;gear\u0026#34;, gear), carb = paste0(\u0026#34;carb\u0026#34;, carb)) str(data_select) ## \u0026#39;data.frame\u0026#39;:\t32 obs. of 3 variables: ## $ cyl : chr \u0026#34;cyl6\u0026#34; \u0026#34;cyl6\u0026#34; \u0026#34;cyl4\u0026#34; \u0026#34;cyl6\u0026#34; ... ## $ gear: chr \u0026#34;gear4\u0026#34; \u0026#34;gear4\u0026#34; \u0026#34;gear4\u0026#34; \u0026#34;gear3\u0026#34; ... ## $ carb: chr \u0026#34;carb4\u0026#34; \u0026#34;carb4\u0026#34; \u0026#34;carb1\u0026#34; \u0026#34;carb1\u0026#34; ... Calculating the Proportion per group plotdata2 \u0026lt;- data_select |\u0026gt; dplyr::group_by(cyl, gear, carb) |\u0026gt; dplyr::summarise(count = length(cyl)) head(plotdata2) ## # A tibble: 6 × 4 ## # Groups: cyl, gear [5] ## cyl gear carb count ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 cyl4 gear3 carb1 1 ## 2 cyl4 gear4 carb1 4 ## 3 cyl4 gear4 carb2 4 ## 4 cyl4 gear5 carb2 2 ## 5 cyl6 gear3 carb1 2 ## 6 cyl6 gear4 carb4 4 Function for drawing chart\nsingle or multiple traits for plotting\nvalues and position of label\nplotting\nget_circle \u0026lt;- function( inputdata, variables = c(\u0026#34;cyl\u0026#34;, \u0026#34;gear\u0026#34;, \u0026#34;carb\u0026#34;), label_size = 2.5, annotate_size = 4, col_number = 5, theme_legend_size = 8, theme_strip_size = 8, axis_size = 10, cyl_names = c(\u0026#34;cyl4\u0026#34;, \u0026#34;cyl6\u0026#34;,\u0026#34;cyl8\u0026#34;), cyl_cols = c(\u0026#34;#D51F26\u0026#34;, \u0026#34;#272E6A\u0026#34;, \u0026#34;#208A42\u0026#34;), gear_names = c(\u0026#34;gear3\u0026#34;, \u0026#34;gear4\u0026#34;, \u0026#34;gear5\u0026#34;), gear_cols = c(\u0026#34;#7DD06F\u0026#34;, \u0026#34;#844081\u0026#34;, \u0026#34;#688EC1\u0026#34;), carb_names = c(\u0026#34;carb1\u0026#34;, \u0026#34;carb2\u0026#34;, \u0026#34;carb3\u0026#34;, \u0026#34;carb4\u0026#34;, \u0026#34;carb6\u0026#34;, \u0026#34;carb8\u0026#34;), carb_cols = c(\u0026#34;#faa818\u0026#34;, \u0026#34;#41a30d\u0026#34;, \u0026#34;#fbdf72\u0026#34;, \u0026#34;#367d7d\u0026#34;, \u0026#34;#d33502\u0026#34;, \u0026#34;#6ebcbc\u0026#34;)) { dat_list \u0026lt;- list() for (i in 1:length(variables)) { temp_var \u0026lt;- rlang::sym(variables[i]) temp_dat \u0026lt;- inputdata %\u0026gt;% dplyr::select(!!temp_var, count) %\u0026gt;% dplyr::group_by(!!temp_var) %\u0026gt;% dplyr::summarise(count = sum(count), .groups = \u0026#34;drop\u0026#34;) dat_list[[i]] \u0026lt;- temp_dat } names(dat_list) \u0026lt;- variables get_plotdata \u0026lt;- function(dat) { dat_cln_list \u0026lt;- list() names_new_list \u0026lt;- list() colors_new_list \u0026lt;- list() for (i in 1:length(dat)) { if (names(dat)[i] == \u0026#34;cyl\u0026#34;) { cyl_cln \u0026lt;- dat[[i]] %\u0026gt;% dplyr::select(cyl, count) %\u0026gt;% dplyr::group_by(cyl) %\u0026gt;% dplyr::summarise(count = sum(count)) %\u0026gt;% dplyr::mutate(prop = round(count / sum(count), 3) * 100) %\u0026gt;% dplyr::ungroup() %\u0026gt;% dplyr::arrange(desc(cyl)) %\u0026gt;% dplyr::mutate(lab.ypos = cumsum(prop) - 0.5*prop) %\u0026gt;% dplyr::mutate(num_prop = paste0(\u0026#34;n = \u0026#34;, count, \u0026#34;\\n\u0026#34;, paste0(prop, \u0026#34;%\u0026#34;))) cyl_cln$level \u0026lt;- i cyl_cln$type \u0026lt;- \u0026#34;cyl\u0026#34; match_order_index1 \u0026lt;- sort(pmatch(unique(cyl_cln$cyl), cyl_names), decreasing = F) cyl_names_new \u0026lt;- cyl_names[match_order_index1] cyl_cols_new \u0026lt;- cyl_cols[match_order_index1] cyl_cln$fill \u0026lt;- factor(cyl_cln$cyl, levels = cyl_names_new, labels = cyl_cols_new) dat_cln_list[[i]] \u0026lt;- cyl_cln names_new_list[[i]] \u0026lt;- cyl_names_new colors_new_list[[i]] \u0026lt;- cyl_cols_new } else if (names(dat)[i] == \u0026#34;gear\u0026#34;) { gear_cln \u0026lt;- dat[[i]] %\u0026gt;% dplyr::select(gear, count) %\u0026gt;% dplyr::group_by(gear) %\u0026gt;% dplyr::summarise(count = sum(count)) %\u0026gt;% dplyr::mutate(prop = round(count / sum(count), 3) * 100) %\u0026gt;% …","date":1689292800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689314727,"objectID":"0245406adfb7699c95e74e723262a092","permalink":"https://zouhua.top/post/visualization/2023-07-14-donut/","publishdate":"2023-07-14T00:00:00Z","relpermalink":"/post/visualization/2023-07-14-donut/","section":"post","summary":"Using ggplot2 to visualize the results","tags":["Visualization"],"title":"R visualization: donut chart by ggplot2","type":"post"},{"authors":["Hua Zou"],"categories":null,"content":"","date":1687132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687132800,"objectID":"9d5e525e7aec86e0256049ceac343270","permalink":"https://zouhua.top/bookdown/2023-metabolome/","publishdate":"2023-07-01T00:00:00Z","relpermalink":"/bookdown/2023-metabolome/","section":"bookdown","summary":"Metabolomics is the large-scale study of small molecules, commonly known as metabolites, within cells, biofluids, tissues or organisms. Collectively, these small molecules and their interactions within a biological system are known as the metabolome. In this tutorial, two modules would be introduced, Statistical Analysis and Functional Analysis.","tags":null,"title":"Data analysis workflow on metabolomic data","type":"bookdown"},{"authors":null,"categories":["Visualization","ggplot2"],"content":"Introduction The simple boxplot only shows the distribution of values per group. However, more information should be displayed in one figure. Here, using points determined by proportion and we could observe the percentage of index per group.\nLoading required packages knitr::opts_chunk$set(message = FALSE, warning = FALSE) library(tidyverse) rm(list = ls()) options(stringsAsFactors = F) # group \u0026amp; color group_names \u0026lt;- c(\u0026#34;setosa\u0026#34;, \u0026#34;versicolor\u0026#34;, \u0026#34;virginica\u0026#34;) group_colors \u0026lt;- c(\u0026#34;#0073C2FF\u0026#34;, \u0026#34;#EFC000FF\u0026#34;, \u0026#34;#CD534CFF\u0026#34;) Data preparation Loading iris dataset\nCalculating ratio per species (Occurrence)\ndata(\u0026#34;iris\u0026#34;) plotdata \u0026lt;- iris |\u0026gt; dplyr::select(Sepal.Length, Species) |\u0026gt; dplyr::mutate(Species = factor(Species, levels = group_names)) |\u0026gt; dplyr::rename(Group = Species, Index = Sepal.Length) # threshold for prevalence occ_cutoff \u0026lt;- 5.2 occ_fun \u0026lt;- function(x) { return(round(length(x[x \u0026gt; occ_cutoff])/length(x), 4)) } plotOcc \u0026lt;- plotdata |\u0026gt; dplyr::group_by(Group) |\u0026gt; dplyr::summarise(occ = occ_fun(Index)) |\u0026gt; dplyr::mutate(occ_lab = paste0(occ * 100, \u0026#34;%\u0026#34;)) |\u0026gt; dplyr::mutate(position = min(plotdata$Index) - min(plotdata$Index) * 0.1) head(plotOcc) ## # A tibble: 3 × 4 ## Group occ occ_lab position ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 setosa 0.22 22% 3.87 ## 2 versicolor 0.9 90% 3.87 ## 3 virginica 0.98 98% 3.87 Plotting Create boxplot with errorbar using key functions: stat_boxplot, geom_boxplot .\nAdd points: geom_point for values and percentages, respectively.\nApply geom_text to display the text of Ratio.\npl \u0026lt;- ggplot(data = plotdata, aes(x = Group, y = Index, color = Group)) + stat_boxplot(geom = \u0026#34;errorbar\u0026#34;, width = 0.15) + geom_boxplot(width = .4, outlier.shape = NA) + geom_point(size = 2, shape = 5) + labs(x = \u0026#34;\u0026#34;) + scale_y_continuous(expand = expansion(mult = c(0.1, 0.1))) + geom_point(data = plotOcc, aes(x = Group, y = position, size = occ), show.legend = FALSE, shape = 1, stroke = 1) + geom_text(data = plotOcc, aes(x = Group, y = position, label = occ_lab), show.legend = FALSE) + scale_size_continuous(range = c(10, 12)) + scale_color_manual(values = group_colors) + coord_flip() + guides(color = \u0026#34;none\u0026#34;) + theme_classic() + theme(axis.title = element_text(size = 12, color = \u0026#34;black\u0026#34;, face = \u0026#34;bold\u0026#34;), axis.text = element_text(size = 10, color = \u0026#34;black\u0026#34;), text = element_text(size = 9, color = \u0026#34;black\u0026#34;)) pl Conclusion We not only observed the differences of Index between three groups but also found the prevalence of Index between three groups from the aforementioned figure.\nSystemic information devtools::session_info() ## ─ Session info ─────────────────────────────────────────────────────────────── ## setting value ## version R version 4.1.3 (2022-03-10) ## os macOS Big Sur/Monterey 10.16 ## system x86_64, darwin17.0 ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz Asia/Shanghai ## date 2023-07-24 ## pandoc 3.1.3 @ /Users/zouhua/opt/anaconda3/bin/ (via rmarkdown) ## ## ─ Packages ─────────────────────────────────────────────────────────────────── ## package * version date (UTC) lib source ## blogdown 1.18 2023-06-19 [2] CRAN (R 4.1.3) ## bookdown 0.34 2023-05-09 [2] CRAN (R 4.1.2) ## bslib 0.5.0 2023-06-09 [2] CRAN (R 4.1.3) ## cachem 1.0.8 2023-05-01 [2] CRAN (R 4.1.2) ## callr 3.7.3 2022-11-02 [2] CRAN (R 4.1.2) ## cli 3.6.1 2023-03-23 [2] CRAN (R 4.1.2) ## colorspace 2.1-0 2023-01-23 [2] CRAN (R 4.1.2) ## crayon 1.5.2 2022-09-29 [2] CRAN (R 4.1.2) ## devtools 2.4.5 2022-10-11 [2] CRAN (R 4.1.2) ## digest 0.6.33 2023-07-07 [1] CRAN (R 4.1.3) ## dplyr * 1.1.2 2023-04-20 [2] CRAN (R 4.1.2) ## ellipsis 0.3.2 2021-04-29 [2] CRAN (R 4.1.0) ## evaluate 0.21 2023-05-05 [2] CRAN (R 4.1.2) ## fansi 1.0.4 2023-01-22 [2] CRAN (R 4.1.2) ## farver 2.1.1 2022-07-06 [2] CRAN (R 4.1.2) ## fastmap 1.1.1 2023-02-24 [2] CRAN (R 4.1.2) ## forcats * 1.0.0 2023-01-29 [2] CRAN (R 4.1.2) ## fs 1.6.2 2023-04-25 [2] CRAN (R 4.1.2) ## generics 0.1.3 2022-07-05 [2] CRAN (R 4.1.2) ## ggplot2 * 3.4.2 2023-04-03 [2] CRAN (R 4.1.2) ## glue 1.6.2 2022-02-24 [2] CRAN (R 4.1.2) ## gtable 0.3.3 2023-03-21 [2] CRAN (R 4.1.2) ## highr 0.10 2022-12-22 [2] CRAN (R 4.1.2) ## hms 1.1.3 2023-03-21 [2] CRAN (R 4.1.2) ## htmltools 0.5.5 2023-03-23 [2] CRAN (R 4.1.2) ## htmlwidgets 1.6.2 2023-03-17 [2] CRAN (R 4.1.2) ## httpuv 1.6.11 2023-05-11 [2] CRAN (R 4.1.3) ## jquerylib 0.1.4 2021-04-26 [2] CRAN (R 4.1.0) ## jsonlite 1.8.7 2023-06-29 [2] CRAN (R 4.1.3) ## knitr 1.43 2023-05-25 [2] CRAN (R 4.1.3) ## labeling 0.4.2 2020-10-20 [2] CRAN (R 4.1.0) ## later 1.3.1 2023-05-02 [2] CRAN (R 4.1.2) ## lifecycle 1.0.3 2022-10-07 [2] CRAN (R 4.1.2) ## lubridate * 1.9.2 2023-02-10 [2] CRAN (R 4.1.2) ## magrittr 2.0.3 2022-03-30 [2] CRAN (R 4.1.2) ## memoise 2.0.1 2021-11-26 [2] CRAN (R 4.1.0) ## mime 0.12 2021-09-28 [2] CRAN (R 4.1.0) ## miniUI 0.1.1.1 2018-05-18 [2] CRAN (R 4.1.0) ## munsell 0.5.0 2018-06-12 [2] CRAN (R 4.1.0) ## pillar 1.9.0 2023-03-22 [2] CRAN (R 4.1.2) ## pkgbuild 1.4.2 2023-06-26 [2] CRAN (R 4.1.3) ## pkgconfig 2.0.3 2019-09-22 [2] …","date":1686700800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1686722727,"objectID":"574d40164ff748e566617234d94ff945","permalink":"https://zouhua.top/post/visualization/2023-06-14-boxplotpoint/","publishdate":"2023-06-14T00:00:00Z","relpermalink":"/post/visualization/2023-06-14-boxplotpoint/","section":"post","summary":"Using ggplot2 to visualize the results","tags":["Visualization"],"title":"R visualization: boxplot with points labeled in percentage by ggplot2","type":"post"},{"authors":["Hua Zou"],"categories":["Machine Learning"],"content":"Random Forest, a ensemble machine learning algorithm with multiple decision trees could be used for classification or regression algorithm, and it has an elegant way of dealing with nonlinear or linear data.\nRandom forest aims to reduce the previously mentioned correlation issue by choosing only a subsample of the feature space at each split. Essentially, it aims to make the trees de-correlated and prune the trees by setting a stopping criteria for node splits, which I will cover in more detail later.\nHowever, how to choose the optimal features to rebuild models is the key step for users. Herein, we gave the detailed tutorial step by step.\nImporting packages R packages used in this tutorial.\nknitr::opts_chunk$set(message = FALSE, warning = FALSE) library(dplyr) library(tibble) library(randomForest) library(ggplot2) library(data.table) library(caret) library(pROC) # rm(list = ls()) options(stringsAsFactors = F) options(future.globals.maxSize = 1000 * 1024^2) Importing data clean_data.csv is from Breast-cancer-risk-prediction. Using wget https://github.com/Jean-njoroge/Breast-cancer-risk-prediction/blob/master/data/clean-data.csv to download it.\nclean_data.csv contains 569 samples of malignant and benign tumor cells\nThe Breast Cancer datasets is available machine learning repository maintained by the University of California, Irvine. The dataset contains 569 samples of malignant and benign tumor cells.\nThe first two columns in the dataset store the unique ID numbers of the samples and the corresponding diagnosis (M=malignant, B=benign), respectively. The columns 3-32 contain 30 real-value features that have been computed from digitized images of the cell nuclei, which can be used to build a model to predict whether a tumor is benign or malignant. datset \u0026lt;- data.table::fread(\u0026#34;clean_data.csv\u0026#34;) head(datset[, 1:6]) V1 diagnosis radius_mean texture_mean perimeter_mean 1: 0 M 17.99 10.38 122.80 2: 1 M 20.57 17.77 132.90 3: 2 M 19.69 21.25 130.00 4: 3 M 11.42 20.38 77.58 5: 4 M 20.29 14.34 135.10 6: 5 M 12.45 15.70 82.57 Data Partition Splitting dataset into TrainSet and TestSet, the former is used to build random forest model and the latter is used to evaluate the performance of model.\nWe used caret::createDataPartition with parameters p = 0.7 to create trainData and testData dataset.\nmdat \u0026lt;- datset %\u0026gt;% dplyr::select(-V1) %\u0026gt;% dplyr::rename(Group = diagnosis) %\u0026gt;% dplyr::mutate(Group = factor(Group)) %\u0026gt;% data.frame() colnames(mdat) \u0026lt;- make.names(colnames(mdat)) set.seed(123) trainIndex \u0026lt;- caret::createDataPartition( mdat$Group, p = 0.7, list = FALSE, times = 1) trainData \u0026lt;- mdat[trainIndex, ] X_train \u0026lt;- trainData[, -1] y_train \u0026lt;- trainData[, 1] testData \u0026lt;- mdat[-trainIndex, ] X_test \u0026lt;- testData[, -1] y_test \u0026lt;- testData[, 1] Building model fitting model with default using randomForest based on trainData set.\nset.seed(123) rf_fit \u0026lt;- randomForest(Group ~ ., data = trainData, importance = TRUE, proximity = TRUE) rf_fit Call: randomForest(formula = Group ~ ., data = trainData, importance = TRUE, proximity = TRUE) Type of random forest: classification Number of trees: 500 No. of variables tried at each split: 5 OOB estimate of error rate: 4.26% Confusion matrix: B M class.error B 241 9 0.03600000 M 8 141 0.05369128 The OOB estimate of error rate is 4.26%, indicating the Accuracy of this RF model is 95.74%.\nBiomarkers ordered by MeanDecreaseAccuracy Ordering the features’ importance by MeanDecreaseAccuracy.\nimp_biomarker \u0026lt;- tibble::as_tibble(round(importance(rf_fit), 2), rownames = \u0026#34;Features\u0026#34;) %\u0026gt;% dplyr::arrange(desc(MeanDecreaseAccuracy)) head(imp_biomarker) # A tibble: 6 × 5 Features B M MeanDecreaseAccuracy MeanDecreaseGini \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; 1 concave.points_worst 14.6 12.5 18.0 24.0 2 perimeter_worst 13.7 11.6 16.2 26.8 3 radius_worst 13.1 11.4 16.1 21.2 4 area_worst 11.9 10.5 14.5 20.5 5 concavity_worst 8.94 9.74 13.3 6.46 6 texture_worst 8.27 10.6 12.9 3.27 MeanDecreaseAccuracy shows that the contribution of the features. For exmple, removing concave.points_worst would decrease the accuracy by 18.0%. The higher the MeanDecreaseAccuracy of the features, the more they contribute to the model.\nCross validation 5 replicates for the 5-folds cross-validation to obtain the mean error in order to avoid the over-fitting and under-fitting. Meanwhile, the optimal number of features is selected according to the number with the mean error.\nerror.cv \u0026lt;- c() for (i in 1:5){ print(i) set.seed(i) fit \u0026lt;- rfcv(trainx = X_train, trainy = y_train, cv.fold = 5, scale = \u0026#34;log\u0026#34;, step = 0.9) error.cv \u0026lt;- cbind(error.cv, fit$error.cv) } n.var \u0026lt;- as.numeric(rownames(error.cv)) colnames(error.cv) \u0026lt;- paste(\u0026#39;error\u0026#39;, 1:5, sep = \u0026#39;.\u0026#39;) err.mean \u0026lt;- apply(error.cv, 1, mean) err.df \u0026lt;- data.frame(num = n.var, err.mean = err.mean, error.cv) head(err.df[, 1:6]) num err.mean error.1 error.2 error.3 error.4 30 30 0.04110276 0.03759398 0.04260652 0.04260652 0.04260652 27 27 0.04160401 0.04010025 0.04010025 0.04761905 0.04010025 24 24 0.04260652 …","date":1682539994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682460794,"objectID":"879e2321435498ba3cfa11a41dc3722c","permalink":"https://zouhua.top/post/machine_learning/2023-04-27-ml010-rfoptimal/","publishdate":"2023-04-26T20:13:14Z","relpermalink":"/post/machine_learning/2023-04-27-ml010-rfoptimal/","section":"post","summary":"Selecting the optimal biomarkers in Random Forest.","tags":["machine learning","Random Forest"],"title":"How to choose the optimal features in Random Forest using R","type":"book"},{"authors":["Yifan Zhang","Siyuan Cheng","Hua Zou"],"categories":null,"content":"","date":1677196800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677196800,"objectID":"6ddc0feb5a914f6cfde3d924d007bc96","permalink":"https://zouhua.top/publication/2023-fcim/","publishdate":"2023-03-24T00:00:00Z","relpermalink":"/publication/2023-fcim/","section":"publication","summary":"In summary, we identified bacterial species and metabolic pathways that might be associated with the occurrence of irAEs in gastric, esophageal, and colon cancers. Ruminococcus callidus and Bacteroides xylanisolvens were enriched in patients without severe irAEs. Several microbial metabolic pathways involved in the urea cycle, including citrulline and arginine biosynthesis, were associated with irAEs. We also found that irAEs in different cancer types and toxicity in specific organs and the endocrine system were associated with different gut microbiota profiles. These findings provide the basis for future mechanistic exploration.","tags":null,"title":"Correlation of the gut microbiome and immune-related adverse events in gastrointestinal cancer patients treated with immune checkpoint inhibitors","type":"publication"},{"authors":["Hua Zou"],"categories":null,"content":"","date":1668816e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668816e3,"objectID":"43f8c58d2f291225c8d59afd674ebaa9","permalink":"https://zouhua.top/bookdown/2022-mgs/","publishdate":"2022-12-19T00:00:00Z","relpermalink":"/bookdown/2022-mgs/","section":"bookdown","summary":"Here, we give users one example to practice the MGS microbiota data analysis workflow by XMAS 2.0. By the way, we also recommend users handling your own microbiota data in a reasonable manner when you utilize this package or workflow. Pay attention to whether your data fit the methods this package provided.","tags":null,"title":"Data analysis workflow on metagenomic sequencing data using XMAS2","type":"bookdown"},{"authors":["Hua Zou"],"categories":["Tool"],"content":" Table Of Contents Introduction Loading R packages Studies in curatedMetagenomicData Acquiring absolute and relative abundance per study Converting datasets into phyloseq Summary Introduction R package curatedMetagenomicData provides more than 20,000 metaphlan’s results of samples. However, to download all the results seems not easy when using simple R codes. Here, we give users codes with step by step to grap all data.\nSince phyloseq object contains abundance table, taxa table and metadata etc, we convert the final metadata and profile including absolute and relative abundance into phyloseq object.\nLoading R packages suppressMessages(library(dplyr)) suppressMessages(library(tibble)) suppressMessages(library(curatedMetagenomicData)) library(phyloseq) Studies in curatedMetagenomicData All the studies’ information were stored in sampleMetadata. Therefore, we obtain the studies’ names by extracting the information from it.\ndata(\u0026#34;sampleMetadata\u0026#34;) availablediseases \u0026lt;- pull(sampleMetadata, study_condition) %\u0026gt;% table() %\u0026gt;% sort(decreasing = TRUE) studies \u0026lt;- lapply(names(availablediseases), function(x) { dplyr::filter(sampleMetadata, study_condition %in% x) %\u0026gt;% dplyr::pull(study_name) %\u0026gt;% unique() }) names(studies) \u0026lt;- names(availablediseases) Acquiring absolute and relative abundance per study Merging metadata without duplicated samples\nWhether there is absolute or relative abundance profile per study\nWhether the samples are identical between metadata and profile\nmetadata \u0026lt;- data.frame() profile_counts \u0026lt;- data.frame() profile_rb \u0026lt;- data.frame() for (i in 1:length(studies)) { disease_dir \u0026lt;- paste0(\u0026#34;./dataset/\u0026#34;, names(studies)[i]) if (!dir.exists(disease_dir)) { dir.create(disease_dir, recursive = TRUE) } for (disease in studies[[i]]) { rb_pattern \u0026lt;- paste0(disease, \u0026#34;.relative_abundance\u0026#34;) # absolute abundance dat_counts_list \u0026lt;- curatedMetagenomicData( pattern = rb_pattern, dryrun = FALSE, counts = TRUE, rownames = \u0026#34;long\u0026#34;) # relative_abundance dat_RB_list \u0026lt;- curatedMetagenomicData( pattern = rb_pattern, dryrun = FALSE, counts = FALSE, rownames = \u0026#34;long\u0026#34;) # whether there is a taxa profile if (length(dat_counts_list) \u0026gt; 0) { dat_counts \u0026lt;- dat_counts_list[[1]] disease_path_counts \u0026lt;- paste0(disease_dir, \u0026#34;/\u0026#34;, paste0(disease, \u0026#34;_counts.RData\u0026#34;)) save(dat_counts, file = disease_path_counts) # metadata \u0026amp; profile temp_metadata \u0026lt;- colData(dat_counts) %\u0026gt;% data.frame() temp_profile_counts \u0026lt;- assay(dat_counts) %\u0026gt;% data.frame() %\u0026gt;% tibble::rownames_to_column(\u0026#34;TaxaID\u0026#34;) if (length(profile_counts) == 0) { profile_counts \u0026lt;- temp_profile_counts } else { colnames(profile_counts) \u0026lt;- gsub(\u0026#34;-\u0026#34;, \u0026#34;.\u0026#34;, colnames(profile_counts)) colnames(temp_profile_counts) \u0026lt;- gsub(\u0026#34;-\u0026#34;, \u0026#34;.\u0026#34;, colnames(temp_profile_counts)) # the overlap samples sid_count \u0026lt;- intersect(colnames(temp_profile_counts), colnames(profile_counts)) sid_count \u0026lt;- sid_count[-which(sid_count == \u0026#34;TaxaID\u0026#34;)] if (length(sid_count) \u0026gt; 0) { profile_counts \u0026lt;- profile_counts[, !colnames(profile_counts)%in%sid_count] } profile_counts \u0026lt;- dplyr::full_join(temp_profile_counts, profile_counts, by = \u0026#34;TaxaID\u0026#34;) colnames(profile_counts) \u0026lt;- gsub(\u0026#34;-\u0026#34;, \u0026#34;.\u0026#34;, colnames(profile_counts)) } } # whether there is a taxa profile if (length(dat_RB_list) \u0026gt; 0) { dat_RB \u0026lt;- dat_RB_list[[1]] disease_path_RB \u0026lt;- paste0(disease_dir, \u0026#34;/\u0026#34;, paste0(disease, \u0026#34;_RB.RData\u0026#34;)) save(dat_RB, file = disease_path_RB) # metadata \u0026amp; profile temp_metadata \u0026lt;- colData(dat_RB) %\u0026gt;% data.frame() temp_profile_rb \u0026lt;- assay(dat_RB) %\u0026gt;% data.frame() %\u0026gt;% tibble::rownames_to_column(\u0026#34;TaxaID\u0026#34;) if (length(profile_rb) == 0) { profile_rb \u0026lt;- temp_profile_rb } else { colnames(profile_rb) \u0026lt;- gsub(\u0026#34;-\u0026#34;, \u0026#34;.\u0026#34;, colnames(profile_rb)) colnames(temp_profile_rb) \u0026lt;- gsub(\u0026#34;-\u0026#34;, \u0026#34;.\u0026#34;, colnames(temp_profile_rb)) # the overlap samples sid_rb \u0026lt;- intersect(colnames(temp_profile_rb), colnames(profile_rb)) sid_rb \u0026lt;- sid_rb[-which(sid_rb == \u0026#34;TaxaID\u0026#34;)] if (length(sid_rb) \u0026gt; 0) { profile_rb \u0026lt;- profile_rb[, !colnames(profile_rb)%in%sid_rb] } profile_rb \u0026lt;- dplyr::full_join(temp_profile_rb, profile_rb, by = \u0026#34;TaxaID\u0026#34;) colnames(profile_rb) \u0026lt;- gsub(\u0026#34;-\u0026#34;, \u0026#34;.\u0026#34;, colnames(profile_rb)) } } if (any(length(dat_counts_list) \u0026gt; 0, length(dat_RB_list) \u0026gt; 0)) { if (length(metadata) == 0) { metadata \u0026lt;- temp_metadata } else { # diff: metadata vs temp_metadata diff1 \u0026lt;- setdiff(colnames(metadata), colnames(temp_metadata)) diff1_df \u0026lt;- data.frame(matrix(NA, nrow = nrow(temp_metadata), ncol = length(diff1)), row.names = rownames(temp_metadata)) colnames(diff1_df) \u0026lt;- diff1 temp_metadata_new \u0026lt;- cbind(temp_metadata, diff1_df) # diff: temp_metadata vs metadata diff2 \u0026lt;- setdiff(colnames(temp_metadata), colnames(metadata)) diff2_df \u0026lt;- data.frame(matrix(NA, nrow = nrow(metadata), ncol = length(diff2)), row.names = rownames(metadata)) colnames(diff2_df) \u0026lt;- diff2 metadata_new \u0026lt;- cbind(metadata, diff2_df) rownames(metadata) \u0026lt;- gsub(\u0026#34;-\u0026#34;, \u0026#34;.\u0026#34;, rownames(metadata)) rownames(temp_metadata) \u0026lt;- gsub(\u0026#34;-\u0026#34;, \u0026#34;.\u0026#34;, rownames(temp_metadata)) # the overlap samples sid \u0026lt;- intersect(rownames(temp_metadata), rownames(metadata)) if (length(sid) \u0026gt; 0) …","date":1668111194,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668204794,"objectID":"33873cf144f18a5a31dd1e2082fb9858","permalink":"https://zouhua.top/post/tool/2022-11-10-metagenomics/","publishdate":"2022-11-10T20:13:14Z","relpermalink":"/post/tool/2022-11-10-metagenomics/","section":"post","summary":"Here, using Rscript to download data from curatedMetagenomicData.","tags":["metaphlan","microbiota"],"title":"How to download taxa profile of multiple samples","type":"post"},{"authors":["Hua Zou"],"categories":["Machine Learning"],"content":"Predictive Model using Random Forest Random Forest, a ensemble machine learning algorithm with multiple decision trees could be used for classification or regression algorithm, and it has an elegant way of dealing with nonlinear or linear data.\nRandom forest aims to reduce the previously mentioned correlation issue by choosing only a subsample of the feature space at each split. Essentially, it aims to make the trees de-correlated and prune the trees by setting a stopping criteria for node splits, which I will cover in more detail later.\nImportant Parameters number of estimators: the number of trees for constructing forest. max depth of each tree: the number of features per decision tree. max features per split: it affects the performance of the whole dicision tree. estimator also termed tree and depth control the complexity of the model, with larger estimators would result in better and high density forest and maximum depth helps fighting with overfitting.\nRandom Forest doesn\u0026#39;t require careful preprocessing such as normalization of data. Standardization is an important requirement for many classification models that should be handled when implementing pre-processing. Some models (like neural networks) can perform poorly if pre-processing isn’t considered, so the describe() function is a good indicator for standardization. Fortunately, random forest does not require any pre-processing.\nLoading libraries %matplotlib inline import matplotlib.pyplot as plt import pandas as pd import numpy as np import time import random from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score from sklearn.metrics import roc_curve, auc from sklearn.metrics import confusion_matrix from sklearn.model_selection import cross_val_score from sklearn.metrics import classification_report from sklearn.model_selection import GridSearchCV, train_test_split import seaborn as sns #plt.style.use(\u0026#39;fivethirtyeight\u0026#39;) plt.style.use(\u0026#39;fivethirtyeight\u0026#39;) sns.set_style(\u0026#34;darkgrid\u0026#34;) plt.rcParams[\u0026#39;figure.figsize\u0026#39;] = (8, 4) Importing data \u0026#39;\u0026#39;\u0026#39; # raw data data_df = pd.read_table(\u0026#39;./dataset/MergeData.tsv\u0026#39;, sep=\u0026#34;\\t\u0026#34;, index_col=0) data = data_df.reset_index(drop=True) data.head() # significant species data_df = pd.read_table(\u0026#39;./dataset/MergeData_clr_signif.tsv\u0026#39;, sep=\u0026#34;\\t\u0026#34;, index_col=0) data = data_df.reset_index(drop=True) data.head() \u0026#39;\u0026#39;\u0026#39; # CLR-transformed data data_df = pd.read_table(\u0026#39;./dataset/MergeData_clr.tsv\u0026#39;, sep=\u0026#34;\\t\u0026#34;, index_col=0) data = data_df.reset_index(drop=True) data.head() disease s__Bacteroides_plebeius s__Bacteroides_dorei s__Faecalibacterium_prausnitzii s__Eubacterium_eligens s__Bacteroides_ovatus s__Parabacteroides_distasonis s__Ruminococcus_gnavus s__Phascolarctobacterium_faecium s__Bacteroides_uniformis ... s__Bacteroides_finegoldii s__Haemophilus_sp_HMSC71H05 s__Clostridium_saccharolyticum s__Streptococcus_anginosus_group s__Streptococcus_sp_A12 s__Klebsiella_pneumoniae s__Bacteroides_coprocola s__Ruminococcus_lactaris s__Turicimonas_muris s__Proteobacteria_bacterium_CAG_139 0 healthy 10.262146 8.532694 7.729839 7.605931 7.477464 7.267232 7.074996 6.832596 6.738413 ... -3.441779 -3.441779 -3.441779 -3.441779 -3.441779 -3.441779 -3.441779 -3.441779 -3.441779 -3.441779 1 healthy 7.609333 4.466872 6.654823 4.236672 4.697277 3.518645 2.603642 -3.929140 5.619399 ... -3.929140 -3.929140 -3.929140 -3.929140 -3.929140 -3.929140 -3.929140 -3.929140 -3.929140 -3.929140 2 healthy 8.411267 7.884081 7.668467 -3.379905 6.577229 6.474488 6.883446 -3.379905 6.725545 ... -3.379905 -3.379905 -3.379905 -3.379905 -3.379905 -3.379905 -3.379905 -3.379905 -3.379905 -3.379905 3 healthy 6.261269 6.939669 5.819509 4.064784 4.026179 7.227381 3.407265 -3.632294 6.170285 ... -3.632294 -3.632294 -3.632294 -3.632294 -3.632294 -3.632294 -3.632294 -3.632294 -3.632294 -3.632294 4 CRC -0.026692 5.697435 3.808831 4.253082 4.284986 5.784833 6.384592 -5.007660 5.492187 ... 4.187313 2.872359 2.219946 0.713786 -5.007660 -5.007660 -5.007660 -5.007660 -5.007660 -5.007660 5 rows × 152 columns\nMissing values or Occurrence the occurrence per features\n#data.iloc[:, 1:].apply(lambda x: np.count_nonzero(x)/len(x), axis=0) print(\u0026#34;Here\u0026#39;s the dimensions of our data frame:\\n\u0026#34;, data.shape) print(\u0026#34;Here\u0026#39;s the data types of our columns:\\n\u0026#34;, data.dtypes) Here\u0026#39;s the dimensions of our data frame: (504, 152) Here\u0026#39;s the data types of our columns: disease object s__Bacteroides_plebeius float64 s__Bacteroides_dorei float64 s__Faecalibacterium_prausnitzii float64 s__Eubacterium_eligens float64 ... s__Klebsiella_pneumoniae float64 s__Bacteroides_coprocola float64 s__Ruminococcus_lactaris float64 s__Turicimonas_muris float64 s__Proteobacteria_bacterium_CAG_139 float64 Length: 152, dtype: object data.describe() s__Bacteroides_plebeius s__Bacteroides_dorei s__Faecalibacterium_prausnitzii s__Eubacterium_eligens s__Bacteroides_ovatus s__Parabacteroides_distasonis …","date":1667506394,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667513594,"objectID":"d960b367878d521f361075b27ae9a06f","permalink":"https://zouhua.top/post/machine_learning/2022-11-03-ml009-rf/","publishdate":"2022-11-03T20:13:14Z","relpermalink":"/post/machine_learning/2022-11-03-ml009-rf/","section":"post","summary":"Predictive Model using Random Forest on gut microbiota.","tags":["machine learning","microbiota"],"title":"Machine Learning on gut microbiota of patients with Colorectal cancer (9)","type":"book"},{"authors":["Hua Zou"],"categories":["Machine Learning"],"content":"Comparison between different classifiers Notebook 8: Comparison between different classifiers.\nThere are standard workflows in a machine learning project that can be automated. In Python scikit-learn, Pipelines help to clearly define and automate these workflows.\nPipelines help overcome common problems like data leakage in your test harness. Python scikit-learn provides a Pipeline utility to help automate machine learning workflows. Pipelines work by allowing for a linear sequence of data transforms to be chained together culminating in a modeling process that can be evaluated. Loading libraries %matplotlib inline import matplotlib.pyplot as plt # Create a pipeline that standardizes the data then creates a model #Load libraries for data processing import pandas as pd import numpy as np from scipy.stats import norm from sklearn.model_selection import train_test_split, KFold #from sklearn.cross_validation import cross_val_score, KFold from sklearn.model_selection import cross_val_score from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.pipeline import Pipeline from sklearn.model_selection import GridSearchCV from sklearn.linear_model import LogisticRegression from sklearn.tree import DecisionTreeClassifier from sklearn.neighbors import KNeighborsClassifier from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.naive_bayes import GaussianNB from sklearn.svm import SVC from sklearn.metrics import confusion_matrix from sklearn.metrics import accuracy_score from sklearn.metrics import classification_report # visualization import seaborn as sns plt.style.use(\u0026#39;fivethirtyeight\u0026#39;) sns.set_style(\u0026#34;white\u0026#34;) plt.rcParams[\u0026#39;figure.figsize\u0026#39;] = (8,4) #plt.rcParams[\u0026#39;axes.titlesize\u0026#39;] = \u0026#39;large\u0026#39; Importing data \u0026#39;\u0026#39;\u0026#39; # raw data data_df = pd.read_table(\u0026#39;./dataset/MergeData.tsv\u0026#39;, sep=\u0026#34;\\t\u0026#34;, index_col=0) data = data_df.reset_index(drop=True) data.head() # CLR-transformed data data_df = pd.read_table(\u0026#39;./dataset/MergeData_clr.tsv\u0026#39;, sep=\u0026#34;\\t\u0026#34;, index_col=0) data = data_df.reset_index(drop=True) data.head() \u0026#39;\u0026#39;\u0026#39; # significant species data_df = pd.read_table(\u0026#39;./dataset/MergeData_clr_signif.tsv\u0026#39;, sep=\u0026#34;\\t\u0026#34;, index_col=0) data = data_df.reset_index(drop=True) data.head() Evaluate Some Algorithms Now it is time to create some models of the data and estimate their accuracy on unseen data. Here is what we are going to cover in this step:\nSeparate out a validation dataset. Setup the test harness to use 10-fold cross validation. Build 5 different models Select the best model Validation Dataset # Split-out validation dataset array = data.values X = array[:, 1:data.shape[1]] y = array[:, 0] # Divide records in training and testing sets. X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7) #transform the class labels from their original string representation (CRC and healthy) into integers le = LabelEncoder() y = le.fit_transform(y) Evaluate Algorithms: Baseline # turn off warnings import warnings def action_with_warnings(): warnings.warn(\u0026#34;forbid warnings\u0026#39; display\u0026#34;) with warnings.catch_warnings(record=True): action_with_warnings() # Spot-Check Algorithms models = [] models.append(( \u0026#39;LR\u0026#39; , LogisticRegression())) models.append(( \u0026#39;LDA\u0026#39; , LinearDiscriminantAnalysis())) models.append(( \u0026#39;KNN\u0026#39; , KNeighborsClassifier())) models.append(( \u0026#39;CART\u0026#39; , DecisionTreeClassifier())) models.append(( \u0026#39;NB\u0026#39; , GaussianNB())) models.append(( \u0026#39;SVM\u0026#39; , SVC())) # Test options and evaluation metric num_folds = 10 num_instances = len(X_train) seed = 7 scoring = \u0026#39;accuracy\u0026#39; # Test options and evaluation metric num_folds = 10 num_instances = len(X_train) seed = 7 scoring = \u0026#39;accuracy\u0026#39; results = [] names = [] for name, model in models: #kfold = KFold(n=num_instances, n_folds=num_folds, random_state=seed) kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True) cv_results = cross_val_score(model, X=X_train, y=y_train, cv=kfold, scoring=scoring) results.append(cv_results) names.append(name) msg = \u0026#34;%s: %f (%f)\u0026#34; % (name, cv_results.mean(), cv_results.std()) print(msg) print(\u0026#39;-\u0026gt; 10-Fold cross-validation accurcay score for the training data for six classifiers\u0026#39;) LR: 0.682063 (0.099066) LDA: 0.684921 (0.101168) KNN: 0.585079 (0.080096) CART: 0.616825 (0.057095) NB: 0.690873 (0.105033) SVM: 0.679286 (0.076846) -\u0026gt; 10-Fold cross-validation accurcay score for the training data for six classifiers Observation The results suggest That both GaussianNB and SVM may be worth further study. These are just mean accuracy values. It is always wise to look at the distribution of accuracy values calculated across cross validation folds. We can do that graphically using box and whisker plots.\n# Compare Algorithms fig = plt.figure() fig.suptitle( \u0026#39;Algorithm Comparison\u0026#39; ) ax = fig.add_subplot(111) plt.boxplot(results) ax.set_xticklabels(names) plt.show() Observation The results show a similar tight distribution for all classifiers except KNN which is …","date":1667419994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667427194,"objectID":"1c1269aad5bd7b2f92cdcd807a432214","permalink":"https://zouhua.top/post/machine_learning/2022-11-02-ml008-compclassifier/","publishdate":"2022-11-02T20:13:14Z","relpermalink":"/post/machine_learning/2022-11-02-ml008-compclassifier/","section":"post","summary":"Comparison between different classifiers.","tags":["machine learning","microbiota"],"title":"Machine Learning on gut microbiota of patients with Colorectal cancer (8)","type":"book"},{"authors":["Hua Zou"],"categories":["Machine Learning"],"content":"Optimizing the SVM Classifier Notebook 7: Optimizing the Support Vector Classifier.\nMachine learning models are parameterized so that their behavior can be tuned for a given problem. Models can have many parameters and finding the best combination of parameters can be treated as a search problem. In this notebook, I aim to tune parameters of the SVM Classification model using scikit-learn.\nLoading libraries %matplotlib inline import matplotlib.pyplot as plt #Load libraries for data processing import pandas as pd import numpy as np from scipy.stats import norm ## Supervised learning. from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import train_test_split from sklearn.svm import SVC from sklearn.model_selection import cross_val_score from sklearn.model_selection import GridSearchCV from sklearn.pipeline import make_pipeline from sklearn.metrics import confusion_matrix from sklearn import metrics, preprocessing from sklearn.metrics import classification_report from sklearn.feature_selection import SelectKBest, f_regression # visualization import seaborn as sns plt.style.use(\u0026#39;fivethirtyeight\u0026#39;) sns.set_style(\u0026#34;white\u0026#34;) plt.rcParams[\u0026#39;figure.figsize\u0026#39;] = (8,4) #plt.rcParams[\u0026#39;axes.titlesize\u0026#39;] = \u0026#39;large\u0026#39; Importing data \u0026#39;\u0026#39;\u0026#39; # raw data data_df = pd.read_table(\u0026#39;./dataset/MergeData.tsv\u0026#39;, sep=\u0026#34;\\t\u0026#34;, index_col=0) data = data_df.reset_index(drop=True) data.head() # CLR-transformed data data_df = pd.read_table(\u0026#39;./dataset/MergeData_clr.tsv\u0026#39;, sep=\u0026#34;\\t\u0026#34;, index_col=0) data = data_df.reset_index(drop=True) data.head() \u0026#39;\u0026#39;\u0026#39; # significant species data_df = pd.read_table(\u0026#39;./dataset/MergeData_clr_signif.tsv\u0026#39;, sep=\u0026#34;\\t\u0026#34;, index_col=0) data = data_df.reset_index(drop=True) data.head() Building a predictive model and evaluate with 5-cross validation using support vector classifies (ref Predictive model using Support Vector Machine) for details #Assign predictors to a variable of ndarray (matrix) type array = data.values X = array[:, 1:data.shape[1]] y = array[:, 0] #transform the class labels from their original string representation (M and B) into integers le = LabelEncoder() y = le.fit_transform(y) # Normalize the data (center around 0 and scale to remove the variance). scaler = StandardScaler() Xs = scaler.fit_transform(X) from sklearn.decomposition import PCA # feature extraction pca = PCA(n_components=10) fit = pca.fit(Xs) X_pca = pca.transform(Xs) # 5. Divide records in training and testing sets. X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=2, stratify=y) # 6. Create an SVM classifier and train it on 70% of the data set. clf = SVC(probability=True) clf.fit(X_train, y_train) #7. Analyze accuracy of predictions on 30% of the holdout test sample. classifier_score = clf.score(X_test, y_test) print (\u0026#39;\\nThe classifier accuracy score is {:03.2f}\\n\u0026#39;.format(classifier_score)) clf2 = make_pipeline(SelectKBest(f_regression, k=3),SVC(probability=True)) scores = cross_val_score(clf2, X_pca, y, cv=3) # Get average of 5-fold cross-validation score using an SVC estimator. n_folds = 5 cv_error = np.average(cross_val_score(SVC(), X_pca, y, cv=n_folds)) #print (\u0026#39;\\nThe {}-fold cross-validation accuracy score for this classifier is {:.2f}\\n\u0026#39;.format(n_folds, cv_error)) y_pred = clf.fit(X_train, y_train).predict(X_test) cm = metrics.confusion_matrix(y_test, y_pred) print(classification_report(y_test, y_pred )) fig, ax = plt.subplots(figsize=(5, 5)) ax.matshow(cm, cmap=plt.cm.Reds, alpha=0.3) for i in range(cm.shape[0]): for j in range(cm.shape[1]): ax.text(x=j, y=i, s=cm[i, j], va=\u0026#39;center\u0026#39;, ha=\u0026#39;center\u0026#39;) plt.xlabel(\u0026#39;Predicted Values\u0026#39;, ) plt.ylabel(\u0026#39;Actual Values\u0026#39;) plt.show() The classifier accuracy score is 0.60 precision recall f1-score support 0 0.64 0.50 0.56 78 1 0.57 0.70 0.63 74 accuracy 0.60 152 macro avg 0.61 0.60 0.60 152 weighted avg 0.61 0.60 0.59 152 Importance of optimizing a classifier We can tune two key parameters of the SVM algorithm:\nthe value of C (how much to relax the margin) and the type of kernel. The default for SVM (the SVC class) is to use the Radial Basis Function (RBF) kernel with a C value set to 1.0. Like with KNN, we will perform a grid search using 10-fold cross validation with a standardized copy of the training dataset. We will try a number of simpler kernel types and C values with less bias and more bias (less than and more than 1.0 respectively).\nPython scikit-learn provides two simple methods for algorithm parameter tuning:\nGrid Search Parameter Tuning. Random Search Parameter Tuning. # Train classifiers. kernel_values = [ \u0026#39;linear\u0026#39; , \u0026#39;poly\u0026#39; , \u0026#39;rbf\u0026#39; , \u0026#39;sigmoid\u0026#39; ] param_grid = {\u0026#39;C\u0026#39;: np.logspace(-3, 2, 6), \u0026#39;gamma\u0026#39;: np.logspace(-3, 2, 6),\u0026#39;kernel\u0026#39;: kernel_values} grid = GridSearchCV(SVC(), param_grid=param_grid, cv=5) grid.fit(X_train, y_train) GridSearchCV(cv=5, estimator=SVC(), param_grid={\u0026#39;C\u0026#39;: array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02]), \u0026#39;gamma\u0026#39;: array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, …","date":1667333594,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667340794,"objectID":"5742820d047a3381b5943f8cdcb031eb","permalink":"https://zouhua.top/post/machine_learning/2022-11-01-ml007-optialclassifier/","publishdate":"2022-11-01T20:13:14Z","relpermalink":"/post/machine_learning/2022-11-01-ml007-optialclassifier/","section":"post","summary":"Tuning parameters for optimal Classifier.","tags":["machine learning","microbiota"],"title":"Machine Learning on gut microbiota of patients with Colorectal cancer (7)","type":"book"},{"authors":["Hua Zou"],"categories":["Machine Learning"],"content":"Predictive model using Support Vector Machine (SVM) Notebook 6 Predictive model using Support Vector Machine (svm).\nSupport vector machines (SVMs) learning algorithm will be used to build the predictive model. SVMs are one of the most popular classification algorithms, and have an elegant way of transforming nonlinear data so that one can use a linear algorithm to fit a linear model to the data (Cortes and Vapnik 1995).\nKernelized support vector machines are powerful models and perform well on a variety of datasets.\nSVMs allow for complex decision boundaries, even if the data has only a few features.\nThey work well on low-dimensional and high-dimensional data (i.e., few and many features), but don’t scale very well with the number of samples.\nRunning an SVM on data with up to 10,000 samples might work well, but working with datasets of size 100,000 or more can become challenging in terms of runtime and memory usage.\nSVMs requires careful preprocessing of the data and tuning of the parameters. This is why, these days, most people instead use tree-based models such as random forests or gradient boosting (which require little or no preprocessing) in many applications.\nSVM models are hard to inspect; it can be difficult to understand why a particular prediction was made, and it might be tricky to explain the model to a nonexpert.\nImportant Parameters The important parameters in kernel SVMs are the\nRegularization parameter C,\nThe choice of the kernel,(linear, radial basis function(RBF) or polynomial)\nKernel-specific parameters.\ngamma and C both control the complexity of the model, with large values in either resulting in a more complex model. Therefore, good settings for the two parameters are usually strongly correlated, and C and gamma should be adjusted together.\nLoading essential libraries %matplotlib inline import matplotlib.pyplot as plt import pandas as pd import numpy as np from scipy.stats import norm ## Supervised learning. from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import train_test_split from sklearn.svm import SVC from sklearn.model_selection import cross_val_score from sklearn.pipeline import make_pipeline from sklearn.metrics import confusion_matrix from sklearn import metrics, preprocessing from sklearn.metrics import classification_report # visualization import seaborn as sns plt.style.use(\u0026#39;fivethirtyeight\u0026#39;) sns.set_style(\u0026#34;white\u0026#34;) plt.rcParams[\u0026#39;figure.figsize\u0026#39;] = (8, 4) #plt.rcParams[\u0026#39;axes.titlesize\u0026#39;] = \u0026#39;large\u0026#39; Importing data \u0026#39;\u0026#39;\u0026#39; # raw data data_df = pd.read_table(\u0026#39;./dataset/MergeData.tsv\u0026#39;, sep=\u0026#34;\\t\u0026#34;, index_col=0) data = data_df.reset_index(drop=True) data.head() # CLR-transformed data data_df = pd.read_table(\u0026#39;./dataset/MergeData_clr.tsv\u0026#39;, sep=\u0026#34;\\t\u0026#34;, index_col=0) data = data_df.reset_index(drop=True) data.head() \u0026#39;\u0026#39;\u0026#39; # significant species data_df = pd.read_table(\u0026#39;./dataset/MergeData_clr_signif.tsv\u0026#39;, sep=\u0026#34;\\t\u0026#34;, index_col=0) data = data_df.reset_index(drop=True) data.head() Transformation # Assign predictors to a variable of ndarray (matrix) type array = data.values X = array[:, 1:data.shape[1]] # features y = array[:, 0] # transform the class labels from their original string representation (CRC and healthy) into integers le = LabelEncoder() y = le.fit_transform(y) # Normalize the data (center around 0 and scale to remove the variance). scaler = StandardScaler() Xs = scaler.fit_transform(X) Classification with cross-validation As discussed in notebook Pre-Processing the data splitting the data into test and training sets is crucial to avoid overfitting. This allows generalization of real, previously-unseen data. Cross-validation extends this idea further. Instead of having a single train/test split, we specify so-called folds so that the data is divided into similarly-sized folds.\nTraining occurs by taking all folds except one – referred to as the holdout sample.\nOn the completion of the training, you test the performance of your fitted model using the holdout sample.\nThe holdout sample is then thrown back with the rest of the other folds, and a different fold is pulled out as the new holdout sample.\nTraining is repeated again with the remaining folds and we measure performance using the holdout sample. This process is repeated until each fold has had a chance to be a test or holdout sample.\nThe expected performance of the classifier, called cross-validation error, is then simply an average of error rates computed on each holdout sample.\nThis process is demonstrated by first performing a standard train/test split, and then computing cross-validation error.\n# Divide records in training and testing sets. X_train, X_test, y_train, y_test = train_test_split(Xs, y, test_size=0.3, random_state=2, stratify=y) # Create an SVM classifier and train it on 70% of the data set. clf = SVC(probability=True) clf.fit(X_train, y_train) # Analyze accuracy of predictions on 30% of the holdout test sample. classifier_score = …","date":1667247194,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667254394,"objectID":"ef472030356b5dba1db59042c38601d7","permalink":"https://zouhua.top/post/machine_learning/2022-10-31-ml006-svm/","publishdate":"2022-10-31T20:13:14Z","relpermalink":"/post/machine_learning/2022-10-31-ml006-svm/","section":"post","summary":"Predictive model using Support Vector Machine on gut microbiota.","tags":["machine learning","microbiota"],"title":"Machine Learning on gut microbiota of patients with Colorectal cancer (6)","type":"book"},{"authors":["Hua Zou"],"categories":["Machine Learning"],"content":"Differential Analysis Notebook 5 Differential Analysis.\nThere are 152 species in our dataset. In order to clinically interpret the potential biomarkers identified by the next step in machine learning, we perform the differential analysis to figure out the significant species as the inputs for building models. The method and criterion are as follows:\nT-test based on CLR tranformed data; Pvalue or Adjusted-Pvalue is less than 0.05; the absolute values of Log2Foldchange on Mean values is more than 1. Goal Find the significant features of the data and choose them for machine learning.\nLoading data and essential libraries import pandas as pd import numpy as np data_df = pd.read_table(\u0026#39;./dataset/MergeData_clr.tsv\u0026#39;, sep=\u0026#34;\\t\u0026#34;, index_col=0) data = data_df.reset_index(drop=True) data.head() T test Since profile are CLR transformed data, we choose t test to identify the significant species\nfrom itertools import combinations from scipy import stats as st def all_pairwise(df, compare_col = \u0026#39;disease\u0026#39;): decade_pairs = [(i, j) for i, j in combinations(df[compare_col].unique().tolist(), 2)] # or add a list of colnames to function signature cols = list(df.columns) cols.remove(compare_col) list_of_dfs = [] for pair in decade_pairs: for col in cols: c1 = df[df[compare_col] == pair[0]][col] c2 = df[df[compare_col] == pair[1]][col] results = st.ttest_ind(c1, c2, nan_policy=\u0026#39;omit\u0026#39;) tmp = pd.DataFrame({\u0026#39;dec1\u0026#39;: pair[0], \u0026#39;dec2\u0026#39;: pair[1], \u0026#39;tstat\u0026#39;: results.statistic, \u0026#39;pvalue\u0026#39;: results.pvalue}, index = [col]) list_of_dfs.append(tmp) df_stats = pd.concat(list_of_dfs) return df_stats ttest_res = all_pairwise(data) ttest_res.head() FDR correction def correct_pvalues_for_multiple_testing(pvalues, correction_type = \u0026#34;Benjamini-Hochberg\u0026#34;): \u0026#34;\u0026#34;\u0026#34; consistent with R - print correct_pvalues_for_multiple_testing([0.0, 0.01, 0.029, 0.03, 0.031, 0.05, 0.069, 0.07, 0.071, 0.09, 0.1]) \u0026#34;\u0026#34;\u0026#34; from numpy import array, empty pvalues = array(pvalues) #n = float(pvalues.shape[0]) n = pvalues.shape[0] new_pvalues = empty(n) if correction_type == \u0026#34;Bonferroni\u0026#34;: new_pvalues = n * pvalues elif correction_type == \u0026#34;Bonferroni-Holm\u0026#34;: values = [ (pvalue, i) for i, pvalue in enumerate(pvalues) ] values.sort() for rank, vals in enumerate(values): pvalue, i = vals new_pvalues[i] = (n-rank) * pvalue elif correction_type == \u0026#34;Benjamini-Hochberg\u0026#34;: values = [ (pvalue, i) for i, pvalue in enumerate(pvalues) ] values.sort() values.reverse() new_values = [] for i, vals in enumerate(values): rank = n - i pvalue, index = vals new_values.append((n/rank) * pvalue) for i in range(0, int(n)-1): if new_values[i] \u0026lt; new_values[i+1]: new_values[i+1] = new_values[i] for i, vals in enumerate(values): pvalue, index = vals new_pvalues[index] = new_values[i] return new_pvalues ttest_res[\u0026#39;Adjusted-pvalue\u0026#39;] = correct_pvalues_for_multiple_testing(ttest_res[\u0026#39;pvalue\u0026#39;], correction_type = \u0026#34;Benjamini-Hochberg\u0026#34;) ttest_res_sort = ttest_res.sort_values(by=\u0026#39;Adjusted-pvalue\u0026#39;) ttest_res_sort.head() Significant species with pvalue less than 0.05 threshold = 0.05 # pvalue or Adjusted-pvalue signif_species = ttest_res_sort.loc[ttest_res_sort[\u0026#39;pvalue\u0026#39;] \u0026lt; threshold] signif_species.shape (40, 5) There are 40 significant species identified by pvalue between CRC and healthy group\nOutput Selecting the 40 species profile to downstream analysis\ndata_signif = pd.concat([data[\u0026#39;disease\u0026#39;], data[signif_species.index.tolist()]], axis=1) data_signif.to_csv(\u0026#39;./dataset/MergeData_clr_signif.tsv\u0026#39;, sep=\u0026#39;\\t\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;, index=True) data_signif.head() correlation matrix # plot correlation matrix import pandas as pd import numpy as np import seaborn as sns from matplotlib import pyplot as plt plt.style.use(\u0026#39;fivethirtyeight\u0026#39;) sns.set_style(\u0026#34;white\u0026#34;) # Compute the correlation matrix data_species = data_signif.iloc[:, 1:data_signif.shape[1]] corr = data_species.corr(method=\u0026#34;spearman\u0026#34;) # Generate a mask for the upper triangle mask = np.zeros_like(corr, dtype=\u0026#39;bool\u0026#39;) mask[np.triu_indices_from(mask)] = True # Set up the matplotlib figure fig, ax = plt.subplots(figsize=(20, 20)) plt.title(\u0026#39;CRC Species Correlation\u0026#39;) # Generate a custom diverging colormap cmap = sns.diverging_palette(260, 10, as_cmap=True) # Draw the heatmap with the mask and correct aspect ratio sns.heatmap(corr, vmax=1.1, square=\u0026#39;square\u0026#39;, cmap=cmap, mask=mask, ax=ax, annot=True, fmt=\u0026#39;.1g\u0026#39;, linewidths=2) ​ A Summary of the significant species here: t-test on microbiota data FDR correction on pvalue of t-test obtain significant species Referenece How to implement R’s p.adjust in Python T Test on Multiple Columns in Dataframe ","date":1667160794,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667167994,"objectID":"f327c0f2e244bcfe0eab69e91ecf51bd","permalink":"https://zouhua.top/post/machine_learning/2022-10-30-ml005-differential/","publishdate":"2022-10-30T20:13:14Z","relpermalink":"/post/machine_learning/2022-10-30-ml005-differential/","section":"post","summary":"Differential Analysis for feature selection before building machine learning model.","tags":["machine learning","microbiota"],"title":"Machine Learning on gut microbiota of patients with Colorectal cancer (5)","type":"book"},{"authors":["Hua Zou"],"categories":["Machine Learning"],"content":"Data PreProcessing Notebook 4 Pre-Processing the data.\nData preprocessing is a crucial step for any data analysis problem. It is often a very good idea to prepare your data in such way to best expose the structure of the problem to the machine learning algorithms that you intend to use. This involves a number of activities such as:\nAssigning numerical values to categorical data; Handling missing values; Normalizing the features (so that features on small scales do not dominate when fitting a model to the data). In Exploratory Data Analysis, I explored the data, to help gain insight on the distribution of the data as well as how the attributes correlate to each other. I identified some features of interest. In this notebook I use feature selection to reduce high-dimension data, feature extraction and transformation for dimensional reduction.\nGoal Find the most predictive features of the data and filter it so it will enhance the predictive power of the analytics model.\nLoading data and essential libraries %matplotlib inline import matplotlib.pyplot as plt import pandas as pd import numpy as np from scipy.stats import norm # visualization import seaborn as sns plt.style.use(\u0026#39;fivethirtyeight\u0026#39;) sns.set_style(\u0026#34;white\u0026#34;) plt.rcParams[\u0026#39;figure.figsize\u0026#39;] = (8,4) #plt.rcParams[\u0026#39;axes.titlesize\u0026#39;] = \u0026#39;large\u0026#39; Importing data data_df = pd.read_table(\u0026#39;./dataset/MergeData.tsv\u0026#39;, sep=\u0026#34;\\t\u0026#34;, index_col=0) data = data_df.reset_index(drop=True) data.head() Label encoding Here, I assign the features to a NumPy array X, and transform the class labels from their original string representation (CRC and healthy) into integers\narray = data.values X_temp = array[:, 1:data.shape[1]] y = array[:, 0] array array([[\u0026#39;healthy\u0026#39;, 46509517, 8249892, ..., 0, 0, 0], [\u0026#39;healthy\u0026#39;, 5334509, 230275, ..., 0, 0, 0], [\u0026#39;healthy\u0026#39;, 6868169, 4054008, ..., 0, 0, 0], ..., [\u0026#39;healthy\u0026#39;, 0, 0, ..., 0, 0, 0], [\u0026#39;healthy\u0026#39;, 2286204, 242316, ..., 630160, 653, 191409], [\u0026#39;healthy\u0026#39;, 0, 0, ..., 0, 0, 0]], dtype=object) centered log-ratio (clr) transformation Transforms compositions from Aitchison geometry to the real space (skbio.stats.composition.clr).\nfrom skbio.stats.composition import clr pseude_value = np.amin(np.array(X_temp)[X_temp != np.amin(X_temp)]) data_temp = data.iloc[:, 1:data.shape[1]].T data_clr = data_temp.apply(lambda x: clr(x + pseude_value), axis=0).T X = data_clr.values[:, 0:data_clr.shape[1]] X array([[10.26214602, 8.53269436, 7.7298392 , ..., -3.44177889, -3.44177889, -3.44177889], [ 7.60933349, 4.46687166, 6.65482253, ..., -3.92913994, -3.92913994, -3.92913994], [ 8.41126745, 7.88408118, 7.66846676, ..., -3.37990451, -3.37990451, -3.37990451], ..., [-1.88844976, -1.88844976, 6.09721786, ..., -1.88844976, -1.88844976, -1.88844976], [ 7.74550107, 5.50128747, 7.86826035, ..., 6.45688651, -0.33872724, 5.26551437], [-1.22443516, -1.22443516, -1.22443516, ..., -1.22443516, -1.22443516, -1.22443516]]) transform the class labels from their original string representation (CRC and healthy) into integers from sklearn.preprocessing import LabelEncoder le = LabelEncoder() y = le.fit_transform(y) #Call the transform method of LabelEncorder on two dummy variables #le.transform ([\u0026#39;CRC\u0026#39;, \u0026#39;healthy\u0026#39;]) y array([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) After encoding the class labels(disease) in an array y, the Responsed patients are now represented as class 1(i.e prescence of Response) and the Non-Responsed patients are represented as class 0 (i.e healthy), respectively, illustrated by calling the transform method of LabelEncorder …","date":1667074394,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667081594,"objectID":"8ce44869d16e09d570d820f26382542d","permalink":"https://zouhua.top/post/machine_learning/2022-10-29-ml004-preprocess/","publishdate":"2022-10-29T20:13:14Z","relpermalink":"/post/machine_learning/2022-10-29-ml004-preprocess/","section":"post","summary":"Data PreProcessing before building machine learning model.","tags":["machine learning","microbiota"],"title":"Machine Learning on gut microbiota of patients with Colorectal cancer (4)","type":"book"},{"authors":["Hua Zou"],"categories":["Machine Learning"],"content":"Exploratory Data Analysis Notebook 3 Exploratory Data Analysis.\nNow that we have a good intuitive sense of the data, Next step involves taking a closer look at attributes and data values. In this section, I am getting familiar with the data, which will provide useful knowledge for data pre-processing.\nObjectives of Data Exploration Exploratory data analysis (EDA) is a very important step which takes place after feature engineering and acquiring data and it should be done before any modeling. This is because it is very important for a data scientist to be able to understand the nature of the data without making assumptions. The results of data exploration can be extremely useful in grasping the structure of the data, the distribution of the values, and the presence of extreme values and interrelationships within the data set.\nThe purpose of EDA is:\nTo use summary statistics and visualizations to better understand data, Finding clues about the tendencies of the data, its quality and to formulate assumptions and the hypothesis of our analysis For data preprocessing to be successful, it is essential to have an overall picture of your data Basic statistical descriptions can be used to identify properties of the data and highlight which data values should be treated as noise or outliers. Next step is to explore the data. There are two approached used to examine the data using:\nDescriptive statistics is the process of condensing key characteristics of the data set into simple numeric metrics. Some of the common metrics used are mean, standard deviation, and correlation.\nVisualization is the process of projecting the data, or parts of it, into Cartesian space or into abstract images. In the data mining process, data exploration is leveraged in many different steps including preprocessing, modeling, and interpretation of results.\nSummary statistics are measurements meant to describe data. In the field of descriptive statistics, there are many summary measurements\nLoading libraries %matplotlib inline import matplotlib.pyplot as plt import pandas as pd import numpy as np from scipy.stats import norm import seaborn as sns import statistics as st Descriptive statistics data = pd.read_table(\u0026#39;./dataset/MergeData.tsv\u0026#39;, sep=\u0026#34;\\t\u0026#34;, index_col=0) data.head() basic descriptive statistics data.describe() distribution data.skew() /var/folders/82/kf2cy4v112b374jb5xcvmwh40000gn/T/ipykernel_71511/1188251951.py:1: FutureWarning: The default value of numeric_only in DataFrame.skew is deprecated. In a future version, it will default to False. In addition, specifying \u0026#39;numeric_only=None\u0026#39; is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning. data.skew() s__Bacteroides_plebeius 3.829753 s__Bacteroides_dorei 3.183878 s__Faecalibacterium_prausnitzii 1.662481 s__Eubacterium_eligens 5.588910 s__Bacteroides_ovatus 7.612611 ... s__Klebsiella_pneumoniae 11.966032 s__Bacteroides_coprocola 3.615256 s__Ruminococcus_lactaris 5.572388 s__Turicimonas_muris 11.987903 s__Proteobacteria_bacterium_CAG_139 7.546497 Length: 151, dtype: float64 The skew result show a positive (right) or negative (left) skew. Values closer to zero show less skew. From the results, we can see that the relative abundance of most taxa are right skew. Since this, we should use CLR transformation to normalize the data\ndisease data.disease.unique() array([\u0026#39;healthy\u0026#39;, \u0026#39;CRC\u0026#39;], dtype=object) Group by disease and review the output diag_gr = data.groupby(\u0026#39;disease\u0026#39;, axis=0) pd.DataFrame(diag_gr.size(), columns=[\u0026#39;# of observations\u0026#39;]) Check binary encoding from 01.DataClean to confirm the coversion of the disease categorical data into numeric, where\nCRC = 1 (indicates prescence of disease) healthy = 0 (indicates control) Observation 258 observations indicating the prescence of CRC and 111 show control\nLets confirm this, by ploting the histogram\nUnimodal Data Visualizations One of the main goals of visualizing the data here is to observe which features are most helpful in predicting CRC or healthy. The other is to see general trends that may aid us in model selection and hyper parameter selection.\nApply 4 techniques that you can use to understand each attribute of your dataset independently.\nHistograms. Density Plots. Box and Whisker Plots. Scatter Plots sns.set_style(\u0026#34;white\u0026#34;) sns.set_context({\u0026#34;figure.figsize\u0026#34;: (5, 6)}) ax = sns.countplot(x = \u0026#39;disease\u0026#39;, data = data, label = \u0026#34;Count\u0026#34;, palette = \u0026#34;Set1\u0026#34;) ax.bar_label(ax.containers[0], fontsize=15) Visualise distribution of data via histograms Histograms are commonly used to visualize numerical variables. A histogram is similar to a bar graph after the values of the variable are grouped (binned) into a finite number of intervals (bins).\nHistograms group data into bins and provide you a count of the number of observations in each bin. From the shape of the bins you can quickly get a feeling for whether an attribute is Gaussian, skewed or even has an exponential distribution. It can also help you see …","date":1666987994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666995194,"objectID":"c31b22c0bfbfef971eceb204ce99a2bc","permalink":"https://zouhua.top/post/machine_learning/2022-10-28-ml003-exploratory/","publishdate":"2022-10-28T20:13:14Z","relpermalink":"/post/machine_learning/2022-10-28-ml003-exploratory/","section":"post","summary":"Exploratory Data Analysis before performing machine learning.","tags":["machine learning","microbiota"],"title":"Machine Learning on gut microbiota of patients with Colorectal cancer (3)","type":"book"},{"authors":["Hua Zou"],"categories":["Machine Learning"],"content":"Data clean Notebook 2: Identifying the problem and Getting data.\nRemoving the unmathed samples or participants.\nIdentify the problem Colorectal cancer (CRC), also known as bowel cancer, colon cancer, or rectal cancer, is the development of cancer from the colon or rectum (parts of the large intestine). Signs and symptoms may include blood in the stool, a change in bowel movements, weight loss, and fatigue. Gut microbiota play crucial role in CRC progression. Here, to investigate whether the gut microbiota could predict healthy control or CRC patients.\nExpected outcome Since this build a model that can classify healthy control or CRC patients using two training classification:\nCRC = Patients - Present healthy = Control - Absent Objective Since the labels in the data are discrete, the predication falls into two categories, (i.e. CRC or healthy). In machine learning this is a classification problem.\nThus, the goal is to classify healthy control or CRC patients. To achieve this we have used machine learning classification methods to fit a function that can predict the discrete class of new input.\nIdentify data sources The datasets contains two files:\nmetadata: The 1st and 2nd columns in the dataset store the unique ID numbers of the samples and disease (CRC=Patients, healthy=Control), respectively. profile: The gut microbial species level profile. Loading libraries import numpy as np import pandas as pd Importing Dataset First, load the TSV and CSV file using read_table or read_csv function of Pandas, respectively\nmetadata = pd.read_csv(\u0026#34;./dataset/metadata.csv\u0026#34;, index_col=0) profile = pd.read_table(\u0026#34;./dataset/species.tsv\u0026#34;, sep=\u0026#34;\\t\u0026#34;) Inspecting the data The first step is to visually inspect datasets.\nmetadata.head() profile.head() Choosing only healthy or CRC samples Selecting healthy or CRC samples to further data analysis\nfiltering disease on metadata dataset phen = metadata.loc[(metadata[\u0026#39;disease\u0026#39;] == \u0026#39;healthy\u0026#39;) | (metadata[\u0026#39;disease\u0026#39;] == \u0026#39;CRC\u0026#39;)] phen filtering the species with low occurrrence profile_trim = profile[phen.index] profile_trim.index = profile.TaxaID prof = profile_trim[profile_trim.apply(lambda x: np.count_nonzero(x)/len(x), axis=1) \u0026gt; 0.2] prof The “info()” method provides a concise summary of the data; from the output, it provides the type of data in each column, the number of non-null values in each column, and how much memory the data frame is using.\nThe method get_dtype_counts() will return the number of columns of each type in a DataFrame:\n# Review data types with \u0026#34;info()\u0026#34;. phen.info() \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; Index: 504 entries, SAMD00114718 to SAMD00165033 Data columns (total 1 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 disease 504 non-null object dtypes: object(1) memory usage: 7.9+ KB # Review number of columns of each data type in a DataFrame: phen.dtypes.value_counts() object 1 dtype: int64 #check for missing variables phen.isnull().any() disease False dtype: bool phen.disease.unique() array([\u0026#39;healthy\u0026#39;, \u0026#39;CRC\u0026#39;], dtype=object) From the results above, disease is a categorical variable, because it represents a fix number of possible values (i.e, disease. The machine learning algorithms wants numbers, and not strings, as their inputs so we need some method of coding to convert them.\nIntegrating the phen and prof data Here, we select disease from phen and then integrate it with prof into new dataset for the downstream analysis\nphen_cln = phen.iloc[:, 0].rename_axis(\u0026#34;SampleID\u0026#34;).reset_index() phen_cln.head() prof_cln = prof.T.rename_axis(\u0026#34;SampleID\u0026#34;).reset_index() prof_cln.head() mdat = pd.merge(phen_cln, prof_cln, on=\u0026#34;SampleID\u0026#34;, how=\u0026#34;inner\u0026#34;) mdat.head() #save the cleaner version of dataframe for future analyis mdat.to_csv(\u0026#39;./dataset/MergeData.tsv\u0026#39;, sep=\u0026#39;\\t\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;, index=False) Summary 151 species were selected more then 0.2 occurrence in Gastric Cancer 504 patients with Gastric Cancer were chosen Reference Breast-cancer-risk-prediction ","date":1666901594,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666908794,"objectID":"e580cb30e620d87f8a2eaf5a2c2901f1","permalink":"https://zouhua.top/post/machine_learning/2022-10-27-ml002-dataclean/","publishdate":"2022-10-27T20:13:14Z","relpermalink":"/post/machine_learning/2022-10-27-ml002-dataclean/","section":"post","summary":"Subsetting dataset for machine learning.","tags":["machine learning","microbiota"],"title":"Machine Learning on gut microbiota of patients with Colorectal cancer (2)","type":"book"},{"authors":["Hua Zou"],"categories":["Machine Learning"],"content":"Obtaining inpudata Rmarkdown 1: Downloading data.\nDownloading datasets using curatedMetagenomicData, which contains the HUMANN or Metaphlan results.\nLoading packages knitr::opts_chunk$set(warning = FALSE) library(dplyr) library(tibble) library(curatedMetagenomicData) #library(curatedMetagenomicAnalyses) # rm(list = ls()) options(stringsAsFactors = F) options(future.globals.maxSize = 1000 * 1024^2) Investigating potential response variables These are the 10 study conditions most commonly found in curatedMetagenomicData: data(\u0026#34;sampleMetadata\u0026#34;) availablediseases \u0026lt;- pull(sampleMetadata, study_condition) %\u0026gt;% table() %\u0026gt;% sort(decreasing = TRUE) availablediseases And the number of studies they are found in: studies \u0026lt;- lapply(names(availablediseases), function(x){ filter(sampleMetadata, study_condition %in% x) %\u0026gt;% pull(study_name) %\u0026gt;% unique() }) names(studies) \u0026lt;- names(availablediseases) studies \u0026lt;- studies[-grep(\u0026#34;control\u0026#34;, names(studies))] #get rid of controls studies \u0026lt;- studies[sapply(studies, length) \u0026gt; 1] #available in more than one study studies Each of these datasets has six data types associated with it; for example: curatedMetagenomicData(pattern = \u0026#34;YachidaS_2019.+\u0026#34;, dryrun = TRUE, counts = TRUE, rownames = \u0026#34;long\u0026#34;) The metagenomics datasets contain more than 13 types data, which comprising taxonomic and functional profile with relative and absolute abundance matrix.\nRelative abundance: storing into TreeSummarizedExperiment object YachidaS_2019_dataset \u0026lt;- curatedMetagenomicData(pattern = \u0026#34;YachidaS_2019.+relative_abundance\u0026#34;, dryrun = FALSE, counts = TRUE, rownames = \u0026#34;long\u0026#34;) YachidaS_2019_RB_TSE \u0026lt;- YachidaS_2019_dataset$`2021-10-14.YachidaS_2019.relative_abundance` YachidaS_2019_RB_TSE Write relative abundance datasets to disk if (0) { for (i in seq_along(studies)){ cond \u0026lt;- names(studies)[i] se \u0026lt;- curatedMetagenomicAnalyses::makeSEforCondition(cond, removestudies = \u0026#34;HMP_2019_ibdmdb\u0026#34;, dataType = \u0026#34;relative_abundance\u0026#34;) print(paste(\u0026#34;Next study condition:\u0026#34;, cond, \u0026#34; /// Body site: \u0026#34;, unique(colData(se)$body_site))) print(with(colData(se), table(study_name, study_condition))) cat(\u0026#34;\\n \\n\u0026#34;) save(se, file = paste0(cond, \u0026#34;.rda\u0026#34;)) flattext \u0026lt;- select(as.data.frame(colData(se)), c(\u0026#34;study_name\u0026#34;, \u0026#34;study_condition\u0026#34;, \u0026#34;subject_id\u0026#34;)) rownames(flattext) \u0026lt;- colData(se)$sample_id flattext \u0026lt;- cbind(flattext, data.frame(t(assay(se)))) write.csv(flattext, file = paste0(cond, \u0026#34;.csv\u0026#34;)) system(paste0(\u0026#34;gzip \u0026#34;, cond, \u0026#34;.csv\u0026#34;)) } } Preparing for machine learning metadata\nrelative abundance profile\nmetadata \u0026lt;- colData(YachidaS_2019_RB_TSE) %\u0026gt;% data.frame() phenotype \u0026lt;- metadata %\u0026gt;% dplyr::select(disease) %\u0026gt;% tibble::rownames_to_column(\u0026#34;SampleID\u0026#34;) %\u0026gt;% dplyr::filter(disease %in% c(\u0026#34;CRC\u0026#34;, \u0026#34;healthy\u0026#34;)) profile \u0026lt;- assay(YachidaS_2019_RB_TSE) sid \u0026lt;- intersect(phenotype$SampleID, colnames(profile)) prof \u0026lt;- profile %\u0026gt;% data.frame() %\u0026gt;% tibble::rownames_to_column(\u0026#34;TaxaID\u0026#34;) %\u0026gt;% dplyr::group_by(TaxaID) %\u0026gt;% dplyr::mutate(TaxaID_new = unlist(strsplit(TaxaID, \u0026#34;\\\\|\u0026#34;))[7]) %\u0026gt;% dplyr::select(TaxaID, TaxaID_new, all_of(sid)) %\u0026gt;% dplyr::ungroup() %\u0026gt;% dplyr::select(-TaxaID) %\u0026gt;% dplyr::rename(TaxaID = TaxaID_new) phen \u0026lt;- phenotype %\u0026gt;% dplyr::filter(SampleID %in% sid) output if (!dir.exists(\u0026#34;./dataset\u0026#34;)) { dir.create(\u0026#34;./dataset\u0026#34;, recursive = TRUE) } write.csv(phen, \u0026#34;./dataset/metadata.csv\u0026#34;, row.names = F) write.table(prof, \u0026#34;./dataset/species.tsv\u0026#34;, sep = \u0026#34;\\t\u0026#34;, quote = F, row.names = F) Session info devtools::session_info() Reference Create datasets for machine learning ","date":1666815194,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666822394,"objectID":"c96342c22d42734e2730080b326d735b","permalink":"https://zouhua.top/post/machine_learning/2022-10-26-ml001-inputdata/","publishdate":"2022-10-26T20:13:14Z","relpermalink":"/post/machine_learning/2022-10-26-ml001-inputdata/","section":"post","summary":"preparing input data for machine learning.","tags":["machine learning","microbiota"],"title":"Machine Learning on gut microbiota of patients with Colorectal cancer (1)","type":"book"},{"authors":["Hua Zou"],"categories":["Statistics"],"content":" Table Of Contents 前言 大纲 导入R包 转录组数据 标准化 整体水平比较 降维分析 热图 DESeq2 提取结果 差异分组 Volcano plot boxplot limma Voom功能 计算过程 差异分组 edgeR 计算过程 差异分组 Wilcox-rank-sum test or t-test 计算差异结果 差异分组 不同方法的结果比较 保存结果 差异基因数目分布 差异基因交集Venn 差异基因heatmap+volcano 共有差异基因venn 差异基因Log2Foldchange相关性分析 共有差异基因总结 总结 systemic information Reference 前言 差异分析是转录组数据分析的必需技能之一，但众多的转录组分析R包如DESeq2，limma和edgeR等让分析人员不知如何选择，还有它们之间的优劣如何？我将在本文详细探讨常用差异分析R包以及结合t-test/wilcox-rank-sum test的差异分析结果的异同点。\n大纲 本文要点由以下几点构成：\n下载以及导入测试数据（批量安装R包）；\n基因表达count矩阵的标准化方法（F(R)PKM/TPM）；\n基因整体水平分布（PCA/tSNE/UMAP；heatmap）；\nDESeq2差异分析实现以及结果解析；\nlimma差异分析实现以及结果解析；\nedgeR差异分析实现以及结果解析；\n结合t-test或wilcox-rank-sum-test方法的差异分析实现以及结果解析（是否符合正态分布选择检验方法）；\n不同方法的结果比较（volcano plot+heatmap+venn）；\n总结。\n导入R包 本次分析需要在R中批量安装包，具体安装方法可参考如何安装R包。先导入基础R包，在后面每个差异分析模块再导入所需要的差异分析R包。\nlibrary(dplyr) library(tibble) library(data.table) library(ggplot2) library(patchwork) library(cowplot) # rm(list = ls()) options(stringsAsFactors = F) options(future.globals.maxSize = 1000 * 1024^2) grp \u0026lt;- c(\u0026#34;Normal\u0026#34;, \u0026#34;Tumor\u0026#34;) grp.col \u0026lt;- c(\u0026#34;#568875\u0026#34;, \u0026#34;#73FAFC\u0026#34;) 转录组数据 本文下载的TCGA-HNSC转录组数据是通过本人先前撰写的R脚本实现的，大家可参考Downloading and preprocessing TCGA Data through R program language文章自行下载，也可以邮件询问我百度网盘密码。\n百度网盘链接：https://pan.baidu.com/s/1Daz5UsOd39T8r8K6zxPASQ\nphenotype \u0026lt;- fread(\u0026#34;TCGA-HNSC-post_mRNA_clinical.csv\u0026#34;) count \u0026lt;- fread(\u0026#34;TCGA-HNSC-post_mRNA_profile.tsv\u0026#34;) table(phenotype$Group) 标准化 标准化的目的是为了降低测序深度以及基因长度对基因表达谱下游分析的影响。测序深度越高则map到基因的reads也越多，同理基因长度越长则map到的reads也越多，最后对应的counts数目也越多。\nRPM/CPM: Reads/Counts of exon model per Million mapped reads (每百万映射读取的reads) $$RPM = \\frac{ExonMappedReads * 10^{6}}{TotalMappedReads}$$\nRPKM: Reads Per Kilobase of exon model per Million mapped reads (每千个碱基的转录每百万映射读取的reads) $$RPKM = \\frac{ExonMappedReads * 10^{9}}{TotalMappedReads * ExonLength}$$\nFPKM: Fragments Per Kilobase of exon model per Million mapped fragments(每千个碱基的转录每百万映射读取的fragments)， 适用于PE测序。 $$ FPKM = \\frac{ExonMappedFragments * 10^{9}}{TotalMappedFragments * ExonLength}$$\nTPM：Transcripts Per Kilobase of exon model per Million mapped reads (每千个碱基的转录每百万映射读取的Transcripts) $$TPM= \\frac{N_i/L_i * 10^{6}}{sum(N_1/L_1+N_2/L_2+…+N_j/L_j+…+N_n/L_n)}$$ $N_i$为比对到第i个exon的reads数； $L_i$为第i个exon的长度；sum()为所有 (n个)exon按长度进行标准化之后数值的和\n获取gene length表：对Homo_sapiens.GRCh38.101版本数据处理获取gene length数据；human_gene_all.tsv是使用biomart包获取gene symbol和ensembleID的对应关系表。 geneLength \u0026lt;- fread(\u0026#34;Homo_sapiens.GRCh38.101.genelength.tsv\u0026#34;) geneIdAll \u0026lt;- fread(\u0026#34;human_gene_all.tsv\u0026#34;) geneIdLength \u0026lt;- geneIdAll %\u0026gt;% filter(transcript_biotype == \u0026#34;protein_coding\u0026#34;) %\u0026gt;% dplyr::select(ensembl_gene_id, external_gene_name) %\u0026gt;% inner_join(geneLength, by = c(\u0026#34;ensembl_gene_id\u0026#34;=\u0026#34;V1\u0026#34;)) %\u0026gt;% dplyr::select(-ensembl_gene_id) %\u0026gt;% dplyr::distinct() %\u0026gt;% dplyr::rename(Length=V2) geneIdLengthUniq \u0026lt;- geneIdLength[pmatch(count$Feature, geneIdLength$external_gene_name), ] %\u0026gt;% filter(!is.na(Length)) %\u0026gt;% arrange(external_gene_name) count_cln \u0026lt;- count %\u0026gt;% filter(Feature%in%geneIdLengthUniq$external_gene_name) %\u0026gt;% arrange(Feature) %\u0026gt;% column_to_rownames(\u0026#34;Feature\u0026#34;) if(!any(geneIdLengthUniq$external_gene_name == rownames(count_cln))){ message(\u0026#34;Order of GeneName is wrong\u0026#34;) } gene_lengths \u0026lt;-geneIdLengthUniq$Length head(geneIdLengthUniq) RPKM/FPKM/TPM 转换 countToFpkm \u0026lt;- function(counts, lengths){ pm \u0026lt;- sum(counts) /1e6 rpm \u0026lt;- counts/pm rpm/(lengths/1000) } countToTpm \u0026lt;- function(counts, lengths) { rpk \u0026lt;- counts/(lengths/1000) coef \u0026lt;- sum(rpk) / 1e6 rpk/coef } # FPKM count_FPKM \u0026lt;- apply(count_cln, 2, function(x){countToFpkm(x, gene_lengths)}) %\u0026gt;% data.frame() # TPM count_TPM \u0026lt;- apply(count_cln, 2, function(x){countToTpm(x, gene_lengths)}) %\u0026gt;% data.frame() head(count_cln) head(count_FPKM) head(count_TPM) 整体水平比较 在做差异分析前，一般可以对数据做一个降维处理，然后看不同分组是否能在二维展开平面区分开。\nExpressionSet 先将数据存成ExpressionSet格式，可参考RNA-seq数据的批次校正方法文章。ExpressionSet对象数据包含表达谱和metadata等数据，这方便后期分析。\ngetExprSet \u0026lt;- function(metadata=phenotype, profile=count_cln, occurrence=0.2){ # metadata=phenotype # profile=count_cln # occurrence=0.2 sid \u0026lt;- intersect(metadata$SampleID, colnames(profile)) # phenotype phe \u0026lt;- metadata %\u0026gt;% filter(SampleID%in%sid) %\u0026gt;% column_to_rownames(\u0026#34;SampleID\u0026#34;) # profile by occurrence prf \u0026lt;- profile %\u0026gt;% rownames_to_column(\u0026#34;tmp\u0026#34;) %\u0026gt;% filter(apply(dplyr::select(., -one_of(\u0026#34;tmp\u0026#34;)), 1, function(x) { sum(x != 0)/length(x)}) \u0026gt; occurrence) %\u0026gt;% dplyr::select(c(tmp, rownames(phe))) %\u0026gt;% column_to_rownames(\u0026#34;tmp\u0026#34;) # determine the right order between profile and phenotype for(i in 1:ncol(prf)){ if (!(colnames(prf)[i] == rownames(phe)[i])) { stop(paste0(i, \u0026#34; Wrong\u0026#34;)) } } require(convert) exprs \u0026lt;- as.matrix(prf) adf \u0026lt;- new(\u0026#34;AnnotatedDataFrame\u0026#34;, data=phe) experimentData \u0026lt;- new(\u0026#34;MIAME\u0026#34;, name=\u0026#34;Hua Zou\u0026#34;, lab=\u0026#34;UCAS\u0026#34;, contact=\u0026#34;zouhua@outlook.com\u0026#34;, title=\u0026#34;TCGA-HNSC\u0026#34;, abstract=\u0026#34;The gene ExpressionSet\u0026#34;, url=\u0026#34;www.zouhua.top\u0026#34;, other=list(notes=\u0026#34;Created from text files\u0026#34;)) expressionSet \u0026lt;- new(\u0026#34;ExpressionSet\u0026#34;, exprs=exprs, phenoData=adf, experimentData=experimentData) return(expressionSet) } ExprSet_count \u0026lt;- getExprSet(profile=count_cln) ExprSet_FPKM \u0026lt;- getExprSet(profile=count_FPKM) ExprSet_TPM \u0026lt;- …","date":1666296794,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666303994,"objectID":"502dca6c954348598c688fab47f1c873","permalink":"https://zouhua.top/post/math_statistics/2022-10-27-da-methods-comparsion/","publishdate":"2022-10-20T20:13:14Z","relpermalink":"/post/math_statistics/2022-10-27-da-methods-comparsion/","section":"post","summary":"差异分析是转录组数据分析的必备技能之一，但众多的转录组分析R包*DESeq2*，*limma*和*edgeR*让分析人员有选择问题，那么它们之间的优劣到底是如何的呢?","tags":["Differential Analysis","RNA-seq"],"title":"转录组差异分析（DESeq2+limma+edgeR+t-test/wilcox-test）总结","type":"post"},{"authors":["Hua Zou"],"categories":null,"content":"","date":1666137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666137600,"objectID":"1c9c128392513bb9f49aea1358092956","permalink":"https://zouhua.top/bookdown/2022-16s/","publishdate":"2022-11-19T00:00:00Z","relpermalink":"/bookdown/2022-16s/","section":"bookdown","summary":"Here, we give users one example to practice the 16s microbiota data analysis workflow by XMAS 2.0. By the way, we also recommend users handling your own microbiota data in a reasonable manner when you utilize this package or workflow. Pay attention to whether your data fit the methods this package provided.","tags":null,"title":"Data analysis workflow on 16s sequencing data using XMAS2","type":"bookdown"},{"authors":["Hua Zou"],"categories":["Tool"],"content":" Table Of Contents Introduction Prerequisites Registering github account Creating repository for blog Setting your ssh key Install Blogdown and Hugo Creating blogdown project Building website Basic Customization Uploading files to github repository Creating Github Actions Deployment Reference Introduction Have you ever thought about making your own website using R and Rstudio? This tutorial would teach you how to create your personal blog via blogdown and github. What’s more, automatically eploying your website is also necessary by using github actions.\nPrerequisites Before running the tutorial, you need have this software in your PC\nR\nRstudio\nRegistering github account Please go to github to obtain your own account. Then create a new repository named [your_github_names].github.io which is applied to render your github pages.\nCreating repository for blog Creating another private repository (named MyOwnWebsite) to save your files which are used to setup blog. Here, suggesting users create two repositories to deploy personal website according to the protection of personal privacy.\n[your_github_names].github.io for saving blog html files MyOwnWebsite for saving blog source files Setting your ssh key Once you created github repository for your blog, and then you need download git bash software (as for mac os, using terminal) to generate your ssh key in your personal computer. Firstly, configuring your git with name and email. Secondly, generating the ssh key by rsa algorithm. Finally, uploading your public ssh key to github Personal photo -\u0026gt; Settings -\u0026gt; SSH and GPG keys\n# configure git config --global user.name \u0026#34;Hua Zou\u0026#34; git config --global user.email \u0026#34;zouhua1@outlook.com\u0026#34; # generate ssh key ssh-keygen -t rsa -C \u0026#34;zouhua1@outlook.com\u0026#34; Checking whether the setting is okay\nssh -T git@github.com # Hi HuaZou! You\u0026#39;ve successfully authenticated, but GitHub does not provide shell access. Install Blogdown and Hugo Blogdown which Creating Blogs and Websites with R Markdown is necessary for building website\n# Install blogdown #install.packages(\u0026#34;blogdown\u0026#34;) remotes::install_github(\u0026#39;rstudio/blogdown\u0026#39;) # Install Hugo blogdown::install_hugo() Creating blogdown project Firstly, open Rstudio to create New Project -\u0026gt; New Directory -\u0026gt; Website using blogdown and then give MyOwnWebsite to Directory name, and then setting Hugo theme by gcushen/hugo-academic. Finally, Kick Create Project to generate website.\nThen, opening Rstudio to create blogdown project (File -\u0026gt; New Project -\u0026gt; Website using blogdown -\u0026gt; New Prject Wizard -\u0026gt; Create Project). YOu could see the following files and directories.\nthe final website folder like this\nBuilding website To build the website by using hugo_build from blogdown R package.\nblogdown::hugo_build(local=TRUE) All the actual files of the website are stored in public/ folder.\nTo preview the website using\nblogdown::serve_site() config.yaml: Hugo and theme configuration file. .Rprofile: File to set up your blogdown options. netlify.toml: File to set up your Netlify options. content/: Website source files to edit and add, such as blog posts. themes/: Hugo theme. Basic Customization The basic files that you want to modify to customize your website are the following:\nconfig/_default/config.yaml: general website information config/_default/params.yaml: website customization config/_default/menus.yaml: top bar / menu customization content/authors/admin/_index.md: personal information Uploading files to github repository Moving the files of public/ in MyOwnWebsite directory into [your_github_names].github.io directory and then using git to push all the files to the remote repository in both [your_github_names].github.io and MyOwnWebsite.\ngit add -A git commit -m \u0026#34;update blog\u0026#34; git push origin master # main Creating Github Actions Opening MyOwnWebsite Repository (Actions -\u0026gt; New workflow) to create deploy.yml.\nname: deploy_blog on: push: branches: [main, master] pull_request: branches: [main, master] release: types: [published] workflow_dispatch: jobs: build: runs-on: ubuntu-latest steps: - name: checkout uses: actions/checkout@v3 with: submodules: true - name: Setup Hugo uses: peaceiris/actions-hugo@v2.5.0 with: hugo-version: \u0026#39;0.105.0\u0026#39; extended: true - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3.8.0 with: deploy_key: ${{ secrets.ACTIONS_DEPLOY_KEY }} external_repository: HuaZou/HuaZou.github.io publish_branch: master publish_dir: ./public allow_empty_commit: true Deployment We need to connect the Github Pages repository ([your_github_names].github.io) and blog repository (MyOwnWebsite) through ssh key. Go to ~/.ssh/.\nssh-keygen -t rsa -C \u0026#34;zouhua1@outlook.com\u0026#34; -f \u0026#34;MyOwnWebsite\u0026#34; private key is for MyOwnWebsite (settings -\u0026gt; Secrets) and add it as ACTIONS_DEPLOY_KEY (Must use this name because the deploy_key uses secrets.ACTIONS_DEPLOY_KEY and is from ssh-keygen).\npublic key is for [your_github_names].github.io (Deploy keys)\nWhen you upload your files to MyOwnWebsite and github …","date":1661199194,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661206394,"objectID":"4b8eda7a56d5803a903c60e6acb57df4","permalink":"https://zouhua.top/post/tool/2022-08-22-blogdown-hugo/","publishdate":"2022-08-22T20:13:14Z","relpermalink":"/post/tool/2022-08-22-blogdown-hugo/","section":"post","summary":"Building Your Own Website by blogdown and hugo","tags":["blogdown","github","hugo"],"title":"How to setup personal blog by blogdown and githup pages","type":"post"},{"authors":["Hua Zou"],"categories":null,"content":"","date":1660867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660867200,"objectID":"87c79596733bec228d6c7e28407533a7","permalink":"https://zouhua.top/bookdown/2022-xmas2/","publishdate":"2022-08-19T00:00:00Z","relpermalink":"/bookdown/2022-xmas2/","section":"bookdown","summary":"In this tutorial, we focus on differential analysis with the step-by-step procedures by using the R programming language. We also briefly introduce the concepts and principals of the statistical methods before the applications and give conclusions on the results.","tags":null,"title":"Statistical toolkits on microbiota data","type":"bookdown"},{"authors":["Hua Zou","Dan Wang","Huahui Ren"],"categories":null,"content":"","date":1582502400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582502400,"objectID":"20d79b52d65fe064fabe61be5389ae1c","permalink":"https://zouhua.top/publication/2020-nutrients/","publishdate":"2020-02-24T00:00:00Z","relpermalink":"/publication/2020-nutrients/","section":"publication","summary":"This non-controlled intervention study revealed associations between baseline gut microbiota and CR-induced BMI loss and provided evidence to accelerate the application of microbiome stratification in future personalized nutrition intervention.","tags":null,"title":"Effect of Caloric Restriction on BMI, Gut Microbiota, and Blood Amino Acid Levels in Non-Obese Adults","type":"publication"}]