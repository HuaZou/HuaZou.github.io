
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Hua Zou is a senior bioinformatics analyst at Xbiome Company since Mar 7 2022. My research interests include host-microbiota intersection, machine learning and multi-omics data integration. I lead the development of XMAS 2.0.\nDownload my resumé .\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Hua Zou is a senior bioinformatics analyst at Xbiome Company since Mar 7 2022. My research interests include host-microbiota intersection, machine learning and multi-omics data integration. I lead the development of XMAS 2.","tags":null,"title":"Hua Zou","type":"authors"},{"authors":null,"categories":["Machine Learning"],"content":"Obtaining inpudata Downloading datasets using curatedMetagenomicData, which contains the HUMANN or Metaphlan results.\nLoading packages knitr::opts_chunk$set(warning = FALSE) library(dplyr) library(tibble) library(curatedMetagenomicData) #library(curatedMetagenomicAnalyses) # rm(list = ls()) options(stringsAsFactors = F) options(future.globals.maxSize = 1000 * 1024^2) Investigate potential response variables These are the 10 study conditions most commonly found in curatedMetagenomicData: data(\u0026#34;sampleMetadata\u0026#34;) availablediseases \u0026lt;- pull(sampleMetadata, study_condition) %\u0026gt;% table() %\u0026gt;% sort(decreasing = TRUE) availablediseases And the number of studies they are found in: studies \u0026lt;- lapply(names(availablediseases), function(x){ filter(sampleMetadata, study_condition %in% x) %\u0026gt;% pull(study_name) %\u0026gt;% unique() }) names(studies) \u0026lt;- names(availablediseases) studies \u0026lt;- studies[-grep(\u0026#34;control\u0026#34;, names(studies))] #get rid of controls studies \u0026lt;- studies[sapply(studies, length) \u0026gt; 1] #available in more than one study studies Each of these datasets has six data types associated with it; for example: curatedMetagenomicData(pattern = \u0026#34;YachidaS_2019.+\u0026#34;, dryrun = TRUE, counts = TRUE, rownames = \u0026#34;long\u0026#34;) The metagenomics datasets contain more than 13 types data, which comprising taxonomic and functional profile with relative and absolute abundance matrix.\nRelative abundance: storing into TreeSummarizedExperiment object YachidaS_2019_dataset \u0026lt;- curatedMetagenomicData(pattern = \u0026#34;YachidaS_2019.+relative_abundance\u0026#34;, dryrun = FALSE, counts = TRUE, rownames = \u0026#34;long\u0026#34;) YachidaS_2019_RB_TSE \u0026lt;- YachidaS_2019_dataset$`2021-10-14.YachidaS_2019.relative_abundance` YachidaS_2019_RB_TSE Write relative abundance datasets to disk if (0) { for (i in seq_along(studies)){ cond \u0026lt;- names(studies)[i] se \u0026lt;- curatedMetagenomicAnalyses::makeSEforCondition(cond, removestudies = \u0026#34;HMP_2019_ibdmdb\u0026#34;, dataType = \u0026#34;relative_abundance\u0026#34;) print(paste(\u0026#34;Next study condition:\u0026#34;, cond, \u0026#34; /// Body site: \u0026#34;, unique(colData(se)$body_site))) print(with(colData(se), table(study_name, study_condition))) cat(\u0026#34;\\n \\n\u0026#34;) save(se, file = paste0(cond, \u0026#34;.rda\u0026#34;)) flattext \u0026lt;- select(as.data.frame(colData(se)), c(\u0026#34;study_name\u0026#34;, \u0026#34;study_condition\u0026#34;, \u0026#34;subject_id\u0026#34;)) rownames(flattext) \u0026lt;- colData(se)$sample_id flattext \u0026lt;- cbind(flattext, data.frame(t(assay(se)))) write.csv(flattext, file = paste0(cond, \u0026#34;.csv\u0026#34;)) system(paste0(\u0026#34;gzip \u0026#34;, cond, \u0026#34;.csv\u0026#34;)) } } Preparing for machine learning metadata\nrelative abundance profile\nmetadata \u0026lt;- colData(YachidaS_2019_RB_TSE) %\u0026gt;% data.frame() phenotype \u0026lt;- metadata %\u0026gt;% dplyr::select(disease) %\u0026gt;% tibble::rownames_to_column(\u0026#34;SampleID\u0026#34;) %\u0026gt;% dplyr::filter(disease %in% c(\u0026#34;CRC\u0026#34;, \u0026#34;healthy\u0026#34;)) profile \u0026lt;- assay(YachidaS_2019_RB_TSE) sid \u0026lt;- intersect(phenotype$SampleID, colnames(profile)) prof \u0026lt;- profile %\u0026gt;% data.frame() %\u0026gt;% tibble::rownames_to_column(\u0026#34;TaxaID\u0026#34;) %\u0026gt;% dplyr::group_by(TaxaID) %\u0026gt;% dplyr::mutate(TaxaID_new = unlist(strsplit(TaxaID, \u0026#34;\\\\|\u0026#34;))[7]) %\u0026gt;% dplyr::select(TaxaID, TaxaID_new, all_of(sid)) %\u0026gt;% dplyr::ungroup() %\u0026gt;% dplyr::select(-TaxaID) %\u0026gt;% dplyr::rename(TaxaID = TaxaID_new) phen \u0026lt;- phenotype %\u0026gt;% dplyr::filter(SampleID %in% sid) output if (!dir.exists(\u0026#34;./dataset\u0026#34;)) { dir.create(\u0026#34;./dataset\u0026#34;, recursive = TRUE) } write.csv(phen, \u0026#34;./dataset/metadata.csv\u0026#34;, row.names = F) write.table(prof, \u0026#34;./dataset/species.tsv\u0026#34;, sep = \u0026#34;\\t\u0026#34;, quote = F, row.names = F) Session info devtools::session_info() Reference Create datasets for machine learning ","date":1667351594,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667351594,"objectID":"1e66f2eb9080e59f9d384dda688d04e8","permalink":"https://zouhua.top/post/machine_learning/2022-11-01-ml001-inputdata/","publishdate":"2022-11-01T20:13:14-05:00","relpermalink":"/post/machine_learning/2022-11-01-ml001-inputdata/","section":"post","summary":"preparing input data for machine learning","tags":["machine learning","SVN","microbiota"],"title":"Machine Learning on gut microbiota of patients with Colorectal cancer (1): Obtaining inpudata","type":"post"},{"authors":null,"categories":["Machine Learning"],"content":"Data clean Removing the unmathed samples or participants.\nIdentify the problem Colorectal cancer (CRC), also known as bowel cancer, colon cancer, or rectal cancer, is the development of cancer from the colon or rectum (parts of the large intestine). Signs and symptoms may include blood in the stool, a change in bowel movements, weight loss, and fatigue. Gut microbiota play crucial role in CRC progression. Here, to investigate whether the gut microbiota could predict healthy control or CRC patients.\nExpected outcome Since this build a model that can classify healthy control or CRC patients using two training classification:\nCRC = Patients - Present healthy = Control - Absent Objective Since the labels in the data are discrete, the predication falls into two categories, (i.e. CRC or healthy). In machine learning this is a classification problem.\nThus, the goal is to classify healthy control or CRC patients. To achieve this we have used machine learning classification methods to fit a function that can predict the discrete class of new input.\nIdentify data sources The datasets contains two files:\nmetadata: The 1st and 2nd columns in the dataset store the unique ID numbers of the samples and disease (CRC=Patients, healthy=Control), respectively. profile: The gut microbial species level profile. Loading libraries import numpy as np import pandas as pd Importing Dataset First, load the TSV and CSV file using read_table or read_csv function of Pandas, respectively\nmetadata = pd.read_csv(\u0026#34;./dataset/metadata.csv\u0026#34;, index_col=0) profile = pd.read_table(\u0026#34;./dataset/species.tsv\u0026#34;, sep=\u0026#34;\\t\u0026#34;) Inspecting the data The first step is to visually inspect datasets.\nmetadata.head() disease SampleID SAMD00114718 healthy SAMD00114719 healthy SAMD00114720 healthy SAMD00114721 healthy SAMD00114722 CRC profile.head() TaxaID SAMD00114718 SAMD00114719 SAMD00114720 SAMD00114721 SAMD00114722 SAMD00114723 SAMD00114724 SAMD00114726 SAMD00114727 ... SAMD00165024 SAMD00165025 SAMD00165026 SAMD00165027 SAMD00165028 SAMD00165029 SAMD00165030 SAMD00165031 SAMD00165032 SAMD00165033 0 s__Bacteroides_plebeius 46509517 5334509 6868169 1029678 7520 105 3515 3047974 90986 ... 0 0 3902772 0 0 0 6311760 0 2286204 0 1 s__Bacteroides_dorei 8249892 230275 4054008 2029259 2318235 0 10920493 1777 4043706 ... 0 138343 3005330 0 0 5136959 18113 0 242316 0 2 s__Faecalibacterium_prausnitzii 3696318 2053756 3267707 661965 350665 393585 536323 648125 1246731 ... 0 2868791 1755420 5699714 0 1948287 0 152752 2584817 0 3 s__Eubacterium_eligens 3265545 182914 0 114447 546829 0 0 10419 895911 ... 0 1370340 0 0 0 400810 578608 0 2600731 0 4 s__Bacteroides_ovatus 2871853 289955 1097263 110111 564558 580 51366 26697 0 ... 1491036 443860 56055 2543082 0 279863 19714 361361 0 0 5 rows × 505 columns\nChoosing only healthy or CRC samples Selecting healthy or CRC samples to further data analysis\nfiltering disease on metadata dataset phen = metadata.loc[(metadata[\u0026#39;disease\u0026#39;] == \u0026#39;healthy\u0026#39;) | (metadata[\u0026#39;disease\u0026#39;] == \u0026#39;CRC\u0026#39;)] phen disease SampleID SAMD00114718 healthy SAMD00114719 healthy SAMD00114720 healthy SAMD00114721 healthy SAMD00114722 CRC ... ... SAMD00165029 healthy SAMD00165030 healthy SAMD00165031 healthy SAMD00165032 healthy SAMD00165033 healthy 504 rows × 1 columns\nfiltering the species with low occurrrence profile_trim = profile[phen.index] profile_trim.index = profile.TaxaID prof = profile_trim[profile_trim.apply(lambda x: np.count_nonzero(x)/len(x), axis=1) \u0026gt; 0.2] prof SAMD00114718 SAMD00114719 SAMD00114720 SAMD00114721 SAMD00114722 SAMD00114723 SAMD00114724 SAMD00114726 SAMD00114727 SAMD00114728 ... SAMD00165024 SAMD00165025 SAMD00165026 SAMD00165027 SAMD00165028 SAMD00165029 SAMD00165030 SAMD00165031 SAMD00165032 SAMD00165033 TaxaID s__Bacteroides_plebeius 46509517 5334509 6868169 1029678 7520 105 3515 3047974 90986 0 ... 0 0 3902772 0 0 0 6311760 0 2286204 0 s__Bacteroides_dorei 8249892 230275 4054008 2029259 2318235 0 10920493 1777 4043706 1072863 ... 0 138343 3005330 0 0 5136959 18113 0 242316 0 s__Faecalibacterium_prausnitzii 3696318 2053756 3267707 661965 350665 393585 536323 648125 1246731 2339359 ... 0 2868791 1755420 5699714 0 1948287 0 152752 2584817 0 s__Eubacterium_eligens 3265545 182914 0 114447 546829 0 0 10419 895911 0 ... 0 1370340 0 0 0 400810 578608 0 2600731 0 s__Bacteroides_ovatus 2871853 289955 1097263 110111 564558 580 51366 26697 0 249687 ... 1491036 443860 56055 2543082 0 279863 19714 361361 0 0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... s__Klebsiella_pneumoniae 0 0 0 0 0 0 0 1544397 0 0 ... 0 0 19537 0 0 0 0 0 0 0 s__Bacteroides_coprocola 0 0 0 0 0 0 0 1029096 0 0 ... 0 0 0 0 0 5218937 0 0 0 0 s__Ruminococcus_lactaris 0 0 0 0 0 0 0 0 0 2800532 ... 113761 0 0 0 0 761927 0 0 630160 0 s__Turicimonas_muris 0 0 0 0 0 0 0 0 0 1530 ... 0 0 357 9347 0 1368 0 0 653 0 s__Proteobacteria_bacterium_CAG_139 0 0 0 0 0 0 0 0 0 1383 ... 0 0 16130 15854 0 15683 0 0 191409 0 151 rows × 504 columns\nThe “info()” method …","date":1667351594,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667351594,"objectID":"b5db4596e0b46208a861cfc7e3ce9c5c","permalink":"https://zouhua.top/post/machine_learning/2022-11-01-ml002-dataclean/","publishdate":"2022-11-01T20:13:14-05:00","relpermalink":"/post/machine_learning/2022-11-01-ml002-dataclean/","section":"post","summary":"Subsetting dataset for machine learning","tags":["machine learning","SVN","microbiota"],"title":"Machine Learning on gut microbiota of patients with Colorectal cancer (2): Data clean","type":"post"},{"authors":null,"categories":["Machine Learning"],"content":"Exploratory Data Analysis Now that we have a good intuitive sense of the data, Next step involves taking a closer look at attributes and data values. In this section, I am getting familiar with the data, which will provide useful knowledge for data pre-processing.\nObjectives of Data Exploration Exploratory data analysis (EDA) is a very important step which takes place after feature engineering and acquiring data and it should be done before any modeling. This is because it is very important for a data scientist to be able to understand the nature of the data without making assumptions. The results of data exploration can be extremely useful in grasping the structure of the data, the distribution of the values, and the presence of extreme values and interrelationships within the data set.\nThe purpose of EDA is:\nTo use summary statistics and visualizations to better understand data, Finding clues about the tendencies of the data, its quality and to formulate assumptions and the hypothesis of our analysis For data preprocessing to be successful, it is essential to have an overall picture of your data Basic statistical descriptions can be used to identify properties of the data and highlight which data values should be treated as noise or outliers. Next step is to explore the data. There are two approached used to examine the data using:\nDescriptive statistics is the process of condensing key characteristics of the data set into simple numeric metrics. Some of the common metrics used are mean, standard deviation, and correlation.\nVisualization is the process of projecting the data, or parts of it, into Cartesian space or into abstract images. In the data mining process, data exploration is leveraged in many different steps including preprocessing, modeling, and interpretation of results.\nSummary statistics are measurements meant to describe data. In the field of descriptive statistics, there are many summary measurements\nLoading libraries %matplotlib inline import matplotlib.pyplot as plt import pandas as pd import numpy as np from scipy.stats import norm import seaborn as sns import statistics as st Descriptive statistics data = pd.read_table(\u0026#39;./dataset/MergeData.tsv\u0026#39;, sep=\u0026#34;\\t\u0026#34;, index_col=0) data.head() disease s__Bacteroides_plebeius s__Bacteroides_dorei s__Faecalibacterium_prausnitzii s__Eubacterium_eligens s__Bacteroides_ovatus s__Parabacteroides_distasonis s__Ruminococcus_gnavus s__Phascolarctobacterium_faecium s__Bacteroides_uniformis ... s__Bacteroides_finegoldii s__Haemophilus_sp_HMSC71H05 s__Clostridium_saccharolyticum s__Streptococcus_anginosus_group s__Streptococcus_sp_A12 s__Klebsiella_pneumoniae s__Bacteroides_coprocola s__Ruminococcus_lactaris s__Turicimonas_muris s__Proteobacteria_bacterium_CAG_139 SampleID SAMD00114718 healthy 46509517 8249892 3696318 3265545 2871853 2327330 1920299 1506928 1371476 ... 0 0 0 0 0 0 0 0 0 0 SAMD00114719 healthy 5334509 230275 2053756 182914 289955 89183 35688 0 729206 ... 0 0 0 0 0 0 0 0 0 0 SAMD00114720 healthy 6868169 4054008 3267707 0 1097263 990122 1490407 0 1272701 ... 0 0 0 0 0 0 0 0 0 0 SAMD00114721 healthy 1029678 2029259 661965 114447 110111 2705778 59274 0 940124 ... 0 0 0 0 0 0 0 0 0 0 SAMD00114722 CRC 7520 2318235 350665 546829 564558 2529966 4608830 0 1888066 ... 512018 137432 71548 15826 0 0 0 0 0 0 5 rows × 152 columns\nbasic descriptive statistics data.describe() s__Bacteroides_plebeius s__Bacteroides_dorei s__Faecalibacterium_prausnitzii s__Eubacterium_eligens s__Bacteroides_ovatus s__Parabacteroides_distasonis s__Ruminococcus_gnavus s__Phascolarctobacterium_faecium s__Bacteroides_uniformis s__Bifidobacterium_longum ... s__Bacteroides_finegoldii s__Haemophilus_sp_HMSC71H05 s__Clostridium_saccharolyticum s__Streptococcus_anginosus_group s__Streptococcus_sp_A12 s__Klebsiella_pneumoniae s__Bacteroides_coprocola s__Ruminococcus_lactaris s__Turicimonas_muris s__Proteobacteria_bacterium_CAG_139 count 5.040000e+02 5.040000e+02 5.040000e+02 5.040000e+02 5.040000e+02 5.040000e+02 5.040000e+02 5.040000e+02 5.040000e+02 5.040000e+02 ... 5.040000e+02 5.040000e+02 504.000000 504.000000 504.000000 5.040000e+02 5.040000e+02 5.040000e+02 504.000000 5.040000e+02 mean 2.139221e+06 1.853404e+06 1.639697e+06 7.471533e+05 5.702796e+05 1.189482e+06 8.616634e+05 8.704456e+04 2.655816e+06 4.948624e+05 ... 1.896052e+05 1.434993e+04 16513.160714 9621.785714 1705.916667 2.354459e+05 4.195657e+05 1.364957e+05 3334.486111 4.703426e+04 std 4.689115e+06 3.560063e+06 1.698016e+06 1.656732e+06 1.403095e+06 2.045147e+06 2.860133e+06 2.014462e+05 3.023979e+06 1.583632e+06 ... 6.347103e+05 1.339195e+05 63994.560492 37684.680559 9572.428444 1.500337e+06 1.146908e+06 4.313418e+05 18921.337212 2.234608e+05 min 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 ... 0.000000e+00 0.000000e+00 0.000000 0.000000 0.000000 0.000000e+00 0.000000e+00 0.000000e+00 0.000000 0.000000e+00 25% 0.000000e+00 0.000000e+00 …","date":1667351594,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667351594,"objectID":"1c44d4b15c9d38a72d4d73cbb5ae3d60","permalink":"https://zouhua.top/post/machine_learning/2022-11-01-ml003-exploratory/","publishdate":"2022-11-01T20:13:14-05:00","relpermalink":"/post/machine_learning/2022-11-01-ml003-exploratory/","section":"post","summary":"Exploratory Data Analysis before performing machine learning","tags":["machine learning","SVN","microbiota"],"title":"Machine Learning on gut microbiota of patients with Colorectal cancer (3): Exploratory Data Analysis","type":"post"},{"authors":null,"categories":["Machine Learning"],"content":"Data PreProcessing Data preprocessing is a crucial step for any data analysis problem. It is often a very good idea to prepare your data in such way to best expose the structure of the problem to the machine learning algorithms that you intend to use. This involves a number of activities such as:\nAssigning numerical values to categorical data; Handling missing values; Normalizing the features (so that features on small scales do not dominate when fitting a model to the data). In Exploratory Data Analysis, I explored the data, to help gain insight on the distribution of the data as well as how the attributes correlate to each other. I identified some features of interest. In this notebook I use feature selection to reduce high-dimension data, feature extraction and transformation for dimensional reduction.\nGoal Find the most predictive features of the data and filter it so it will enhance the predictive power of the analytics model.\nLoading data and essential libraries %matplotlib inline import matplotlib.pyplot as plt import pandas as pd import numpy as np from scipy.stats import norm # visualization import seaborn as sns plt.style.use(\u0026#39;fivethirtyeight\u0026#39;) sns.set_style(\u0026#34;white\u0026#34;) plt.rcParams[\u0026#39;figure.figsize\u0026#39;] = (8,4) #plt.rcParams[\u0026#39;axes.titlesize\u0026#39;] = \u0026#39;large\u0026#39; Importing data data_df = pd.read_table(\u0026#39;./dataset/MergeData.tsv\u0026#39;, sep=\u0026#34;\\t\u0026#34;, index_col=0) data = data_df.reset_index(drop=True) data.head() disease s__Bacteroides_plebeius s__Bacteroides_dorei s__Faecalibacterium_prausnitzii s__Eubacterium_eligens s__Bacteroides_ovatus s__Parabacteroides_distasonis s__Ruminococcus_gnavus s__Phascolarctobacterium_faecium s__Bacteroides_uniformis ... s__Bacteroides_finegoldii s__Haemophilus_sp_HMSC71H05 s__Clostridium_saccharolyticum s__Streptococcus_anginosus_group s__Streptococcus_sp_A12 s__Klebsiella_pneumoniae s__Bacteroides_coprocola s__Ruminococcus_lactaris s__Turicimonas_muris s__Proteobacteria_bacterium_CAG_139 0 healthy 46509517 8249892 3696318 3265545 2871853 2327330 1920299 1506928 1371476 ... 0 0 0 0 0 0 0 0 0 0 1 healthy 5334509 230275 2053756 182914 289955 89183 35688 0 729206 ... 0 0 0 0 0 0 0 0 0 0 2 healthy 6868169 4054008 3267707 0 1097263 990122 1490407 0 1272701 ... 0 0 0 0 0 0 0 0 0 0 3 healthy 1029678 2029259 661965 114447 110111 2705778 59274 0 940124 ... 0 0 0 0 0 0 0 0 0 0 4 CRC 7520 2318235 350665 546829 564558 2529966 4608830 0 1888066 ... 512018 137432 71548 15826 0 0 0 0 0 0 5 rows × 152 columns\nLabel encoding Here, I assign the features to a NumPy array X, and transform the class labels from their original string representation (CRC and healthy) into integers\narray = data.values X_temp = array[:, 1:data.shape[1]] y = array[:, 0] array array([[\u0026#39;healthy\u0026#39;, 46509517, 8249892, ..., 0, 0, 0], [\u0026#39;healthy\u0026#39;, 5334509, 230275, ..., 0, 0, 0], [\u0026#39;healthy\u0026#39;, 6868169, 4054008, ..., 0, 0, 0], ..., [\u0026#39;healthy\u0026#39;, 0, 0, ..., 0, 0, 0], [\u0026#39;healthy\u0026#39;, 2286204, 242316, ..., 630160, 653, 191409], [\u0026#39;healthy\u0026#39;, 0, 0, ..., 0, 0, 0]], dtype=object) centered log-ratio (clr) transformation Transforms compositions from Aitchison geometry to the real space (skbio.stats.composition.clr).\nfrom skbio.stats.composition import clr pseude_value = np.amin(np.array(X_temp)[X_temp != np.amin(X_temp)]) data_temp = data.iloc[:, 1:data.shape[1]].T data_clr = data_temp.apply(lambda x: clr(x + pseude_value), axis=0).T X = data_clr.values[:, 0:data_clr.shape[1]] X array([[10.26214602, 8.53269436, 7.7298392 , ..., -3.44177889, -3.44177889, -3.44177889], [ 7.60933349, 4.46687166, 6.65482253, ..., -3.92913994, -3.92913994, -3.92913994], [ 8.41126745, 7.88408118, 7.66846676, ..., -3.37990451, -3.37990451, -3.37990451], ..., [-1.88844976, -1.88844976, 6.09721786, ..., -1.88844976, -1.88844976, -1.88844976], [ 7.74550107, 5.50128747, 7.86826035, ..., 6.45688651, -0.33872724, 5.26551437], [-1.22443516, -1.22443516, -1.22443516, ..., -1.22443516, -1.22443516, -1.22443516]]) transform the class labels from their original string representation (CRC and healthy) into integers from sklearn.preprocessing import LabelEncoder le = LabelEncoder() y = le.fit_transform(y) #Call the transform method of LabelEncorder on two dummy variables #le.transform ([\u0026#39;CRC\u0026#39;, \u0026#39;healthy\u0026#39;]) y array([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, …","date":1667351594,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667351594,"objectID":"2e2ec424c689ca52daeeb743480d7d00","permalink":"https://zouhua.top/post/machine_learning/2022-11-01-ml004-preprocess/","publishdate":"2022-11-01T20:13:14-05:00","relpermalink":"/post/machine_learning/2022-11-01-ml004-preprocess/","section":"post","summary":"Data PreProcessing before building machine learning model","tags":["machine learning","SVN","microbiota"],"title":"Machine Learning on gut microbiota of patients with Colorectal cancer (4): Data Processing","type":"post"},{"authors":null,"categories":["Machine Learning"],"content":"Predictive model using Support Vector Machine (SVM) Support vector machines (SVMs) learning algorithm will be used to build the predictive model. SVMs are one of the most popular classification algorithms, and have an elegant way of transforming nonlinear data so that one can use a linear algorithm to fit a linear model to the data (Cortes and Vapnik 1995).\nKernelized support vector machines are powerful models and perform well on a variety of datasets.\nSVMs allow for complex decision boundaries, even if the data has only a few features.\nThey work well on low-dimensional and high-dimensional data (i.e., few and many features), but don’t scale very well with the number of samples.\nRunning an SVM on data with up to 10,000 samples might work well, but working with datasets of size 100,000 or more can become challenging in terms of runtime and memory usage.\nSVMs requires careful preprocessing of the data and tuning of the parameters. This is why, these days, most people instead use tree-based models such as random forests or gradient boosting (which require little or no preprocessing) in many applications.\nSVM models are hard to inspect; it can be difficult to understand why a particular prediction was made, and it might be tricky to explain the model to a nonexpert.\nImportant Parameters The important parameters in kernel SVMs are the\nRegularization parameter C,\nThe choice of the kernel,(linear, radial basis function(RBF) or polynomial)\nKernel-specific parameters.\ngamma and C both control the complexity of the model, with large values in either resulting in a more complex model. Therefore, good settings for the two parameters are usually strongly correlated, and C and gamma should be adjusted together.\nLoading essential libraries %matplotlib inline import matplotlib.pyplot as plt import pandas as pd import numpy as np from scipy.stats import norm ## Supervised learning. from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import train_test_split from sklearn.svm import SVC from sklearn.model_selection import cross_val_score from sklearn.pipeline import make_pipeline from sklearn.metrics import confusion_matrix from sklearn import metrics, preprocessing from sklearn.metrics import classification_report # visualization import seaborn as sns plt.style.use(\u0026#39;fivethirtyeight\u0026#39;) sns.set_style(\u0026#34;white\u0026#34;) plt.rcParams[\u0026#39;figure.figsize\u0026#39;] = (8, 4) #plt.rcParams[\u0026#39;axes.titlesize\u0026#39;] = \u0026#39;large\u0026#39; Importing data \u0026#39;\u0026#39;\u0026#39; # raw data data_df = pd.read_table(\u0026#39;./dataset/MergeData.tsv\u0026#39;, sep=\u0026#34;\\t\u0026#34;, index_col=0) data = data_df.reset_index(drop=True) data.head() # CLR-transformed data data_df = pd.read_table(\u0026#39;./dataset/MergeData_clr.tsv\u0026#39;, sep=\u0026#34;\\t\u0026#34;, index_col=0) data = data_df.reset_index(drop=True) data.head() \u0026#39;\u0026#39;\u0026#39; # significant species data_df = pd.read_table(\u0026#39;./dataset/MergeData_clr_signif.tsv\u0026#39;, sep=\u0026#34;\\t\u0026#34;, index_col=0) data = data_df.reset_index(drop=True) data.head() Transformation # Assign predictors to a variable of ndarray (matrix) type array = data.values X = array[:, 1:data.shape[1]] # features y = array[:, 0] # transform the class labels from their original string representation (CRC and healthy) into integers le = LabelEncoder() y = le.fit_transform(y) # Normalize the data (center around 0 and scale to remove the variance). scaler = StandardScaler() Xs = scaler.fit_transform(X) Classification with cross-validation As discussed in notebook Pre-Processing the data splitting the data into test and training sets is crucial to avoid overfitting. This allows generalization of real, previously-unseen data. Cross-validation extends this idea further. Instead of having a single train/test split, we specify so-called folds so that the data is divided into similarly-sized folds.\nTraining occurs by taking all folds except one – referred to as the holdout sample.\nOn the completion of the training, you test the performance of your fitted model using the holdout sample.\nThe holdout sample is then thrown back with the rest of the other folds, and a different fold is pulled out as the new holdout sample.\nTraining is repeated again with the remaining folds and we measure performance using the holdout sample. This process is repeated until each fold has had a chance to be a test or holdout sample.\nThe expected performance of the classifier, called cross-validation error, is then simply an average of error rates computed on each holdout sample.\nThis process is demonstrated by first performing a standard train/test split, and then computing cross-validation error.\n# Divide records in training and testing sets. X_train, X_test, y_train, y_test = train_test_split(Xs, y, test_size=0.3, random_state=2, stratify=y) # Create an SVM classifier and train it on 70% of the data set. clf = SVC(probability=True) clf.fit(X_train, y_train) # Analyze accuracy of predictions on 30% of the holdout test sample. classifier_score = clf.score(X_test, y_test) print(\u0026#39;\\nThe classifier accuracy score is …","date":1667351594,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667351594,"objectID":"9cea0f4fc90aba9e1326264acfc9a96e","permalink":"https://zouhua.top/post/machine_learning/2022-11-01-ml005-svm/","publishdate":"2022-11-01T20:13:14-05:00","relpermalink":"/post/machine_learning/2022-11-01-ml005-svm/","section":"post","summary":"Predictive model using Support Vector Machine on gut microbiota","tags":["machine learning","SVN","microbiota"],"title":"Machine Learning on gut microbiota of patients with Colorectal cancer (5): Predictive model using Support Vector Machine","type":"post"},{"authors":null,"categories":["Statistics"],"content":"前言 差异分析是转录组数据分析的必需技能之一，但众多的转录组分析R包如DESeq2，limma和edgeR等让分析人员不知如何选择，还有它们之间的优劣如何？我将在本文详细探讨常用差异分析R包以及结合t-test/wilcox-rank-sum test的差异分析结果的异同点。\n大纲 本文要点由以下几点构成：\n下载以及导入测试数据（批量安装R包）；\n基因表达count矩阵的标准化方法（F(R)PKM/TPM）；\n基因整体水平分布（PCA/tSNE/UMAP；heatmap）；\nDESeq2差异分析实现以及结果解析；\nlimma差异分析实现以及结果解析；\nedgeR差异分析实现以及结果解析；\n结合t-test或wilcox-rank-sum-test方法的差异分析实现以及结果解析（是否符合正态分布选择检验方法）；\n不同方法的结果比较（volcano plot+heatmap+venn）；\n总结。\n导入R包 本次分析需要在R中批量安装包，具体安装方法可参考如何安装R包。先导入基础R包，在后面每个差异分析模块再导入所需要的差异分析R包。\nlibrary(dplyr) library(tibble) library(data.table) library(ggplot2) library(patchwork) library(cowplot) # rm(list = ls()) options(stringsAsFactors = F) options(future.globals.maxSize = 1000 * 1024^2) grp \u0026lt;- c(\u0026#34;Normal\u0026#34;, \u0026#34;Tumor\u0026#34;) grp.col \u0026lt;- c(\u0026#34;#568875\u0026#34;, \u0026#34;#73FAFC\u0026#34;) 转录组数据 本文下载的TCGA-HNSC转录组数据是通过本人先前撰写的R脚本实现的，大家可参考Downloading and preprocessing TCGA Data through R program language文章自行下载，也可以邮件询问我百度网盘密码。\n百度网盘链接：https://pan.baidu.com/s/1Daz5UsOd39T8r8K6zxPASQ\nphenotype \u0026lt;- fread(\u0026#34;TCGA-HNSC-post_mRNA_clinical.csv\u0026#34;) count \u0026lt;- fread(\u0026#34;TCGA-HNSC-post_mRNA_profile.tsv\u0026#34;) table(phenotype$Group) 标准化 标准化的目的是为了降低测序深度以及基因长度对基因表达谱下游分析的影响。测序深度越高则map到基因的reads也越多，同理基因长度越长则map到的reads也越多，最后对应的counts数目也越多。\nRPM/CPM: Reads/Counts of exon model per Million mapped reads (每百万映射读取的reads) $$RPM = \\frac{ExonMappedReads * 10^{6}}{TotalMappedReads}$$\nRPKM: Reads Per Kilobase of exon model per Million mapped reads (每千个碱基的转录每百万映射读取的reads) $$RPKM = \\frac{ExonMappedReads * 10^{9}}{TotalMappedReads * ExonLength}$$\nFPKM: Fragments Per Kilobase of exon model per Million mapped fragments(每千个碱基的转录每百万映射读取的fragments)， 适用于PE测序。 $$ FPKM = \\frac{ExonMappedFragments * 10^{9}}{TotalMappedFragments * ExonLength}$$\nTPM：Transcripts Per Kilobase of exon model per Million mapped reads (每千个碱基的转录每百万映射读取的Transcripts) $$TPM= \\frac{N_i/L_i * 10^{6}}{sum(N_1/L_1+N_2/L_2+…+N_j/L_j+…+N_n/L_n)}$$ $N_i$为比对到第i个exon的reads数； $L_i$为第i个exon的长度；sum()为所有 (n个)exon按长度进行标准化之后数值的和\n获取gene length表：对Homo_sapiens.GRCh38.101版本数据处理获取gene length数据；human_gene_all.tsv是使用biomart包获取gene symbol和ensembleID的对应关系表。 geneLength \u0026lt;- fread(\u0026#34;Homo_sapiens.GRCh38.101.genelength.tsv\u0026#34;) geneIdAll \u0026lt;- fread(\u0026#34;human_gene_all.tsv\u0026#34;) geneIdLength \u0026lt;- geneIdAll %\u0026gt;% filter(transcript_biotype == \u0026#34;protein_coding\u0026#34;) %\u0026gt;% dplyr::select(ensembl_gene_id, external_gene_name) %\u0026gt;% inner_join(geneLength, by = c(\u0026#34;ensembl_gene_id\u0026#34;=\u0026#34;V1\u0026#34;)) %\u0026gt;% dplyr::select(-ensembl_gene_id) %\u0026gt;% dplyr::distinct() %\u0026gt;% dplyr::rename(Length=V2) geneIdLengthUniq \u0026lt;- geneIdLength[pmatch(count$Feature, geneIdLength$external_gene_name), ] %\u0026gt;% filter(!is.na(Length)) %\u0026gt;% arrange(external_gene_name) count_cln \u0026lt;- count %\u0026gt;% filter(Feature%in%geneIdLengthUniq$external_gene_name) %\u0026gt;% arrange(Feature) %\u0026gt;% column_to_rownames(\u0026#34;Feature\u0026#34;) if(!any(geneIdLengthUniq$external_gene_name == rownames(count_cln))){ message(\u0026#34;Order of GeneName is wrong\u0026#34;) } gene_lengths \u0026lt;-geneIdLengthUniq$Length head(geneIdLengthUniq) RPKM/FPKM/TPM 转换 countToFpkm \u0026lt;- function(counts, lengths){ pm \u0026lt;- sum(counts) /1e6 rpm \u0026lt;- counts/pm rpm/(lengths/1000) } countToTpm \u0026lt;- function(counts, lengths) { rpk \u0026lt;- counts/(lengths/1000) coef \u0026lt;- sum(rpk) / 1e6 rpk/coef } # FPKM count_FPKM \u0026lt;- apply(count_cln, 2, function(x){countToFpkm(x, gene_lengths)}) %\u0026gt;% data.frame() # TPM count_TPM \u0026lt;- apply(count_cln, 2, function(x){countToTpm(x, gene_lengths)}) %\u0026gt;% data.frame() head(count_cln) head(count_FPKM) head(count_TPM) 整体水平比较 在做差异分析前，一般可以对数据做一个降维处理，然后看不同分组是否能在二维展开平面区分开。\nExpressionSet 先将数据存成ExpressionSet格式，可参考RNA-seq数据的批次校正方法文章。ExpressionSet对象数据包含表达谱和metadata等数据，这方便后期分析。\ngetExprSet \u0026lt;- function(metadata=phenotype, profile=count_cln, occurrence=0.2){ # metadata=phenotype # profile=count_cln # occurrence=0.2 sid \u0026lt;- intersect(metadata$SampleID, colnames(profile)) # phenotype phe \u0026lt;- metadata %\u0026gt;% filter(SampleID%in%sid) %\u0026gt;% column_to_rownames(\u0026#34;SampleID\u0026#34;) # profile by occurrence prf \u0026lt;- profile %\u0026gt;% rownames_to_column(\u0026#34;tmp\u0026#34;) %\u0026gt;% filter(apply(dplyr::select(., -one_of(\u0026#34;tmp\u0026#34;)), 1, function(x) { sum(x != 0)/length(x)}) \u0026gt; occurrence) %\u0026gt;% dplyr::select(c(tmp, rownames(phe))) %\u0026gt;% column_to_rownames(\u0026#34;tmp\u0026#34;) # determine the right order between profile and phenotype for(i in 1:ncol(prf)){ if (!(colnames(prf)[i] == rownames(phe)[i])) { stop(paste0(i, \u0026#34; Wrong\u0026#34;)) } } require(convert) exprs \u0026lt;- as.matrix(prf) adf \u0026lt;- new(\u0026#34;AnnotatedDataFrame\u0026#34;, data=phe) experimentData \u0026lt;- new(\u0026#34;MIAME\u0026#34;, name=\u0026#34;Hua Zou\u0026#34;, lab=\u0026#34;UCAS\u0026#34;, contact=\u0026#34;zouhua@outlook.com\u0026#34;, title=\u0026#34;TCGA-HNSC\u0026#34;, abstract=\u0026#34;The gene ExpressionSet\u0026#34;, url=\u0026#34;www.zouhua.top\u0026#34;, other=list(notes=\u0026#34;Created from text files\u0026#34;)) expressionSet \u0026lt;- new(\u0026#34;ExpressionSet\u0026#34;, exprs=exprs, phenoData=adf, experimentData=experimentData) return(expressionSet) } ExprSet_count \u0026lt;- getExprSet(profile=count_cln) ExprSet_FPKM \u0026lt;- getExprSet(profile=count_FPKM) ExprSet_TPM \u0026lt;- getExprSet(profile=count_TPM) ExprSet_count ExprSet_FPKM ExprSet_TPM 降维分析 高纬度数据降维方法很多，我们这里选择了PCA+tSNE+UMAP分别展示降维后的结果，可参考文章高纬度数据降维方法的R实现。另外合并多个图的方法，请参考文章patchwork:合并多个R图的包。\nPCA PCAFun \u0026lt;- function(dataset = ExprSet_count){ # dataset = ExprSet_count require(convert) metadata \u0026lt;- pData(dataset) profile \u0026lt;- …","date":1666919594,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666919594,"objectID":"502dca6c954348598c688fab47f1c873","permalink":"https://zouhua.top/post/math_statistics/2022-10-27-da-methods-comparsion/","publishdate":"2022-10-27T20:13:14-05:00","relpermalink":"/post/math_statistics/2022-10-27-da-methods-comparsion/","section":"post","summary":"差异分析是转录组数据分析的必备技能之一，但众多的转录组分析R包*DESeq2*，*limma*和*edgeR*让分析人员有选择问题，那么它们之间的优劣到底是如何的呢?","tags":["RNA-seq","DifferentialAnalysis"],"title":"转录组差异分析（DESeq2+limma+edgeR+t-test/wilcox-test）总结","type":"post"},{"authors":null,"categories":["Tool"],"content":"Introduction Have you ever thought about making your own website using R and Rstudio? This tutorial would teach you how to create your personal blog via blogdown and github. What’s more, automatically eploying your website is also necessary by using github actions.\nPrerequisites Before running the tutorial, you need have this software in your PC\nR\nRstudio\nRegistering github account Please go to github to obtain your own account. Then create a new repository named [your_github_names].github.io which is applied to render your github pages.\nCreating repository for blog Creating another private repository (named MyBlog) to save your files which are used to setup blog. Here, suggesting users create two repositories to deploy personal website according to the protection of personal privacy.\n[your_github_names].github.io for saving blog html files MyBlog for saving blog source files Install Blogdown and Hugo Blogdown which Creating Blogs and Websites with R Markdown is necessary for building website\n# Install blogdown #install.packages(\u0026#34;blogdown\u0026#34;) remotes::install_github(\u0026#39;rstudio/blogdown\u0026#39;) # Install Hugo blogdown::install_hugo() Creating blogdown project Firstly, open Rstudio to create New Project -\u0026gt; New Directory -\u0026gt; Website using blogdown and then give MyOwnWebsite to Directory name, and then setting Hugo theme by gcushen/hugo-academic. Finally, Kick Create Project to generate website.\nThen, opening Rstudio to create blogdown project (File -\u0026gt; New Project -\u0026gt; Website using blogdown -\u0026gt; New Prject Wizard -\u0026gt; Create Project). YOu could see the following files and directories.\nthe final website folder like this\nBuilding website To build the website by using hugo_build from blogdown R package.\nblogdown::hugo_build(local=TRUE) All the actual files of the website are stored in public/ folder.\nTo preview the website using\nblogdown::serve_site() config.yaml: Hugo and theme configuration file. .Rprofile: File to set up your blogdown options. netlify.toml: File to set up your Netlify options. content/: Website source files to edit and add, such as blog posts. themes/: Hugo theme. Basic Customization The basic files that you want to modify to customize your website are the following:\nconfig/_default/config.yaml: general website information config/_default/params.yaml: website customization config/_default/menus.yaml: top bar / menu customization content/authors/admin/_index.md: personal information Uploading blogdown project to MyBlog Repository Moving the files of blogdown into MyOwnWebsite directory and then using git to push all the files to the remote repository.\nCreating Github Actions Opening MyBlog Repository (Actions -\u0026gt; New workflow) to create deploy.yml.\nname: deploy_blog on: push: branches: [main, master] pull_request: branches: [main, master] release: types: [published] workflow_dispatch: jobs: build: runs-on: ubuntu-latest steps: - name: checkout uses: actions/checkout@v3 with: submodules: true - name: Setup Hugo uses: peaceiris/actions-hugo@v2.5.0 with: hugo-version: \u0026#39;0.105.0\u0026#39; extended: true - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3.8.0 with: deploy_key: ${{ secrets.ACTIONS_DEPLOY_KEY }} external_repository: HuaZou/HuaZou.github.io publish_branch: master publish_dir: ./public allow_empty_commit: true Deployment We need to connect the Github Pages repository ([your_github_names].github.io) and blog repository (MyBlog) through ssh key. Go to ~/.ssh/.\nssh-keygen -t rsa -C \u0026#34;zouhua1@outlook.com\u0026#34; private key is for MyOwnWebsite (settings -\u0026gt; Secrets)\npublic key is for [your_github_names].github.io (Deploy keys)\nWhen you upload your files to MyOwnWebsite and github will automatically compile the files and then deploy html files to [your_github_names].github.io to render your website.\nReference How To Make A Personal Website with Hugo ","date":1661217194,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661217194,"objectID":"4b8eda7a56d5803a903c60e6acb57df4","permalink":"https://zouhua.top/post/tool/2022-08-22-blogdown-hugo/","publishdate":"2022-08-22T20:13:14-05:00","relpermalink":"/post/tool/2022-08-22-blogdown-hugo/","section":"post","summary":"Introduction Have you ever thought about making your own website using R and Rstudio? This tutorial would teach you how to create your personal blog via blogdown and github. What’s more, automatically eploying your website is also necessary by using github actions.","tags":["blogdown","github","hugo"],"title":"How to setup personal blog by blogdown and githup pages","type":"post"},{"authors":["Hua Zou"],"categories":null,"content":"","date":1660867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660867200,"objectID":"87c79596733bec228d6c7e28407533a7","permalink":"https://zouhua.top/bookdown/2022-xmas2/","publishdate":"2022-08-19T00:00:00Z","relpermalink":"/bookdown/2022-xmas2/","section":"bookdown","summary":"In this tutorial, we focus on differential analysis with the step-by-step procedures by using the R programming language. We also briefly introduce the concepts and principals of the statistical methods before the applications and give conclusions on the results.","tags":null,"title":"Statistical toolkits on microbiota data","type":"bookdown"},{"authors":["Hua Zou","Dan Wang","Huahui Ren"],"categories":null,"content":"","date":1582502400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582502400,"objectID":"20d79b52d65fe064fabe61be5389ae1c","permalink":"https://zouhua.top/publication/2020-nutrients/","publishdate":"2020-02-24T00:00:00Z","relpermalink":"/publication/2020-nutrients/","section":"publication","summary":"This non-controlled intervention study revealed associations between baseline gut microbiota and CR-induced BMI loss and provided evidence to accelerate the application of microbiome stratification in future personalized nutrition intervention.","tags":null,"title":"Effect of Caloric Restriction on BMI, Gut Microbiota, and Blood Amino Acid Levels in Non-Obese Adults","type":"publication"}]