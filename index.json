
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Hua Zou is a senior bioinformatics analyst at Xbiome Company since Mar 7 2022. My research interests include host-microbiota intersection, machine learning and multi-omics data integration. I lead the development of XMAS 2.0.\nDownload my resumé .\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Hua Zou is a senior bioinformatics analyst at Xbiome Company since Mar 7 2022. My research interests include host-microbiota intersection, machine learning and multi-omics data integration. I lead the development of XMAS 2.","tags":null,"title":"Hua Zou","type":"authors"},{"authors":null,"categories":["Machine Learning"],"content":"Optimizing the SVM Classifier Notebook 7: Optimizing the Support Vector Classifier.\nMachine learning models are parameterized so that their behavior can be tuned for a given problem. Models can have many parameters and finding the best combination of parameters can be treated as a search problem. In this notebook, I aim to tune parameters of the SVM Classification model using scikit-learn.\nLoading libraries %matplotlib inline import matplotlib.pyplot as plt #Load libraries for data processing import pandas as pd import numpy as np from scipy.stats import norm ## Supervised learning. from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import train_test_split from sklearn.svm import SVC from sklearn.model_selection import cross_val_score from sklearn.model_selection import GridSearchCV from sklearn.pipeline import make_pipeline from sklearn.metrics import confusion_matrix from sklearn import metrics, preprocessing from sklearn.metrics import classification_report from sklearn.feature_selection import SelectKBest, f_regression # visualization import seaborn as sns plt.style.use(\u0026#39;fivethirtyeight\u0026#39;) sns.set_style(\u0026#34;white\u0026#34;) plt.rcParams[\u0026#39;figure.figsize\u0026#39;] = (8,4) #plt.rcParams[\u0026#39;axes.titlesize\u0026#39;] = \u0026#39;large\u0026#39; Importing data \u0026#39;\u0026#39;\u0026#39; # raw data data_df = pd.read_table(\u0026#39;./dataset/MergeData.tsv\u0026#39;, sep=\u0026#34;\\t\u0026#34;, index_col=0) data = data_df.reset_index(drop=True) data.head() # CLR-transformed data data_df = pd.read_table(\u0026#39;./dataset/MergeData_clr.tsv\u0026#39;, sep=\u0026#34;\\t\u0026#34;, index_col=0) data = data_df.reset_index(drop=True) data.head() \u0026#39;\u0026#39;\u0026#39; # significant species data_df = pd.read_table(\u0026#39;./dataset/MergeData_clr_signif.tsv\u0026#39;, sep=\u0026#34;\\t\u0026#34;, index_col=0) data = data_df.reset_index(drop=True) data.head() Building a predictive model and evaluate with 5-cross validation using support vector classifies (ref Predictive model using Support Vector Machine) for details #Assign predictors to a variable of ndarray (matrix) type array = data.values X = array[:, 1:data.shape[1]] y = array[:, 0] #transform the class labels from their original string representation (M and B) into integers le = LabelEncoder() y = le.fit_transform(y) # Normalize the data (center around 0 and scale to remove the variance). scaler = StandardScaler() Xs = scaler.fit_transform(X) from sklearn.decomposition import PCA # feature extraction pca = PCA(n_components=10) fit = pca.fit(Xs) X_pca = pca.transform(Xs) # 5. Divide records in training and testing sets. X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=2, stratify=y) # 6. Create an SVM classifier and train it on 70% of the data set. clf = SVC(probability=True) clf.fit(X_train, y_train) #7. Analyze accuracy of predictions on 30% of the holdout test sample. classifier_score = clf.score(X_test, y_test) print (\u0026#39;\\nThe classifier accuracy score is {:03.2f}\\n\u0026#39;.format(classifier_score)) clf2 = make_pipeline(SelectKBest(f_regression, k=3),SVC(probability=True)) scores = cross_val_score(clf2, X_pca, y, cv=3) # Get average of 5-fold cross-validation score using an SVC estimator. n_folds = 5 cv_error = np.average(cross_val_score(SVC(), X_pca, y, cv=n_folds)) #print (\u0026#39;\\nThe {}-fold cross-validation accuracy score for this classifier is {:.2f}\\n\u0026#39;.format(n_folds, cv_error)) y_pred = clf.fit(X_train, y_train).predict(X_test) cm = metrics.confusion_matrix(y_test, y_pred) print(classification_report(y_test, y_pred )) fig, ax = plt.subplots(figsize=(5, 5)) ax.matshow(cm, cmap=plt.cm.Reds, alpha=0.3) for i in range(cm.shape[0]): for j in range(cm.shape[1]): ax.text(x=j, y=i, s=cm[i, j], va=\u0026#39;center\u0026#39;, ha=\u0026#39;center\u0026#39;) plt.xlabel(\u0026#39;Predicted Values\u0026#39;, ) plt.ylabel(\u0026#39;Actual Values\u0026#39;) plt.show() The classifier accuracy score is 0.60 precision recall f1-score support 0 0.64 0.50 0.56 78 1 0.57 0.70 0.63 74 accuracy 0.60 152 macro avg 0.61 0.60 0.60 152 weighted avg 0.61 0.60 0.59 152 Importance of optimizing a classifier We can tune two key parameters of the SVM algorithm:\nthe value of C (how much to relax the margin) and the type of kernel. The default for SVM (the SVC class) is to use the Radial Basis Function (RBF) kernel with a C value set to 1.0. Like with KNN, we will perform a grid search using 10-fold cross validation with a standardized copy of the training dataset. We will try a number of simpler kernel types and C values with less bias and more bias (less than and more than 1.0 respectively).\nPython scikit-learn provides two simple methods for algorithm parameter tuning:\nGrid Search Parameter Tuning. Random Search Parameter Tuning. # Train classifiers. kernel_values = [ \u0026#39;linear\u0026#39; , \u0026#39;poly\u0026#39; , \u0026#39;rbf\u0026#39; , \u0026#39;sigmoid\u0026#39; ] param_grid = {\u0026#39;C\u0026#39;: np.logspace(-3, 2, 6), \u0026#39;gamma\u0026#39;: np.logspace(-3, 2, 6),\u0026#39;kernel\u0026#39;: kernel_values} grid = GridSearchCV(SVC(), param_grid=param_grid, cv=5) grid.fit(X_train, y_train) GridSearchCV(cv=5, estimator=SVC(), param_grid={\u0026#39;C\u0026#39;: array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02]), \u0026#39;gamma\u0026#39;: array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, …","date":1667387594,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667387594,"objectID":"5cf11f0fd30893f7fc286771f84c21fd","permalink":"https://zouhua.top/post/machine_learning/ml007-optialclassifier/","publishdate":"2022-11-02T06:13:14-05:00","relpermalink":"/post/machine_learning/ml007-optialclassifier/","section":"post","summary":"Tuning parameters for optimal Classifier","tags":["machine learning","SVM","microbiota"],"title":"Machine Learning on gut microbiota of patients with Colorectal cancer (7): Optimizing the SVM Classifier","type":"post"},{"authors":null,"categories":["Machine Learning"],"content":"Predictive model using Support Vector Machine (SVM) Notebook 6 Predictive model using Support Vector Machine (svm).\nSupport vector machines (SVMs) learning algorithm will be used to build the predictive model. SVMs are one of the most popular classification algorithms, and have an elegant way of transforming nonlinear data so that one can use a linear algorithm to fit a linear model to the data (Cortes and Vapnik 1995).\nKernelized support vector machines are powerful models and perform well on a variety of datasets.\nSVMs allow for complex decision boundaries, even if the data has only a few features.\nThey work well on low-dimensional and high-dimensional data (i.e., few and many features), but don’t scale very well with the number of samples.\nRunning an SVM on data with up to 10,000 samples might work well, but working with datasets of size 100,000 or more can become challenging in terms of runtime and memory usage.\nSVMs requires careful preprocessing of the data and tuning of the parameters. This is why, these days, most people instead use tree-based models such as random forests or gradient boosting (which require little or no preprocessing) in many applications.\nSVM models are hard to inspect; it can be difficult to understand why a particular prediction was made, and it might be tricky to explain the model to a nonexpert.\nImportant Parameters The important parameters in kernel SVMs are the\nRegularization parameter C,\nThe choice of the kernel,(linear, radial basis function(RBF) or polynomial)\nKernel-specific parameters.\ngamma and C both control the complexity of the model, with large values in either resulting in a more complex model. Therefore, good settings for the two parameters are usually strongly correlated, and C and gamma should be adjusted together.\nLoading essential libraries %matplotlib inline import matplotlib.pyplot as plt import pandas as pd import numpy as np from scipy.stats import norm ## Supervised learning. from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import train_test_split from sklearn.svm import SVC from sklearn.model_selection import cross_val_score from sklearn.pipeline import make_pipeline from sklearn.metrics import confusion_matrix from sklearn import metrics, preprocessing from sklearn.metrics import classification_report # visualization import seaborn as sns plt.style.use(\u0026#39;fivethirtyeight\u0026#39;) sns.set_style(\u0026#34;white\u0026#34;) plt.rcParams[\u0026#39;figure.figsize\u0026#39;] = (8, 4) #plt.rcParams[\u0026#39;axes.titlesize\u0026#39;] = \u0026#39;large\u0026#39; Importing data \u0026#39;\u0026#39;\u0026#39; # raw data data_df = pd.read_table(\u0026#39;./dataset/MergeData.tsv\u0026#39;, sep=\u0026#34;\\t\u0026#34;, index_col=0) data = data_df.reset_index(drop=True) data.head() # CLR-transformed data data_df = pd.read_table(\u0026#39;./dataset/MergeData_clr.tsv\u0026#39;, sep=\u0026#34;\\t\u0026#34;, index_col=0) data = data_df.reset_index(drop=True) data.head() \u0026#39;\u0026#39;\u0026#39; # significant species data_df = pd.read_table(\u0026#39;./dataset/MergeData_clr_signif.tsv\u0026#39;, sep=\u0026#34;\\t\u0026#34;, index_col=0) data = data_df.reset_index(drop=True) data.head() Transformation # Assign predictors to a variable of ndarray (matrix) type array = data.values X = array[:, 1:data.shape[1]] # features y = array[:, 0] # transform the class labels from their original string representation (CRC and healthy) into integers le = LabelEncoder() y = le.fit_transform(y) # Normalize the data (center around 0 and scale to remove the variance). scaler = StandardScaler() Xs = scaler.fit_transform(X) Classification with cross-validation As discussed in notebook Pre-Processing the data splitting the data into test and training sets is crucial to avoid overfitting. This allows generalization of real, previously-unseen data. Cross-validation extends this idea further. Instead of having a single train/test split, we specify so-called folds so that the data is divided into similarly-sized folds.\nTraining occurs by taking all folds except one – referred to as the holdout sample.\nOn the completion of the training, you test the performance of your fitted model using the holdout sample.\nThe holdout sample is then thrown back with the rest of the other folds, and a different fold is pulled out as the new holdout sample.\nTraining is repeated again with the remaining folds and we measure performance using the holdout sample. This process is repeated until each fold has had a chance to be a test or holdout sample.\nThe expected performance of the classifier, called cross-validation error, is then simply an average of error rates computed on each holdout sample.\nThis process is demonstrated by first performing a standard train/test split, and then computing cross-validation error.\n# Divide records in training and testing sets. X_train, X_test, y_train, y_test = train_test_split(Xs, y, test_size=0.3, random_state=2, stratify=y) # Create an SVM classifier and train it on 70% of the data set. clf = SVC(probability=True) clf.fit(X_train, y_train) # Analyze accuracy of predictions on 30% of the holdout test sample. classifier_score = …","date":1667351594,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667351594,"objectID":"f7a4bb179a185c848ddf980839047c86","permalink":"https://zouhua.top/post/machine_learning/ml006-svm/","publishdate":"2022-11-01T20:13:14-05:00","relpermalink":"/post/machine_learning/ml006-svm/","section":"post","summary":"Predictive model using Support Vector Machine on gut microbiota","tags":["machine learning","SVM","microbiota"],"title":"Machine Learning on gut microbiota of patients with Colorectal cancer (6): Predictive model using Support Vector Machine","type":"post"},{"authors":null,"categories":["Machine Learning"],"content":"Differential Analysis Notebook 5 Differential Analysis.\nThere are 152 species in our dataset. In order to clinically interpret the potential biomarkers identified by the next step in machine learning, we perform the differential analysis to figure out the significant species as the inputs for building models. The method and criterion are as follows:\nT-test based on CLR tranformed data; Pvalue or Adjusted-Pvalue is less than 0.05; the absolute values of Log2Foldchange on Mean values is more than 1. Goal Find the significant features of the data and choose them for machine learning.\nLoading data and essential libraries import pandas as pd import numpy as np data_df = pd.read_table(\u0026#39;./dataset/MergeData_clr.tsv\u0026#39;, sep=\u0026#34;\\t\u0026#34;, index_col=0) data = data_df.reset_index(drop=True) data.head() T test Since profile are CLR transformed data, we choose t test to identify the significant species\nfrom itertools import combinations from scipy import stats as st def all_pairwise(df, compare_col = \u0026#39;disease\u0026#39;): decade_pairs = [(i, j) for i, j in combinations(df[compare_col].unique().tolist(), 2)] # or add a list of colnames to function signature cols = list(df.columns) cols.remove(compare_col) list_of_dfs = [] for pair in decade_pairs: for col in cols: c1 = df[df[compare_col] == pair[0]][col] c2 = df[df[compare_col] == pair[1]][col] results = st.ttest_ind(c1, c2, nan_policy=\u0026#39;omit\u0026#39;) tmp = pd.DataFrame({\u0026#39;dec1\u0026#39;: pair[0], \u0026#39;dec2\u0026#39;: pair[1], \u0026#39;tstat\u0026#39;: results.statistic, \u0026#39;pvalue\u0026#39;: results.pvalue}, index = [col]) list_of_dfs.append(tmp) df_stats = pd.concat(list_of_dfs) return df_stats ttest_res = all_pairwise(data) ttest_res.head() FDR correction def correct_pvalues_for_multiple_testing(pvalues, correction_type = \u0026#34;Benjamini-Hochberg\u0026#34;): \u0026#34;\u0026#34;\u0026#34; consistent with R - print correct_pvalues_for_multiple_testing([0.0, 0.01, 0.029, 0.03, 0.031, 0.05, 0.069, 0.07, 0.071, 0.09, 0.1]) \u0026#34;\u0026#34;\u0026#34; from numpy import array, empty pvalues = array(pvalues) #n = float(pvalues.shape[0]) n = pvalues.shape[0] new_pvalues = empty(n) if correction_type == \u0026#34;Bonferroni\u0026#34;: new_pvalues = n * pvalues elif correction_type == \u0026#34;Bonferroni-Holm\u0026#34;: values = [ (pvalue, i) for i, pvalue in enumerate(pvalues) ] values.sort() for rank, vals in enumerate(values): pvalue, i = vals new_pvalues[i] = (n-rank) * pvalue elif correction_type == \u0026#34;Benjamini-Hochberg\u0026#34;: values = [ (pvalue, i) for i, pvalue in enumerate(pvalues) ] values.sort() values.reverse() new_values = [] for i, vals in enumerate(values): rank = n - i pvalue, index = vals new_values.append((n/rank) * pvalue) for i in range(0, int(n)-1): if new_values[i] \u0026lt; new_values[i+1]: new_values[i+1] = new_values[i] for i, vals in enumerate(values): pvalue, index = vals new_pvalues[index] = new_values[i] return new_pvalues ttest_res[\u0026#39;Adjusted-pvalue\u0026#39;] = correct_pvalues_for_multiple_testing(ttest_res[\u0026#39;pvalue\u0026#39;], correction_type = \u0026#34;Benjamini-Hochberg\u0026#34;) ttest_res_sort = ttest_res.sort_values(by=\u0026#39;Adjusted-pvalue\u0026#39;) ttest_res_sort.head() Significant species with pvalue less than 0.05 threshold = 0.05 # pvalue or Adjusted-pvalue signif_species = ttest_res_sort.loc[ttest_res_sort[\u0026#39;pvalue\u0026#39;] \u0026lt; threshold] signif_species.shape (40, 5) There are 40 significant species identified by pvalue between CRC and healthy group\nOutput Selecting the 40 species profile to downstream analysis\ndata_signif = pd.concat([data[\u0026#39;disease\u0026#39;], data[signif_species.index.tolist()]], axis=1) data_signif.to_csv(\u0026#39;./dataset/MergeData_clr_signif.tsv\u0026#39;, sep=\u0026#39;\\t\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;, index=True) data_signif.head() correlation matrix # plot correlation matrix import pandas as pd import numpy as np import seaborn as sns from matplotlib import pyplot as plt plt.style.use(\u0026#39;fivethirtyeight\u0026#39;) sns.set_style(\u0026#34;white\u0026#34;) # Compute the correlation matrix data_species = data_signif.iloc[:, 1:data_signif.shape[1]] corr = data_species.corr(method=\u0026#34;spearman\u0026#34;) # Generate a mask for the upper triangle mask = np.zeros_like(corr, dtype=\u0026#39;bool\u0026#39;) mask[np.triu_indices_from(mask)] = True # Set up the matplotlib figure fig, ax = plt.subplots(figsize=(20, 20)) plt.title(\u0026#39;CRC Species Correlation\u0026#39;) # Generate a custom diverging colormap cmap = sns.diverging_palette(260, 10, as_cmap=True) # Draw the heatmap with the mask and correct aspect ratio sns.heatmap(corr, vmax=1.1, square=\u0026#39;square\u0026#39;, cmap=cmap, mask=mask, ax=ax, annot=True, fmt=\u0026#39;.1g\u0026#39;, linewidths=2) ​ A Summary of the significant species here: t-test on microbiota data FDR correction on pvalue of t-test obtain significant species Referenece How to implement R’s p.adjust in Python T Test on Multiple Columns in Dataframe ","date":1667265194,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667265194,"objectID":"2e667db37a878d414361f9db86a1d449","permalink":"https://zouhua.top/post/machine_learning/ml005-differential/","publishdate":"2022-10-31T20:13:14-05:00","relpermalink":"/post/machine_learning/ml005-differential/","section":"post","summary":"Differential Analysis before building machine learning model","tags":["machine learning","SVM","microbiota"],"title":"Machine Learning on gut microbiota of patients with Colorectal cancer (5): Differential Analysis","type":"post"},{"authors":null,"categories":["Machine Learning"],"content":"Data PreProcessing Notebook 4 Pre-Processing the data.\nData preprocessing is a crucial step for any data analysis problem. It is often a very good idea to prepare your data in such way to best expose the structure of the problem to the machine learning algorithms that you intend to use. This involves a number of activities such as:\nAssigning numerical values to categorical data; Handling missing values; Normalizing the features (so that features on small scales do not dominate when fitting a model to the data). In Exploratory Data Analysis, I explored the data, to help gain insight on the distribution of the data as well as how the attributes correlate to each other. I identified some features of interest. In this notebook I use feature selection to reduce high-dimension data, feature extraction and transformation for dimensional reduction.\nGoal Find the most predictive features of the data and filter it so it will enhance the predictive power of the analytics model.\nLoading data and essential libraries %matplotlib inline import matplotlib.pyplot as plt import pandas as pd import numpy as np from scipy.stats import norm # visualization import seaborn as sns plt.style.use(\u0026#39;fivethirtyeight\u0026#39;) sns.set_style(\u0026#34;white\u0026#34;) plt.rcParams[\u0026#39;figure.figsize\u0026#39;] = (8,4) #plt.rcParams[\u0026#39;axes.titlesize\u0026#39;] = \u0026#39;large\u0026#39; Importing data data_df = pd.read_table(\u0026#39;./dataset/MergeData.tsv\u0026#39;, sep=\u0026#34;\\t\u0026#34;, index_col=0) data = data_df.reset_index(drop=True) data.head() Label encoding Here, I assign the features to a NumPy array X, and transform the class labels from their original string representation (CRC and healthy) into integers\narray = data.values X_temp = array[:, 1:data.shape[1]] y = array[:, 0] array array([[\u0026#39;healthy\u0026#39;, 46509517, 8249892, ..., 0, 0, 0], [\u0026#39;healthy\u0026#39;, 5334509, 230275, ..., 0, 0, 0], [\u0026#39;healthy\u0026#39;, 6868169, 4054008, ..., 0, 0, 0], ..., [\u0026#39;healthy\u0026#39;, 0, 0, ..., 0, 0, 0], [\u0026#39;healthy\u0026#39;, 2286204, 242316, ..., 630160, 653, 191409], [\u0026#39;healthy\u0026#39;, 0, 0, ..., 0, 0, 0]], dtype=object) centered log-ratio (clr) transformation Transforms compositions from Aitchison geometry to the real space (skbio.stats.composition.clr).\nfrom skbio.stats.composition import clr pseude_value = np.amin(np.array(X_temp)[X_temp != np.amin(X_temp)]) data_temp = data.iloc[:, 1:data.shape[1]].T data_clr = data_temp.apply(lambda x: clr(x + pseude_value), axis=0).T X = data_clr.values[:, 0:data_clr.shape[1]] X array([[10.26214602, 8.53269436, 7.7298392 , ..., -3.44177889, -3.44177889, -3.44177889], [ 7.60933349, 4.46687166, 6.65482253, ..., -3.92913994, -3.92913994, -3.92913994], [ 8.41126745, 7.88408118, 7.66846676, ..., -3.37990451, -3.37990451, -3.37990451], ..., [-1.88844976, -1.88844976, 6.09721786, ..., -1.88844976, -1.88844976, -1.88844976], [ 7.74550107, 5.50128747, 7.86826035, ..., 6.45688651, -0.33872724, 5.26551437], [-1.22443516, -1.22443516, -1.22443516, ..., -1.22443516, -1.22443516, -1.22443516]]) transform the class labels from their original string representation (CRC and healthy) into integers from sklearn.preprocessing import LabelEncoder le = LabelEncoder() y = le.fit_transform(y) #Call the transform method of LabelEncorder on two dummy variables #le.transform ([\u0026#39;CRC\u0026#39;, \u0026#39;healthy\u0026#39;]) y array([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) After encoding the class labels(disease) in an array y, the Responsed patients are now represented as class 1(i.e prescence of Response) and the Non-Responsed patients are represented as class 0 (i.e healthy), respectively, illustrated by calling the transform method of LabelEncorder …","date":1667178794,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667178794,"objectID":"0cbad5f306e3bc9b4e582609c1a1ed03","permalink":"https://zouhua.top/post/machine_learning/ml004-preprocess/","publishdate":"2022-10-30T20:13:14-05:00","relpermalink":"/post/machine_learning/ml004-preprocess/","section":"post","summary":"Data PreProcessing before building machine learning model","tags":["machine learning","SVM","microbiota"],"title":"Machine Learning on gut microbiota of patients with Colorectal cancer (4): Data Processing","type":"post"},{"authors":null,"categories":["Machine Learning"],"content":"Exploratory Data Analysis Notebook 3 Exploratory Data Analysis.\nNow that we have a good intuitive sense of the data, Next step involves taking a closer look at attributes and data values. In this section, I am getting familiar with the data, which will provide useful knowledge for data pre-processing.\nObjectives of Data Exploration Exploratory data analysis (EDA) is a very important step which takes place after feature engineering and acquiring data and it should be done before any modeling. This is because it is very important for a data scientist to be able to understand the nature of the data without making assumptions. The results of data exploration can be extremely useful in grasping the structure of the data, the distribution of the values, and the presence of extreme values and interrelationships within the data set.\nThe purpose of EDA is:\nTo use summary statistics and visualizations to better understand data, Finding clues about the tendencies of the data, its quality and to formulate assumptions and the hypothesis of our analysis For data preprocessing to be successful, it is essential to have an overall picture of your data Basic statistical descriptions can be used to identify properties of the data and highlight which data values should be treated as noise or outliers. Next step is to explore the data. There are two approached used to examine the data using:\nDescriptive statistics is the process of condensing key characteristics of the data set into simple numeric metrics. Some of the common metrics used are mean, standard deviation, and correlation.\nVisualization is the process of projecting the data, or parts of it, into Cartesian space or into abstract images. In the data mining process, data exploration is leveraged in many different steps including preprocessing, modeling, and interpretation of results.\nSummary statistics are measurements meant to describe data. In the field of descriptive statistics, there are many summary measurements\nLoading libraries %matplotlib inline import matplotlib.pyplot as plt import pandas as pd import numpy as np from scipy.stats import norm import seaborn as sns import statistics as st Descriptive statistics data = pd.read_table(\u0026#39;./dataset/MergeData.tsv\u0026#39;, sep=\u0026#34;\\t\u0026#34;, index_col=0) data.head() basic descriptive statistics data.describe() distribution data.skew() /var/folders/82/kf2cy4v112b374jb5xcvmwh40000gn/T/ipykernel_71511/1188251951.py:1: FutureWarning: The default value of numeric_only in DataFrame.skew is deprecated. In a future version, it will default to False. In addition, specifying \u0026#39;numeric_only=None\u0026#39; is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning. data.skew() s__Bacteroides_plebeius 3.829753 s__Bacteroides_dorei 3.183878 s__Faecalibacterium_prausnitzii 1.662481 s__Eubacterium_eligens 5.588910 s__Bacteroides_ovatus 7.612611 ... s__Klebsiella_pneumoniae 11.966032 s__Bacteroides_coprocola 3.615256 s__Ruminococcus_lactaris 5.572388 s__Turicimonas_muris 11.987903 s__Proteobacteria_bacterium_CAG_139 7.546497 Length: 151, dtype: float64 The skew result show a positive (right) or negative (left) skew. Values closer to zero show less skew. From the results, we can see that the relative abundance of most taxa are right skew. Since this, we should use CLR transformation to normalize the data\ndisease data.disease.unique() array([\u0026#39;healthy\u0026#39;, \u0026#39;CRC\u0026#39;], dtype=object) Group by disease and review the output diag_gr = data.groupby(\u0026#39;disease\u0026#39;, axis=0) pd.DataFrame(diag_gr.size(), columns=[\u0026#39;# of observations\u0026#39;]) Check binary encoding from 01.DataClean to confirm the coversion of the disease categorical data into numeric, where\nCRC = 1 (indicates prescence of disease) healthy = 0 (indicates control) Observation 258 observations indicating the prescence of CRC and 111 show control\nLets confirm this, by ploting the histogram\nUnimodal Data Visualizations One of the main goals of visualizing the data here is to observe which features are most helpful in predicting CRC or healthy. The other is to see general trends that may aid us in model selection and hyper parameter selection.\nApply 4 techniques that you can use to understand each attribute of your dataset independently.\nHistograms. Density Plots. Box and Whisker Plots. Scatter Plots sns.set_style(\u0026#34;white\u0026#34;) sns.set_context({\u0026#34;figure.figsize\u0026#34;: (5, 6)}) ax = sns.countplot(x = \u0026#39;disease\u0026#39;, data = data, label = \u0026#34;Count\u0026#34;, palette = \u0026#34;Set1\u0026#34;) ax.bar_label(ax.containers[0], fontsize=15) Visualise distribution of data via histograms Histograms are commonly used to visualize numerical variables. A histogram is similar to a bar graph after the values of the variable are grouped (binned) into a finite number of intervals (bins).\nHistograms group data into bins and provide you a count of the number of observations in each bin. From the shape of the bins you can quickly get a feeling for whether an attribute is Gaussian, skewed or even has an exponential distribution. It can also help you see …","date":1667092394,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667092394,"objectID":"7144546fa651a48d04d68b50f9cc370b","permalink":"https://zouhua.top/post/machine_learning/ml003-exploratory/","publishdate":"2022-10-29T20:13:14-05:00","relpermalink":"/post/machine_learning/ml003-exploratory/","section":"post","summary":"Exploratory Data Analysis before performing machine learning","tags":["machine learning","SVM","microbiota"],"title":"Machine Learning on gut microbiota of patients with Colorectal cancer (3): Exploratory Data Analysis","type":"post"},{"authors":null,"categories":["Machine Learning"],"content":"Data clean Notebook 2: Identifying the problem and Getting data.\nRemoving the unmathed samples or participants.\nIdentify the problem Colorectal cancer (CRC), also known as bowel cancer, colon cancer, or rectal cancer, is the development of cancer from the colon or rectum (parts of the large intestine). Signs and symptoms may include blood in the stool, a change in bowel movements, weight loss, and fatigue. Gut microbiota play crucial role in CRC progression. Here, to investigate whether the gut microbiota could predict healthy control or CRC patients.\nExpected outcome Since this build a model that can classify healthy control or CRC patients using two training classification:\nCRC = Patients - Present healthy = Control - Absent Objective Since the labels in the data are discrete, the predication falls into two categories, (i.e. CRC or healthy). In machine learning this is a classification problem.\nThus, the goal is to classify healthy control or CRC patients. To achieve this we have used machine learning classification methods to fit a function that can predict the discrete class of new input.\nIdentify data sources The datasets contains two files:\nmetadata: The 1st and 2nd columns in the dataset store the unique ID numbers of the samples and disease (CRC=Patients, healthy=Control), respectively. profile: The gut microbial species level profile. Loading libraries import numpy as np import pandas as pd Importing Dataset First, load the TSV and CSV file using read_table or read_csv function of Pandas, respectively\nmetadata = pd.read_csv(\u0026#34;./dataset/metadata.csv\u0026#34;, index_col=0) profile = pd.read_table(\u0026#34;./dataset/species.tsv\u0026#34;, sep=\u0026#34;\\t\u0026#34;) Inspecting the data The first step is to visually inspect datasets.\nmetadata.head() profile.head() Choosing only healthy or CRC samples Selecting healthy or CRC samples to further data analysis\nfiltering disease on metadata dataset phen = metadata.loc[(metadata[\u0026#39;disease\u0026#39;] == \u0026#39;healthy\u0026#39;) | (metadata[\u0026#39;disease\u0026#39;] == \u0026#39;CRC\u0026#39;)] phen filtering the species with low occurrrence profile_trim = profile[phen.index] profile_trim.index = profile.TaxaID prof = profile_trim[profile_trim.apply(lambda x: np.count_nonzero(x)/len(x), axis=1) \u0026gt; 0.2] prof The “info()” method provides a concise summary of the data; from the output, it provides the type of data in each column, the number of non-null values in each column, and how much memory the data frame is using.\nThe method get_dtype_counts() will return the number of columns of each type in a DataFrame:\n# Review data types with \u0026#34;info()\u0026#34;. phen.info() \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; Index: 504 entries, SAMD00114718 to SAMD00165033 Data columns (total 1 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 disease 504 non-null object dtypes: object(1) memory usage: 7.9+ KB # Review number of columns of each data type in a DataFrame: phen.dtypes.value_counts() object 1 dtype: int64 #check for missing variables phen.isnull().any() disease False dtype: bool phen.disease.unique() array([\u0026#39;healthy\u0026#39;, \u0026#39;CRC\u0026#39;], dtype=object) From the results above, disease is a categorical variable, because it represents a fix number of possible values (i.e, disease. The machine learning algorithms wants numbers, and not strings, as their inputs so we need some method of coding to convert them.\nIntegrating the phen and prof data Here, we select disease from phen and then integrate it with prof into new dataset for the downstream analysis\nphen_cln = phen.iloc[:, 0].rename_axis(\u0026#34;SampleID\u0026#34;).reset_index() phen_cln.head() prof_cln = prof.T.rename_axis(\u0026#34;SampleID\u0026#34;).reset_index() prof_cln.head() mdat = pd.merge(phen_cln, prof_cln, on=\u0026#34;SampleID\u0026#34;, how=\u0026#34;inner\u0026#34;) mdat.head() #save the cleaner version of dataframe for future analyis mdat.to_csv(\u0026#39;./dataset/MergeData.tsv\u0026#39;, sep=\u0026#39;\\t\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;, index=False) Summary 151 species were selected more then 0.2 occurrence in Gastric Cancer 504 patients with Gastric Cancer were chosen Reference Breast-cancer-risk-prediction ","date":1667005994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667005994,"objectID":"7bccd486c4f7d4b3238b7248bfc37c65","permalink":"https://zouhua.top/post/machine_learning/ml002-dataclean/","publishdate":"2022-10-28T20:13:14-05:00","relpermalink":"/post/machine_learning/ml002-dataclean/","section":"post","summary":"Subsetting dataset for machine learning","tags":["machine learning","SVM","microbiota"],"title":"Machine Learning on gut microbiota of patients with Colorectal cancer (2): Data clean","type":"post"},{"authors":null,"categories":["Machine Learning"],"content":"Obtaining inpudata Rmarkdown 1: Downloading data.\nDownloading datasets using curatedMetagenomicData, which contains the HUMANN or Metaphlan results.\nLoading packages knitr::opts_chunk$set(warning = FALSE) library(dplyr) library(tibble) library(curatedMetagenomicData) #library(curatedMetagenomicAnalyses) # rm(list = ls()) options(stringsAsFactors = F) options(future.globals.maxSize = 1000 * 1024^2) Investigating potential response variables These are the 10 study conditions most commonly found in curatedMetagenomicData: data(\u0026#34;sampleMetadata\u0026#34;) availablediseases \u0026lt;- pull(sampleMetadata, study_condition) %\u0026gt;% table() %\u0026gt;% sort(decreasing = TRUE) availablediseases And the number of studies they are found in: studies \u0026lt;- lapply(names(availablediseases), function(x){ filter(sampleMetadata, study_condition %in% x) %\u0026gt;% pull(study_name) %\u0026gt;% unique() }) names(studies) \u0026lt;- names(availablediseases) studies \u0026lt;- studies[-grep(\u0026#34;control\u0026#34;, names(studies))] #get rid of controls studies \u0026lt;- studies[sapply(studies, length) \u0026gt; 1] #available in more than one study studies Each of these datasets has six data types associated with it; for example: curatedMetagenomicData(pattern = \u0026#34;YachidaS_2019.+\u0026#34;, dryrun = TRUE, counts = TRUE, rownames = \u0026#34;long\u0026#34;) The metagenomics datasets contain more than 13 types data, which comprising taxonomic and functional profile with relative and absolute abundance matrix.\nRelative abundance: storing into TreeSummarizedExperiment object YachidaS_2019_dataset \u0026lt;- curatedMetagenomicData(pattern = \u0026#34;YachidaS_2019.+relative_abundance\u0026#34;, dryrun = FALSE, counts = TRUE, rownames = \u0026#34;long\u0026#34;) YachidaS_2019_RB_TSE \u0026lt;- YachidaS_2019_dataset$`2021-10-14.YachidaS_2019.relative_abundance` YachidaS_2019_RB_TSE Write relative abundance datasets to disk if (0) { for (i in seq_along(studies)){ cond \u0026lt;- names(studies)[i] se \u0026lt;- curatedMetagenomicAnalyses::makeSEforCondition(cond, removestudies = \u0026#34;HMP_2019_ibdmdb\u0026#34;, dataType = \u0026#34;relative_abundance\u0026#34;) print(paste(\u0026#34;Next study condition:\u0026#34;, cond, \u0026#34; /// Body site: \u0026#34;, unique(colData(se)$body_site))) print(with(colData(se), table(study_name, study_condition))) cat(\u0026#34;\\n \\n\u0026#34;) save(se, file = paste0(cond, \u0026#34;.rda\u0026#34;)) flattext \u0026lt;- select(as.data.frame(colData(se)), c(\u0026#34;study_name\u0026#34;, \u0026#34;study_condition\u0026#34;, \u0026#34;subject_id\u0026#34;)) rownames(flattext) \u0026lt;- colData(se)$sample_id flattext \u0026lt;- cbind(flattext, data.frame(t(assay(se)))) write.csv(flattext, file = paste0(cond, \u0026#34;.csv\u0026#34;)) system(paste0(\u0026#34;gzip \u0026#34;, cond, \u0026#34;.csv\u0026#34;)) } } Preparing for machine learning metadata\nrelative abundance profile\nmetadata \u0026lt;- colData(YachidaS_2019_RB_TSE) %\u0026gt;% data.frame() phenotype \u0026lt;- metadata %\u0026gt;% dplyr::select(disease) %\u0026gt;% tibble::rownames_to_column(\u0026#34;SampleID\u0026#34;) %\u0026gt;% dplyr::filter(disease %in% c(\u0026#34;CRC\u0026#34;, \u0026#34;healthy\u0026#34;)) profile \u0026lt;- assay(YachidaS_2019_RB_TSE) sid \u0026lt;- intersect(phenotype$SampleID, colnames(profile)) prof \u0026lt;- profile %\u0026gt;% data.frame() %\u0026gt;% tibble::rownames_to_column(\u0026#34;TaxaID\u0026#34;) %\u0026gt;% dplyr::group_by(TaxaID) %\u0026gt;% dplyr::mutate(TaxaID_new = unlist(strsplit(TaxaID, \u0026#34;\\\\|\u0026#34;))[7]) %\u0026gt;% dplyr::select(TaxaID, TaxaID_new, all_of(sid)) %\u0026gt;% dplyr::ungroup() %\u0026gt;% dplyr::select(-TaxaID) %\u0026gt;% dplyr::rename(TaxaID = TaxaID_new) phen \u0026lt;- phenotype %\u0026gt;% dplyr::filter(SampleID %in% sid) output if (!dir.exists(\u0026#34;./dataset\u0026#34;)) { dir.create(\u0026#34;./dataset\u0026#34;, recursive = TRUE) } write.csv(phen, \u0026#34;./dataset/metadata.csv\u0026#34;, row.names = F) write.table(prof, \u0026#34;./dataset/species.tsv\u0026#34;, sep = \u0026#34;\\t\u0026#34;, quote = F, row.names = F) Session info devtools::session_info() Reference Create datasets for machine learning ","date":1666919594,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666919594,"objectID":"5a881fc531763c1e7c78d97a61e10d43","permalink":"https://zouhua.top/post/machine_learning/ml001-inputdata/","publishdate":"2022-10-27T20:13:14-05:00","relpermalink":"/post/machine_learning/ml001-inputdata/","section":"post","summary":"preparing input data for machine learning","tags":["machine learning","SVM","microbiota"],"title":"Machine Learning on gut microbiota of patients with Colorectal cancer (1): Obtaining inpudata","type":"post"},{"authors":null,"categories":["Statistics"],"content":"前言 差异分析是转录组数据分析的必需技能之一，但众多的转录组分析R包如DESeq2，limma和edgeR等让分析人员不知如何选择，还有它们之间的优劣如何？我将在本文详细探讨常用差异分析R包以及结合t-test/wilcox-rank-sum test的差异分析结果的异同点。\n大纲 本文要点由以下几点构成：\n下载以及导入测试数据（批量安装R包）；\n基因表达count矩阵的标准化方法（F(R)PKM/TPM）；\n基因整体水平分布（PCA/tSNE/UMAP；heatmap）；\nDESeq2差异分析实现以及结果解析；\nlimma差异分析实现以及结果解析；\nedgeR差异分析实现以及结果解析；\n结合t-test或wilcox-rank-sum-test方法的差异分析实现以及结果解析（是否符合正态分布选择检验方法）；\n不同方法的结果比较（volcano plot+heatmap+venn）；\n总结。\n导入R包 本次分析需要在R中批量安装包，具体安装方法可参考如何安装R包。先导入基础R包，在后面每个差异分析模块再导入所需要的差异分析R包。\nlibrary(dplyr) library(tibble) library(data.table) library(ggplot2) library(patchwork) library(cowplot) # rm(list = ls()) options(stringsAsFactors = F) options(future.globals.maxSize = 1000 * 1024^2) grp \u0026lt;- c(\u0026#34;Normal\u0026#34;, \u0026#34;Tumor\u0026#34;) grp.col \u0026lt;- c(\u0026#34;#568875\u0026#34;, \u0026#34;#73FAFC\u0026#34;) 转录组数据 本文下载的TCGA-HNSC转录组数据是通过本人先前撰写的R脚本实现的，大家可参考Downloading and preprocessing TCGA Data through R program language文章自行下载，也可以邮件询问我百度网盘密码。\n百度网盘链接：https://pan.baidu.com/s/1Daz5UsOd39T8r8K6zxPASQ\nphenotype \u0026lt;- fread(\u0026#34;TCGA-HNSC-post_mRNA_clinical.csv\u0026#34;) count \u0026lt;- fread(\u0026#34;TCGA-HNSC-post_mRNA_profile.tsv\u0026#34;) table(phenotype$Group) 标准化 标准化的目的是为了降低测序深度以及基因长度对基因表达谱下游分析的影响。测序深度越高则map到基因的reads也越多，同理基因长度越长则map到的reads也越多，最后对应的counts数目也越多。\nRPM/CPM: Reads/Counts of exon model per Million mapped reads (每百万映射读取的reads) $$RPM = \\frac{ExonMappedReads * 10^{6}}{TotalMappedReads}$$\nRPKM: Reads Per Kilobase of exon model per Million mapped reads (每千个碱基的转录每百万映射读取的reads) $$RPKM = \\frac{ExonMappedReads * 10^{9}}{TotalMappedReads * ExonLength}$$\nFPKM: Fragments Per Kilobase of exon model per Million mapped fragments(每千个碱基的转录每百万映射读取的fragments)， 适用于PE测序。 $$ FPKM = \\frac{ExonMappedFragments * 10^{9}}{TotalMappedFragments * ExonLength}$$\nTPM：Transcripts Per Kilobase of exon model per Million mapped reads (每千个碱基的转录每百万映射读取的Transcripts) $$TPM= \\frac{N_i/L_i * 10^{6}}{sum(N_1/L_1+N_2/L_2+…+N_j/L_j+…+N_n/L_n)}$$ $N_i$为比对到第i个exon的reads数； $L_i$为第i个exon的长度；sum()为所有 (n个)exon按长度进行标准化之后数值的和\n获取gene length表：对Homo_sapiens.GRCh38.101版本数据处理获取gene length数据；human_gene_all.tsv是使用biomart包获取gene symbol和ensembleID的对应关系表。 geneLength \u0026lt;- fread(\u0026#34;Homo_sapiens.GRCh38.101.genelength.tsv\u0026#34;) geneIdAll \u0026lt;- fread(\u0026#34;human_gene_all.tsv\u0026#34;) geneIdLength \u0026lt;- geneIdAll %\u0026gt;% filter(transcript_biotype == \u0026#34;protein_coding\u0026#34;) %\u0026gt;% dplyr::select(ensembl_gene_id, external_gene_name) %\u0026gt;% inner_join(geneLength, by = c(\u0026#34;ensembl_gene_id\u0026#34;=\u0026#34;V1\u0026#34;)) %\u0026gt;% dplyr::select(-ensembl_gene_id) %\u0026gt;% dplyr::distinct() %\u0026gt;% dplyr::rename(Length=V2) geneIdLengthUniq \u0026lt;- geneIdLength[pmatch(count$Feature, geneIdLength$external_gene_name), ] %\u0026gt;% filter(!is.na(Length)) %\u0026gt;% arrange(external_gene_name) count_cln \u0026lt;- count %\u0026gt;% filter(Feature%in%geneIdLengthUniq$external_gene_name) %\u0026gt;% arrange(Feature) %\u0026gt;% column_to_rownames(\u0026#34;Feature\u0026#34;) if(!any(geneIdLengthUniq$external_gene_name == rownames(count_cln))){ message(\u0026#34;Order of GeneName is wrong\u0026#34;) } gene_lengths \u0026lt;-geneIdLengthUniq$Length head(geneIdLengthUniq) RPKM/FPKM/TPM 转换 countToFpkm \u0026lt;- function(counts, lengths){ pm \u0026lt;- sum(counts) /1e6 rpm \u0026lt;- counts/pm rpm/(lengths/1000) } countToTpm \u0026lt;- function(counts, lengths) { rpk \u0026lt;- counts/(lengths/1000) coef \u0026lt;- sum(rpk) / 1e6 rpk/coef } # FPKM count_FPKM \u0026lt;- apply(count_cln, 2, function(x){countToFpkm(x, gene_lengths)}) %\u0026gt;% data.frame() # TPM count_TPM \u0026lt;- apply(count_cln, 2, function(x){countToTpm(x, gene_lengths)}) %\u0026gt;% data.frame() head(count_cln) head(count_FPKM) head(count_TPM) 整体水平比较 在做差异分析前，一般可以对数据做一个降维处理，然后看不同分组是否能在二维展开平面区分开。\nExpressionSet 先将数据存成ExpressionSet格式，可参考RNA-seq数据的批次校正方法文章。ExpressionSet对象数据包含表达谱和metadata等数据，这方便后期分析。\ngetExprSet \u0026lt;- function(metadata=phenotype, profile=count_cln, occurrence=0.2){ # metadata=phenotype # profile=count_cln # occurrence=0.2 sid \u0026lt;- intersect(metadata$SampleID, colnames(profile)) # phenotype phe \u0026lt;- metadata %\u0026gt;% filter(SampleID%in%sid) %\u0026gt;% column_to_rownames(\u0026#34;SampleID\u0026#34;) # profile by occurrence prf \u0026lt;- profile %\u0026gt;% rownames_to_column(\u0026#34;tmp\u0026#34;) %\u0026gt;% filter(apply(dplyr::select(., -one_of(\u0026#34;tmp\u0026#34;)), 1, function(x) { sum(x != 0)/length(x)}) \u0026gt; occurrence) %\u0026gt;% dplyr::select(c(tmp, rownames(phe))) %\u0026gt;% column_to_rownames(\u0026#34;tmp\u0026#34;) # determine the right order between profile and phenotype for(i in 1:ncol(prf)){ if (!(colnames(prf)[i] == rownames(phe)[i])) { stop(paste0(i, \u0026#34; Wrong\u0026#34;)) } } require(convert) exprs \u0026lt;- as.matrix(prf) adf \u0026lt;- new(\u0026#34;AnnotatedDataFrame\u0026#34;, data=phe) experimentData \u0026lt;- new(\u0026#34;MIAME\u0026#34;, name=\u0026#34;Hua Zou\u0026#34;, lab=\u0026#34;UCAS\u0026#34;, contact=\u0026#34;zouhua@outlook.com\u0026#34;, title=\u0026#34;TCGA-HNSC\u0026#34;, abstract=\u0026#34;The gene ExpressionSet\u0026#34;, url=\u0026#34;www.zouhua.top\u0026#34;, other=list(notes=\u0026#34;Created from text files\u0026#34;)) expressionSet \u0026lt;- new(\u0026#34;ExpressionSet\u0026#34;, exprs=exprs, phenoData=adf, experimentData=experimentData) return(expressionSet) } ExprSet_count \u0026lt;- getExprSet(profile=count_cln) ExprSet_FPKM \u0026lt;- getExprSet(profile=count_FPKM) ExprSet_TPM \u0026lt;- getExprSet(profile=count_TPM) ExprSet_count ExprSet_FPKM ExprSet_TPM 降维分析 高纬度数据降维方法很多，我们这里选择了PCA+tSNE+UMAP分别展示降维后的结果，可参考文章高纬度数据降维方法的R实现。另外合并多个图的方法，请参考文章patchwork:合并多个R图的包。\nPCA PCAFun \u0026lt;- function(dataset = ExprSet_count){ # dataset = ExprSet_count require(convert) metadata \u0026lt;- pData(dataset) profile \u0026lt;- …","date":1666919594,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666919594,"objectID":"502dca6c954348598c688fab47f1c873","permalink":"https://zouhua.top/post/math_statistics/2022-10-27-da-methods-comparsion/","publishdate":"2022-10-27T20:13:14-05:00","relpermalink":"/post/math_statistics/2022-10-27-da-methods-comparsion/","section":"post","summary":"差异分析是转录组数据分析的必备技能之一，但众多的转录组分析R包*DESeq2*，*limma*和*edgeR*让分析人员有选择问题，那么它们之间的优劣到底是如何的呢?","tags":["RNA-seq","DifferentialAnalysis"],"title":"转录组差异分析（DESeq2+limma+edgeR+t-test/wilcox-test）总结","type":"post"},{"authors":null,"categories":["Tool"],"content":"Introduction Have you ever thought about making your own website using R and Rstudio? This tutorial would teach you how to create your personal blog via blogdown and github. What’s more, automatically eploying your website is also necessary by using github actions.\nPrerequisites Before running the tutorial, you need have this software in your PC\nR\nRstudio\nRegistering github account Please go to github to obtain your own account. Then create a new repository named [your_github_names].github.io which is applied to render your github pages.\nCreating repository for blog Creating another private repository (named MyBlog) to save your files which are used to setup blog. Here, suggesting users create two repositories to deploy personal website according to the protection of personal privacy.\n[your_github_names].github.io for saving blog html files MyBlog for saving blog source files Install Blogdown and Hugo Blogdown which Creating Blogs and Websites with R Markdown is necessary for building website\n# Install blogdown #install.packages(\u0026#34;blogdown\u0026#34;) remotes::install_github(\u0026#39;rstudio/blogdown\u0026#39;) # Install Hugo blogdown::install_hugo() Creating blogdown project Firstly, open Rstudio to create New Project -\u0026gt; New Directory -\u0026gt; Website using blogdown and then give MyOwnWebsite to Directory name, and then setting Hugo theme by gcushen/hugo-academic. Finally, Kick Create Project to generate website.\nThen, opening Rstudio to create blogdown project (File -\u0026gt; New Project -\u0026gt; Website using blogdown -\u0026gt; New Prject Wizard -\u0026gt; Create Project). YOu could see the following files and directories.\nthe final website folder like this\nBuilding website To build the website by using hugo_build from blogdown R package.\nblogdown::hugo_build(local=TRUE) All the actual files of the website are stored in public/ folder.\nTo preview the website using\nblogdown::serve_site() config.yaml: Hugo and theme configuration file. .Rprofile: File to set up your blogdown options. netlify.toml: File to set up your Netlify options. content/: Website source files to edit and add, such as blog posts. themes/: Hugo theme. Basic Customization The basic files that you want to modify to customize your website are the following:\nconfig/_default/config.yaml: general website information config/_default/params.yaml: website customization config/_default/menus.yaml: top bar / menu customization content/authors/admin/_index.md: personal information Uploading blogdown project to MyBlog Repository Moving the files of blogdown into MyOwnWebsite directory and then using git to push all the files to the remote repository.\nCreating Github Actions Opening MyBlog Repository (Actions -\u0026gt; New workflow) to create deploy.yml.\nname: deploy_blog on: push: branches: [main, master] pull_request: branches: [main, master] release: types: [published] workflow_dispatch: jobs: build: runs-on: ubuntu-latest steps: - name: checkout uses: actions/checkout@v3 with: submodules: true - name: Setup Hugo uses: peaceiris/actions-hugo@v2.5.0 with: hugo-version: \u0026#39;0.105.0\u0026#39; extended: true - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3.8.0 with: deploy_key: ${{ secrets.ACTIONS_DEPLOY_KEY }} external_repository: HuaZou/HuaZou.github.io publish_branch: master publish_dir: ./public allow_empty_commit: true Deployment We need to connect the Github Pages repository ([your_github_names].github.io) and blog repository (MyBlog) through ssh key. Go to ~/.ssh/.\nssh-keygen -t rsa -C \u0026#34;zouhua1@outlook.com\u0026#34; private key is for MyOwnWebsite (settings -\u0026gt; Secrets)\npublic key is for [your_github_names].github.io (Deploy keys)\nWhen you upload your files to MyOwnWebsite and github will automatically compile the files and then deploy html files to [your_github_names].github.io to render your website.\nReference How To Make A Personal Website with Hugo ","date":1661217194,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661217194,"objectID":"4b8eda7a56d5803a903c60e6acb57df4","permalink":"https://zouhua.top/post/tool/2022-08-22-blogdown-hugo/","publishdate":"2022-08-22T20:13:14-05:00","relpermalink":"/post/tool/2022-08-22-blogdown-hugo/","section":"post","summary":"Introduction Have you ever thought about making your own website using R and Rstudio? This tutorial would teach you how to create your personal blog via blogdown and github. What’s more, automatically eploying your website is also necessary by using github actions.","tags":["blogdown","github","hugo"],"title":"How to setup personal blog by blogdown and githup pages","type":"post"},{"authors":["Hua Zou"],"categories":null,"content":"","date":1660867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660867200,"objectID":"87c79596733bec228d6c7e28407533a7","permalink":"https://zouhua.top/bookdown/2022-xmas2/","publishdate":"2022-08-19T00:00:00Z","relpermalink":"/bookdown/2022-xmas2/","section":"bookdown","summary":"In this tutorial, we focus on differential analysis with the step-by-step procedures by using the R programming language. We also briefly introduce the concepts and principals of the statistical methods before the applications and give conclusions on the results.","tags":null,"title":"Statistical toolkits on microbiota data","type":"bookdown"},{"authors":["Hua Zou","Dan Wang","Huahui Ren"],"categories":null,"content":"","date":1582502400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582502400,"objectID":"20d79b52d65fe064fabe61be5389ae1c","permalink":"https://zouhua.top/publication/2020-nutrients/","publishdate":"2020-02-24T00:00:00Z","relpermalink":"/publication/2020-nutrients/","section":"publication","summary":"This non-controlled intervention study revealed associations between baseline gut microbiota and CR-induced BMI loss and provided evidence to accelerate the application of microbiome stratification in future personalized nutrition intervention.","tags":null,"title":"Effect of Caloric Restriction on BMI, Gut Microbiota, and Blood Amino Acid Levels in Non-Obese Adults","type":"publication"}]